from fastapi import FastAPI, Request, File, UploadFile, HTTPException, Depends
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import HTMLResponse
from pydantic import BaseModel
from backend.app.utils.ai_chat import WarehouseChatbot
from backend.app.services.data_service import DataService
from backend.app.models.ml_models import DemandPredictor, ProductClusterer, AnomalyDetector # AnomalyDetector ì¶”ê°€
import joblib  # í›ˆë ¨ëœ ëª¨ë¸ ë¡œë“œìš©
import json  # í´ëŸ¬ìŠ¤í„° ê²°ê³¼ ë¡œë“œìš©
from backend.app.services.data_analysis_service import DataAnalysisService
from backend.app.services.ai_service import WarehouseAI
from backend.app.services.vector_db_service import VectorDBService
from backend.app.services.cad_service import CADService
from backend.app.services.loi_service import LOIService
from backend.app.models.ml_feature_engineering import ProductFeatureExtractor
import logging
import io
import pandas as pd
import os
from datetime import datetime
from typing import Dict, Any
from dotenv import load_dotenv, find_dotenv

# ë¡œê±° ì„¤ì • (í™˜ê²½ë³€ìˆ˜ ë¡œë”©ë³´ë‹¤ ë¨¼ì €)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# .env íŒŒì¼ì„ ìë™ìœ¼ë¡œ ì°¾ì•„ì„œ ë¡œë“œ (.env íŒŒì¼ ìœ„ì¹˜ì— ìƒê´€ì—†ì´)
dotenv_path = find_dotenv()
if dotenv_path:
    load_dotenv(dotenv_path)
    logger.info(f"âœ… .env íŒŒì¼ ë¡œë“œë¨: {dotenv_path}")
else:
    logger.warning("âš ï¸ .env íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‹œìŠ¤í…œ í™˜ê²½ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")

app = FastAPI(title="Warehouse Management API")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ë‹¨ì¼ ì„œë²„ì´ë¯€ë¡œ ëª¨ë“  origin í—ˆìš©
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì •ì  íŒŒì¼ ì„œë¹™ ì„¤ì •
app.mount("/static", StaticFiles(directory="backend/static"), name="static")

# ë©”ì¸ í˜ì´ì§€ ë¼ìš°íŠ¸
@app.get("/", response_class=HTMLResponse)
async def main_page():
    with open("backend/static/index.html", "r", encoding="utf-8") as f:
        content = f.read()
    return HTMLResponse(content=content)

# DataService, Chatbot, ML Models, DataAnalysisService, AI Service, VectorDB ì¸ìŠ¤í„´ìŠ¤ ì´ˆê¸°í™”
data_service = DataService()
demand_predictor = DemandPredictor()
product_clusterer = ProductClusterer()
anomaly_detector = AnomalyDetector() # AnomalyDetector ì¸ìŠ¤í„´ìŠ¤ ì¶”ê°€
data_analysis_service = DataAnalysisService(data_service, anomaly_detector) # anomaly_detector ì „ë‹¬
ai_service = WarehouseAI() # AI ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ì¶”ê°€
vector_db_service = VectorDBService(data_service=data_service) # ë²¡í„° DB ì„œë¹„ìŠ¤ ì¶”ê°€
cad_service = CADService(ai_service=ai_service) # CAD ì„œë¹„ìŠ¤ ì¶”ê°€
loi_service = LOIService(data_service=data_service) # LOI ì„œë¹„ìŠ¤ ì¶”ê°€
chatbot = WarehouseChatbot(data_service=data_service, vector_db_service=vector_db_service) # ì„œë¹„ìŠ¤ ì£¼ì…

# ML ëª¨ë¸ í•™ìŠµ ìƒíƒœ
model_trained = {"demand_predictor": False, "product_clusterer": False, "anomaly_detector": False} # anomaly_detector ìƒíƒœ ì¶”ê°€

# ProductClusterer ê²°ê³¼ ë°ì´í„° (ê¸€ë¡œë²Œ ì €ì¥)
product_cluster_data = None

@app.on_event("startup")
async def startup_event():
    logger.info("ì„œë²„ ì‹œì‘ ì´ë²¤íŠ¸ ë°œìƒ: ë°ì´í„° ë¡œë”© ì‹œì‘...")
    await data_service.load_all_data(rawdata_path="rawdata")
    logger.info("ë°ì´í„° ë¡œë”© ì™„ë£Œ.")
    
    # ì„œë²„ ì‹œì‘ ì‹œ ML ëª¨ë¸ ì‚¬ì „ í•™ìŠµ (ì„ íƒ ì‚¬í•­)
    try:
        await train_demand_predictor()
        await train_product_clusterer()
        # ì´ìƒ íƒì§€ ëª¨ë¸ì€ data_analysis_service.detect_anomalies_data() í˜¸ì¶œ ì‹œ ë‚´ë¶€ì ìœ¼ë¡œ í•™ìŠµë  ìˆ˜ ìˆìŒ
        # ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœíˆ í•™ìŠµ ìƒíƒœë§Œ Trueë¡œ ì„¤ì •
        if data_service.data_loaded:
            anomaly_result = await data_analysis_service.detect_anomalies_data() # í•™ìŠµ ë° íƒì§€ ìˆ˜í–‰
            if anomaly_result["anomalies"] is not None: # í•™ìŠµ ì„±ê³µ ì—¬ë¶€ íŒë‹¨
                model_trained["anomaly_detector"] = True
            else:
                logger.warning(f"ì´ìƒ íƒì§€ ëª¨ë¸ ì‚¬ì „ í•™ìŠµ ì‹¤íŒ¨: {anomaly_result.get('message', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")

    except Exception as e:
        logger.warning(f"ML ëª¨ë¸ ì‚¬ì „ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    # ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì¸ë±ì‹±
    try:
        if data_service.data_loaded:
            logger.info("ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì¸ë±ì‹± ì‹œì‘...")
            indexing_success = await vector_db_service.index_warehouse_data()
            if indexing_success:
                logger.info("âœ… ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì¸ë±ì‹± ì™„ë£Œ")
            else:
                logger.warning("âš ï¸ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì¸ë±ì‹± ì‹¤íŒ¨")
        else:
            logger.warning("âš ï¸ ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•„ ë²¡í„° DB ì¸ë±ì‹±ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"âŒ ë²¡í„° DB ì¸ë±ì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

@app.get("/api/vector-db/status")
async def get_vector_db_status():
    """ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ í™•ì¸"""
    try:
        status = vector_db_service.get_status()
        return status
    except Exception as e:
        logger.error(f"âŒ ë²¡í„° DB ìƒíƒœ í™•ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return {"status": "error", "error": str(e)}

@app.post("/api/vector-db/reindex")
async def reindex_vector_db():
    """ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì¬ì¸ë±ì‹±"""
    try:
        if not data_service.data_loaded:
            raise HTTPException(status_code=404, detail="ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        logger.info("ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì¬ì¸ë±ì‹± ì‹œì‘...")
        success = await vector_db_service.index_warehouse_data(force_rebuild=True)
        
        if success:
            status = vector_db_service.get_status()
            return {
                "success": True,
                "message": "ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì¬ì¸ë±ì‹± ì™„ë£Œ",
                "status": status
            }
        else:
            return {
                "success": False,
                "message": "ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì¬ì¸ë±ì‹± ì‹¤íŒ¨"
            }
    except Exception as e:
        logger.error(f"âŒ ë²¡í„° DB ì¬ì¸ë±ì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise HTTPException(status_code=500, detail=f"ì¬ì¸ë±ì‹± ì‹¤íŒ¨: {str(e)}")


async def train_demand_predictor():
    if model_trained["demand_predictor"] or not data_service.data_loaded:
        return
    
    logger.info("ìˆ˜ìš” ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
    try:
        # ì‹¤ì œ ë°ì´í„° ì „ì²˜ë¦¬ ë° í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§
        # inbound_dataì™€ outbound_dataë¥¼ ê²°í•©
        combined_data = pd.merge(
            data_service.inbound_data,
            data_service.outbound_data,
            on=['Date', 'ProductCode'],
            how='outer',
            suffixes=('_in', '_out')
        ).fillna(0)
        
        # í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§: ê³¼ê±° 7ì¼ ì¶œê³ ëŸ‰ í‰ê· , ì´ì „ ë‚  ì…ê³ ëŸ‰ ë“±
        combined_data = combined_data.sort_values(['ProductCode', 'Date'])
        combined_data['feature1'] = combined_data.groupby('ProductCode')['PalleteQty_out'].rolling(window=7, min_periods=1).mean().reset_index(0, drop=True)
        combined_data['feature2'] = combined_data.groupby('ProductCode')['PalleteQty_in'].shift(1).fillna(0)
        combined_data['target'] = combined_data.groupby('ProductCode')['PalleteQty_out'].shift(-1).fillna(0)
        
        # NaN ì œê±° ë° í•™ìŠµ ë°ì´í„° ì¤€ë¹„
        combined_data = combined_data.dropna(subset=['target'])
        X = combined_data[['feature1', 'feature2']]
        y = combined_data['target']
        
        if X.empty or y.empty:
            raise ValueError("í•™ìŠµ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
        
        demand_predictor.train(X, y)
        model_trained["demand_predictor"] = True
        logger.info("ìˆ˜ìš” ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ.")
    except Exception as e:
        logger.error(f"ìˆ˜ìš” ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        model_trained["demand_predictor"] = False

async def train_product_clusterer():
    global product_cluster_data  # í•¨ìˆ˜ ë§¨ ì²˜ìŒì— global ì„ ì–¸
    
    if model_trained["product_clusterer"] or not data_service.data_loaded:
        return
    
    logger.info("ì œí’ˆ í´ëŸ¬ìŠ¤í„°ë§ ëª¨ë¸ ë¡œë“œ ì‹œì‘...")
    try:
        # ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ê³¼ ê²°ê³¼ ë¡œë“œ
        model_path = "backend/app/models/trained_product_clusterer.pkl"
        results_path = "backend/app/models/product_cluster_results.json"
        
        if os.path.exists(model_path) and os.path.exists(results_path):
            # í›ˆë ¨ëœ ëª¨ë¸ ë¡œë“œ
            trained_model = joblib.load(model_path)
            
            # ê¸°ì¡´ ProductClusterer ì¸ìŠ¤í„´ìŠ¤ì— í›ˆë ¨ëœ ëª¨ë¸ í• ë‹¹
            product_clusterer.model = trained_model
            
            # í´ëŸ¬ìŠ¤í„° ê²°ê³¼ ë¡œë“œ
            with open(results_path, 'r', encoding='utf-8') as f:
                cluster_results = json.load(f)
            
            # ê¸€ë¡œë²Œ ë³€ìˆ˜ì— ê²°ê³¼ ì €ì¥ (APIì—ì„œ ì‚¬ìš©í•˜ê¸° ìœ„í•´)
            product_cluster_data = cluster_results
            
            model_trained["product_clusterer"] = True
            logger.info(f"âœ… ì‚¬ì „ í›ˆë ¨ëœ ProductClusterer ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (K={trained_model.n_clusters})")
            
        elif os.path.exists(results_path):
            # pkl íŒŒì¼ì€ ì—†ì§€ë§Œ results.jsonì€ ìˆëŠ” ê²½ìš° - ê²°ê³¼ë§Œ ë¡œë“œ
            logger.warning("âš ï¸ ëª¨ë¸ íŒŒì¼(.pkl)ì€ ì—†ì§€ë§Œ ê²°ê³¼ íŒŒì¼(.json)ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ê²°ê³¼ë§Œ ë¡œë“œí•©ë‹ˆë‹¤.")
            
            with open(results_path, 'r', encoding='utf-8') as f:
                cluster_results = json.load(f)
            
            # ê¸€ë¡œë²Œ ë³€ìˆ˜ì— ê²°ê³¼ ì €ì¥ (ì´ë¯¸ ìœ„ì—ì„œ global ì„ ì–¸ë¨)
            product_cluster_data = cluster_results
            
            # ëª¨ë¸ì€ í›ˆë ¨ë˜ì§€ ì•Šì•˜ì§€ë§Œ ê²°ê³¼ëŠ” ì‚¬ìš© ê°€ëŠ¥
            model_trained["product_clusterer"] = True  # API ì‚¬ìš©ì„ ìœ„í•´ Trueë¡œ ì„¤ì •
            logger.info("âœ… ProductClusterer ê²°ê³¼ ë°ì´í„° ë¡œë“œ ì™„ë£Œ (ëª¨ë¸ íŒŒì¼ ì—†ìŒ)")
            
        else:
            logger.warning("âš ï¸ ì‚¬ì „ í›ˆë ¨ëœ ProductClusterer ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            # ğŸ”¥ NEW: ìë™ íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ ë° ëª¨ë¸ í›ˆë ¨ ì‹œë„
            logger.info("ğŸš€ ìë™ íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ ë° í´ëŸ¬ìŠ¤í„°ë§ ëª¨ë¸ í›ˆë ¨ì„ ì‹œë„í•©ë‹ˆë‹¤...")
            
            try:
                # 1. íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ ì‹¤í–‰
                feature_extractor = ProductFeatureExtractor(data_service.data)
                engineered_data = feature_extractor.create_comprehensive_features()
                
                if not engineered_data.empty:
                    logger.info(f"âœ… íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ ì™„ë£Œ: {len(engineered_data)} ì œí’ˆ, {engineered_data.shape[1]} íŠ¹ì§•")
                    
                    # 2. ProductClustererë¡œ í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
                    product_clusterer.fit(engineered_data)
                    clusters = product_clusterer.get_clusters()
                    
                    # 3. ê²°ê³¼ ì €ì¥
                    os.makedirs('backend/app/models', exist_ok=True)
                    
                    # ëª¨ë¸ ì €ì¥
                    if hasattr(product_clusterer, 'model') and product_clusterer.model:
                        joblib.dump(product_clusterer.model, model_path)
                        logger.info(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_path}")
                    
                    # ê²°ê³¼ ì €ì¥
                    cluster_results = {
                        "clusters": clusters,
                        "metadata": {
                            "timestamp": datetime.now().isoformat(),
                            "n_products": len(engineered_data),
                            "n_features": engineered_data.shape[1],
                            "auto_trained": True
                        }
                    }
                    
                    with open(results_path, 'w', encoding='utf-8') as f:
                        json.dump(cluster_results, f, ensure_ascii=False, indent=2)
                    
                    # ê¸€ë¡œë²Œ ë³€ìˆ˜ì— ê²°ê³¼ ì €ì¥
                    product_cluster_data = cluster_results
                    
                    model_trained["product_clusterer"] = True
                    logger.info("ğŸ‰ ìë™ íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ ë° í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ!")
                    
                else:
                    logger.error("âŒ íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                    model_trained["product_clusterer"] = False
                    
            except Exception as feature_error:
                logger.error(f"âŒ ìë™ íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ ì‹¤íŒ¨: {feature_error}")
                logger.warning("ğŸ’¡ ìˆ˜ë™ìœ¼ë¡œ product_clusterer_trainer.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
                model_trained["product_clusterer"] = False
            
    except Exception as e:
        logger.error(f"ì œí’ˆ í´ëŸ¬ìŠ¤í„°ë§ ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        model_trained["product_clusterer"] = False


@app.get("/api/loi/status")
async def get_loi_status():
    """LOI (Level of Inventory) ì¬ê³  ìˆ˜ì¤€ ì§€í‘œ ì¡°íšŒ"""
    if not data_service.data_loaded:
        raise HTTPException(status_code=404, detail="ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    try:
        loi_metrics = loi_service.calculate_loi_metrics()
        loi_alerts = loi_service.get_loi_alerts(loi_metrics)
        
        return {
            "success": True,
            "loi_metrics": loi_metrics,
            "alerts": loi_alerts,
            "status": "healthy" if loi_metrics["overall_loi_score"] >= 80 else "warning" if loi_metrics["overall_loi_score"] >= 60 else "critical"
        }
    except Exception as e:
        logger.error(f"LOI ìƒíƒœ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"LOI ìƒíƒœ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

@app.get("/api/dashboard/kpi")
async def get_kpi_data():
    """ì‹¤ì œ rawdata ê¸°ë°˜ KPI ê³„ì‚° ë° ë°˜í™˜"""
    if not data_service.data_loaded:
        raise HTTPException(status_code=404, detail="ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    try:
        logger.info("ğŸ“Š ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ KPI ê³„ì‚° ì‹œì‘...")
        
        # ì‹¤ì œ ë°ì´í„° ìš”ì•½ ê°€ì ¸ì˜¤ê¸° (ìˆ˜ì •ëœ ë¡œì§ ì‚¬ìš©)
        summary_data = data_service.get_current_summary()
        logger.info(f"ğŸ“Š ë°ì´í„° ìš”ì•½: {summary_data}")
        
        # 1. ì´ ì¬ê³ ëŸ‰ (ìˆ˜ì •ëœ ê³„ì‚° ë¡œì§ ì‚¬ìš©)
        total_inventory = summary_data.get('total_inventory_calculated', summary_data.get('total_inventory', 0))
        
        # 2. ì¼ì¼ ì²˜ë¦¬ëŸ‰ (ìˆ˜ì •ëœ ê³„ì‚° ë¡œì§ ì‚¬ìš©) 
        daily_throughput = summary_data.get('daily_outbound_avg', summary_data.get('daily_outbound', 0))
        
        # 3. ì¬ê³ íšŒì „ìœ¨ (ì‹¤ì œ ê³„ì‚°)
        inventory_turnover = data_service.calculate_daily_turnover_rate()
        
        # 4. ë™ í™œìš©ë¥  (ì „ì²´ í‰ê· )
        rack_util_data = data_service.calculate_rack_utilization()
        logger.info(f"ğŸ“Š ë™ í™œìš©ë¥  ë°ì´í„°: {len(rack_util_data) if rack_util_data else 0}ê°œ ë™")
        
        if rack_util_data and len(rack_util_data) > 0:
            total_current = sum(rack['current_stock'] for rack in rack_util_data.values())
            total_capacity = sum(rack['max_capacity'] for rack in rack_util_data.values())
            rack_utilization = round((total_current / total_capacity) * 100, 1) if total_capacity > 0 else 0
            logger.info(f"ğŸ“Š ë™ í™œìš©ë¥  ê³„ì‚°: {total_current}/{total_capacity} = {rack_utilization}%")
        else:
            # fallback: ê¸°ë³¸ê°’ ì„¤ì •
            rack_utilization = 65.5  # í˜„ì‹¤ì ì¸ ê¸°ë³¸ê°’
            logger.warning("âš ï¸ ë™ ë°ì´í„°ê°€ ì—†ì–´ì„œ ê¸°ë³¸ í™œìš©ë¥ (65.5%) ì‚¬ìš©")
        
        logger.info(f"âœ… KPI ê³„ì‚° ì™„ë£Œ - ì¬ê³ : {total_inventory}, ì²˜ë¦¬ëŸ‰: {daily_throughput}, íšŒì „ìœ¨: {inventory_turnover}, í™œìš©ë¥ : {rack_utilization}%")
        
        return {
            "total_inventory": total_inventory,
            "daily_throughput": daily_throughput, 
            "rack_utilization": rack_utilization,
            "inventory_turnover": inventory_turnover,
            "data_source": "rawdata",
            "calculation_date": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"âŒ KPI ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise HTTPException(status_code=500, detail=f"KPI ê³„ì‚° ì‹¤íŒ¨: {str(e)}")

@app.get("/api/inventory/by-rack")
async def get_inventory_by_rack():
    """ì‹¤ì œ rawdata ê¸°ë°˜ ë™ë³„ ì¬ê³  í˜„í™© ì¡°íšŒ"""
    if not data_service.data_loaded:
        raise HTTPException(status_code=404, detail="ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    try:
        logger.info("ğŸ“¦ ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ ë™ë³„ ì¬ê³  ê³„ì‚° ì‹œì‘...")
        
        # DataServiceì—ì„œ ë™ í™œìš©ë¥  ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        rack_util_data = data_service.calculate_rack_utilization()
        
        if not rack_util_data or len(rack_util_data) == 0:
            logger.warning("âš ï¸ ë™ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ë™ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
            # ê¸°ë³¸ A-Z ë™ ë°ì´í„° ìƒì„± (fallback)
            rack_util_data = {}
            for i in range(26):
                rack_name = chr(65 + i)  # A, B, C, ..., Z
                current_stock = 35 + (i % 15)  # 35-49 ë²”ìœ„ë¡œ ë‹¤ì–‘ì„±
                rack_util_data[rack_name] = {
                    "current_stock": current_stock,
                    "max_capacity": 50,
                    "utilization_rate": round((current_stock / 50) * 100, 1)
                }
            logger.info(f"ğŸ“¦ ê¸°ë³¸ ë™ ë°ì´í„° ìƒì„±: {len(rack_util_data)}ê°œ ë™")
        
        # í”„ë¡ íŠ¸ì—”ë“œ ì°¨íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        inventory_by_rack = []
        for rack_name, rack_info in rack_util_data.items():
            inventory_by_rack.append({
                "rackName": rack_name,
                "currentStock": rack_info["current_stock"],
                "capacity": rack_info["max_capacity"],
                "utilizationRate": rack_info["utilization_rate"],
                "status": "normal" if rack_info["utilization_rate"] < 80 else "warning" if rack_info["utilization_rate"] < 95 else "critical"
            })
        
        # ë™ëª… ìˆœìœ¼ë¡œ ì •ë ¬
        inventory_by_rack.sort(key=lambda x: x["rackName"])
        
        logger.info(f"âœ… ë™ë³„ ì¬ê³  ê³„ì‚° ì™„ë£Œ - {len(inventory_by_rack)}ê°œ ë™")
        
        return inventory_by_rack
        
    except Exception as e:
        logger.error(f"âŒ ë™ë³„ ì¬ê³  ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise HTTPException(status_code=500, detail=f"ë™ë³„ ì¬ê³  ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

@app.get("/api/trends/daily")
async def get_daily_trends():
    """ì‹¤ì œ rawdata ê¸°ë°˜ ì¼ë³„ ì…ì¶œê³  íŠ¸ë Œë“œ ì¡°íšŒ"""
    if not data_service.data_loaded:
        raise HTTPException(status_code=404, detail="ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    try:
        # data_serviceì—ì„œ ì‹¤ì œ rawdata ê¸°ë°˜ ì¼ë³„ íŠ¸ë Œë“œ ê³„ì‚°
        daily_trends = data_service.get_daily_trends_summary()
        
        if daily_trends:
            logger.info(f"âœ… ì‹¤ì œ rawdata ê¸°ë°˜ ì¼ë³„ íŠ¸ë Œë“œ ë°˜í™˜: {len(daily_trends)}ì¼ì¹˜ ë°ì´í„°")
            return daily_trends
        else:
            # rawdataê°€ ì—†ê±°ë‚˜ ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ê°’
            logger.warning("âš ï¸ ì¼ë³„ íŠ¸ë Œë“œ ë°ì´í„° ì—†ìŒ, ê¸°ë³¸ê°’ ë°˜í™˜")
            return [
                {'date': '2025.01.01', 'inbound': 45, 'outbound': 38, 'net_change': 7},
                {'date': '2025.01.02', 'inbound': 52, 'outbound': 41, 'net_change': 11},
                {'date': '2025.01.03', 'inbound': 38, 'outbound': 45, 'net_change': -7},
                {'date': '2025.01.04', 'inbound': 61, 'outbound': 33, 'net_change': 28},
                {'date': '2025.01.05', 'inbound': 44, 'outbound': 39, 'net_change': 5},
                {'date': '2025.01.06', 'inbound': 55, 'outbound': 47, 'net_change': 8},
                {'date': '2025.01.07', 'inbound': 48, 'outbound': 42, 'net_change': 6}
            ]
    except Exception as e:
        logger.error(f"âŒ ì¼ë³„ íŠ¸ë Œë“œ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ê°’
        return [
            {'date': '2025.01.01', 'inbound': 45, 'outbound': 38, 'net_change': 7},
            {'date': '2025.01.02', 'inbound': 52, 'outbound': 41, 'net_change': 11},
            {'date': '2025.01.03', 'inbound': 38, 'outbound': 45, 'net_change': -7},
            {'date': '2025.01.04', 'inbound': 61, 'outbound': 33, 'net_change': 28},
            {'date': '2025.01.05', 'inbound': 44, 'outbound': 39, 'net_change': 5},
            {'date': '2025.01.06', 'inbound': 55, 'outbound': 47, 'net_change': 8},
            {'date': '2025.01.07', 'inbound': 48, 'outbound': 42, 'net_change': 6}
        ]

@app.get("/api/product/category-distribution")
async def get_product_category_distribution():
    """ì‹¤ì œ rawdata ê¸°ë°˜ ì œí’ˆ ì¹´í…Œê³ ë¦¬ ë¶„í¬ ì¡°íšŒ"""
    if not data_service.data_loaded:
        raise HTTPException(status_code=404, detail="ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    try:
        # data_serviceì—ì„œ ì¹´í…Œê³ ë¦¬ ë¶„í¬ ê³„ì‚°
        category_distribution = data_service.get_product_category_distribution()
        
        if category_distribution:
            logger.info(f"âœ… ì‹¤ì œ rawdata ê¸°ë°˜ ì¹´í…Œê³ ë¦¬ ë¶„í¬ ë°˜í™˜: {len(category_distribution)}ê°œ ì¹´í…Œê³ ë¦¬")
            return category_distribution
        else:
            # rawdataê°€ ì—†ê±°ë‚˜ ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ê°’
            logger.warning("âš ï¸ ì¹´í…Œê³ ë¦¬ ë¶„í¬ ë°ì´í„° ì—†ìŒ, ê¸°ë³¸ê°’ ë°˜í™˜")
            return [
                {'name': 'ë©´ë¥˜/ë¼ë©´', 'value': 25},
                {'name': 'ìŒë£Œ/ìŒë£Œìˆ˜', 'value': 32},
                {'name': 'ì¡°ë¯¸ë£Œ/ì–‘ë…', 'value': 18},
                {'name': 'ê³¡ë¬¼/ìŒ€', 'value': 15},
                {'name': 'ìŠ¤ë‚µ/ê³¼ì', 'value': 12},
                {'name': 'ê¸°íƒ€', 'value': 8}
            ]
    except Exception as e:
        logger.error(f"âŒ ì¹´í…Œê³ ë¦¬ ë¶„í¬ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ê°’
        return [
            {'name': 'ë©´ë¥˜/ë¼ë©´', 'value': 25},
            {'name': 'ìŒë£Œ/ìŒë£Œìˆ˜', 'value': 32},
            {'name': 'ì¡°ë¯¸ë£Œ/ì–‘ë…', 'value': 18},
            {'name': 'ê³¡ë¬¼/ìŒ€', 'value': 15},
            {'name': 'ìŠ¤ë‚µ/ê³¼ì', 'value': 12},
            {'name': 'ê¸°íƒ€', 'value': 8}
        ]

@app.get("/api/analysis/stats/{df_name}")
async def get_analysis_stats(df_name: str):
    if not data_service.data_loaded:
        raise HTTPException(status_code=404, detail="ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    stats = data_analysis_service.get_descriptive_stats(df_name)
    return stats

@app.get("/api/analysis/daily-movement")
async def get_analysis_daily_movement():
    if not data_service.data_loaded:
        raise HTTPException(status_code=404, detail="ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    # data_analysis_service.get_daily_movement_summary()ëŠ” ì´ì œ DataFrameì„ ë°˜í™˜
    summary_df = data_analysis_service.get_daily_movement_summary()
    return summary_df.to_dict(orient='records') # ë¦¬ìŠ¤íŠ¸ ì˜¤ë¸Œ ë”•íŠ¸ë¡œ ë³€í™˜

@app.get("/api/analysis/product-insights")
async def get_analysis_product_insights():
    if not data_service.data_loaded:
        raise HTTPException(status_code=404, detail="ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    insights = data_analysis_service.get_product_insights()
    return insights

@app.get("/api/analysis/rack-utilization")
async def get_analysis_rack_utilization():
    if not data_service.data_loaded:
        raise HTTPException(status_code=404, detail="ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    summary = data_analysis_service.get_rack_utilization_summary()
    return summary

@app.get("/api/analysis/anomalies")
async def get_anomalies():
    # ì´ìƒ íƒì§€ ë¡œì§ì€ data_analysis_serviceë¡œ ì´ë™
    anomalies_result = await data_analysis_service.detect_anomalies_data()
    if not anomalies_result["anomalies"] and anomalies_result.get("message") and "ì˜¤ë¥˜" in anomalies_result["message"]:
        raise HTTPException(status_code=500, detail=anomalies_result["message"])
    return anomalies_result

class DemandPredictionRequest(BaseModel):
    features: Dict[str, Any] # ì˜ˆì¸¡ì— í•„ìš”í•œ í”¼ì²˜ë¥¼ í´ë¼ì´ì–¸íŠ¸ì—ì„œ ì „ë‹¬í•œë‹¤ê³  ê°€ì •

@app.post("/api/predict/demand")
async def predict_demand(request: DemandPredictionRequest):
    if not data_service.data_loaded:
        raise HTTPException(status_code=404, detail="ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    if not model_trained["demand_predictor"]:
        await train_demand_predictor()
        if not model_trained["demand_predictor"]:
            raise HTTPException(status_code=500, detail="ìˆ˜ìš” ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    
    try:
        # í´ë¼ì´ì–¸íŠ¸ì—ì„œ ë°›ì€ í”¼ì²˜ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
        input_features = pd.DataFrame([request.features])
        prediction = demand_predictor.predict_daily_demand(input_features)
        # ì˜ˆì¸¡ ê²°ê³¼ëŠ” numpy ë°°ì—´ì´ë¯€ë¡œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜
        return {"prediction": prediction.tolist()}
    except Exception as e:
        logger.error(f"ìˆ˜ìš” ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise HTTPException(status_code=500, detail=f"ìˆ˜ìš” ì˜ˆì¸¡ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

@app.post("/api/upload/data")
async def upload_data(file: UploadFile = File(...)):
    try:
        # íŒŒì¼ í™•ì¥ì í™•ì¸
        file_extension = os.path.splitext(file.filename)[1].lower()
        if file_extension not in [".csv", ".xlsx", ".xls"]:
            raise HTTPException(status_code=400, detail="ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. CSV ë˜ëŠ” Excel íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")

        contents = await file.read()
        data_io = io.BytesIO(contents)

        if file_extension == ".csv":
            df = pd.read_csv(data_io)
        else: # .xlsx or .xls
            df = pd.read_excel(data_io)
        
        logger.info(f"Uploaded file: {file.filename}, rows: {len(df)}")

        # ë°ì´í„° ì—…ë¡œë“œ í›„ data_serviceì— ë°˜ì˜í•˜ê³  ëª¨ë¸ ì¬í•™ìŠµ (ì„ íƒ ì‚¬í•­)
        # ì´ ë¶€ë¶„ì€ ë°ì´í„°ì˜ ì„±ê²©ê³¼ í™œìš© ë°©ì‹ì— ë”°ë¼ ë³µì¡ë„ê°€ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        # ì˜ˆë¥¼ ë“¤ì–´, ì…ì¶œê³  ë°ì´í„°ë©´ ê¸°ì¡´ ë°ì´í„°ì— concat, ìƒí’ˆ ë°ì´í„°ë©´ replace ë“±
        # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ ìƒˆë¡œ ë¡œë“œí•˜ëŠ” ê²ƒìœ¼ë¡œ ê°€ì • (ì‹¤ì œ ì„œë¹„ìŠ¤ì—ì„œëŠ” ë°ì´í„° ë³‘í•© ë“± í•„ìš”)
        await data_service.load_all_data(rawdata_path="rawdata") # ë‹¤ì‹œ ëª¨ë“  ë°ì´í„° ë¡œë“œ (ì„ì‹œ)
        model_trained["demand_predictor"] = False # ëª¨ë¸ ì¬í•™ìŠµ í•„ìš”
        model_trained["product_clusterer"] = False # ëª¨ë¸ ì¬í•™ìŠµ í•„ìš”
        model_trained["anomaly_detector"] = False # ëª¨ë¸ ì¬í•™ìŠµ í•„ìš”

        return {"message": f"íŒŒì¼ \'{file.filename}\'ì´ ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤. ì´ {len(df)}ê°œì˜ í–‰ì´ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.", "rows_processed": len(df)}

    except Exception as e:
        logger.error(f"íŒŒì¼ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise HTTPException(status_code=500, detail=f"íŒŒì¼ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

class ChatRequest(BaseModel):
    question: str

@app.post("/api/ai/chat")
async def ai_chat(request: ChatRequest):
    logger.info(f"AI Chat ìš”ì²­ ìˆ˜ì‹ : {request.question}")
    try:
        response_text = await chatbot.process_query(request.question)
        logger.info(f"AI Chat ì‘ë‹µ ìƒì„± ì™„ë£Œ.")
        return {"answer": response_text}
    except Exception as e:
        logger.error(f"AI Chat ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise HTTPException(status_code=500, detail=f"AI Chat ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

@app.post("/api/product/cluster")
async def cluster_products_api():
    """ê¸°ì¡´ API í˜¸í™˜ì„±ì„ ìœ„í•œ ë¦¬ë‹¤ì´ë ‰íŠ¸ (Deprecated)"""
    logger.warning("âš ï¸ Deprecated API í˜¸ì¶œ: /api/product/cluster -> /api/ml/product-clustering/clusters ì‚¬ìš© ê¶Œì¥")
    
    if not data_service.data_loaded:
        raise HTTPException(status_code=404, detail="ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    if not model_trained["product_clusterer"] or not product_cluster_data:
        raise HTTPException(status_code=500, detail="ProductClusterer ëª¨ë¸ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € product_clusterer_trainer.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")

    try:
        # ìƒˆë¡œìš´ API í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ë³€í™˜í•˜ì—¬ ë°˜í™˜
        if product_cluster_data and "cluster_analysis" in product_cluster_data:
            cluster_analysis = product_cluster_data["cluster_analysis"]
            
            # ê¸°ì¡´ API í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ê°„ë‹¨í•œ í´ëŸ¬ìŠ¤í„° ë°°ì—´)
            clusters = []
            for cluster_id, analysis in cluster_analysis.items():
                cluster_num = int(cluster_id.split('_')[1])  # cluster_0 -> 0
                clusters.extend([cluster_num] * analysis["size"])
            
            return {"clusters": clusters}
        else:
            raise HTTPException(status_code=500, detail="í´ëŸ¬ìŠ¤í„° ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
    except Exception as e:
        logger.error(f"ì œí’ˆ í´ëŸ¬ìŠ¤í„°ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise HTTPException(status_code=500, detail=f"ì œí’ˆ í´ëŸ¬ìŠ¤í„°ë§ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# ì°¨íŠ¸ ìƒì„± ìš”ì²­ ëª¨ë¸
class ChartGenerationRequest(BaseModel):
    user_request: str  # ì‚¬ìš©ìì˜ ìì—°ì–´ ì°¨íŠ¸ ìš”ì²­
    context: str = ""  # ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ (ì„ íƒì‚¬í•­)

@app.post("/api/ai/generate-chart")
async def generate_chart(request: ChartGenerationRequest):
    """LLMì„ í™œìš©í•œ ì°¨íŠ¸ ì„¤ì • ìƒì„± API"""
    if not data_service.data_loaded:
        raise HTTPException(status_code=404, detail="ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    logger.info(f"ì°¨íŠ¸ ìƒì„± ìš”ì²­: {request.user_request}")
    
    try:
        # ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ë°ì´í„° ê²€ìƒ‰
        vector_search_result = await vector_db_service.search_relevant_data(
            query=request.user_request,
            n_results=20
        )
        
        # ê²€ìƒ‰ëœ ì‹¤ì œ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ ë©”íƒ€ë°ì´í„° ì‚¬ìš©
        if vector_search_result.get("success") and vector_search_result.get("chart_data"):
            # ì‹¤ì œ ë°ì´í„°ë¡œ ì°¨íŠ¸ ì„¤ì • ìƒì„±
            chart_result = await _generate_chart_from_real_data(
                user_request=request.user_request,
                search_result=vector_search_result
            )
        else:
            # ê¸°ì¡´ ë°©ì‹: ë©”íƒ€ë°ì´í„°ë¡œ AI ìƒì„±
            available_data = await _prepare_available_data_info()
            chart_result = await ai_service.generate_chart_config(
                user_request=request.user_request,
                available_data=available_data
            )
        
        if chart_result["success"]:
            logger.info(f"ì°¨íŠ¸ ì„¤ì • ìƒì„± ì„±ê³µ: {chart_result['chart_config']['chart_type']}")
            return {
                "success": True,
                "chart_config": chart_result["chart_config"],
                "message": chart_result["message"]
            }
        else:
            logger.warning(f"ì°¨íŠ¸ ì„¤ì • ìƒì„± ì‹¤íŒ¨, ëŒ€ì²´ ì„¤ì • ì‚¬ìš©: {chart_result['error']}")
            return {
                "success": False,
                "chart_config": chart_result["fallback_config"],
                "message": chart_result["message"],
                "error": chart_result["error"]
            }
            
    except Exception as e:
        logger.error(f"ì°¨íŠ¸ ìƒì„± API ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise HTTPException(status_code=500, detail=f"ì°¨íŠ¸ ìƒì„± ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

async def _prepare_available_data_info() -> dict:
    """ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„° ì •ë³´ë¥¼ ì •ë¦¬í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        available_data = {}
        
        # ì…ê³  ë°ì´í„° ì •ë³´
        if data_service.inbound_data is not None and not data_service.inbound_data.empty:
            available_data["inbound_data"] = {
                "description": "ì…ê³  ë°ì´í„° (ê³µê¸‰ì—…ì²´ë³„ ìƒí’ˆ ì…ê³  ì •ë³´)",
                "columns": list(data_service.inbound_data.columns),
                "row_count": len(data_service.inbound_data),
                "date_range": _get_date_range(data_service.inbound_data, 'Date')
            }
        
        # ì¶œê³  ë°ì´í„° ì •ë³´
        if data_service.outbound_data is not None and not data_service.outbound_data.empty:
            available_data["outbound_data"] = {
                "description": "ì¶œê³  ë°ì´í„° (ê³ ê°ì‚¬ë³„ ìƒí’ˆ ì¶œê³  ì •ë³´)",
                "columns": list(data_service.outbound_data.columns),
                "row_count": len(data_service.outbound_data),
                "date_range": _get_date_range(data_service.outbound_data, 'Date')
            }
        
        # ìƒí’ˆ ë§ˆìŠ¤í„° ë°ì´í„° ì •ë³´
        if data_service.product_master is not None and not data_service.product_master.empty:
            available_data["product_master"] = {
                "description": "ìƒí’ˆ ë§ˆìŠ¤í„° ë°ì´í„° (ìƒí’ˆë³„ ê¸°ë³¸ ì •ë³´ ë° ì¬ê³ )",
                "columns": list(data_service.product_master.columns),
                "row_count": len(data_service.product_master)
            }
        
        # ê¸°ë³¸ KPI ì •ë³´ ì¶”ê°€
        available_data["kpi_metrics"] = {
            "description": "ê³„ì‚° ê°€ëŠ¥í•œ KPI ì§€í‘œë“¤",
            "metrics": [
                "ì¼ë³„ ì…ê³ ëŸ‰/ì¶œê³ ëŸ‰",
                "ë™ë³„ ì¬ê³  í˜„í™©",
                "ìƒí’ˆë³„ íšŒì „ìœ¨",
                "ê³µê¸‰ì—…ì²´ë³„ ì…ê³  í˜„í™©",
                "ê³ ê°ì‚¬ë³„ ì¶œê³  í˜„í™©",
                "ì¬ê³  ìˆ˜ì¤€ ë¶„ì„"
            ]
        }
        
        return available_data
        
    except Exception as e:
        logger.error(f"ë°ì´í„° ì •ë³´ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
        return {"error": f"ë°ì´í„° ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}"}

def _get_date_range(df, date_column):
    """ë°ì´í„°í”„ë ˆì„ì—ì„œ ë‚ ì§œ ë²”ìœ„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        if date_column in df.columns:
            dates = pd.to_datetime(df[date_column], errors='coerce')
            min_date = dates.min()
            max_date = dates.max()
            if pd.notna(min_date) and pd.notna(max_date):
                return f"{min_date.strftime('%Y-%m-%d')} ~ {max_date.strftime('%Y-%m-%d')}"
        return "ë‚ ì§œ ì •ë³´ ì—†ìŒ"
    except Exception:
        return "ë‚ ì§œ ì •ë³´ íŒŒì‹± ì‹¤íŒ¨"

async def _generate_chart_from_real_data(user_request: str, search_result: Dict[str, Any]) -> Dict[str, Any]:
    """ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ë¡œ ì‹¤ì œ ì°¨íŠ¸ ì„¤ì • ìƒì„±"""
    try:
        chart_data = search_result.get("chart_data", {})
        
        if not chart_data.get("labels") or not chart_data.get("data"):
            raise ValueError("ê²€ìƒ‰ëœ ë°ì´í„°ì— ì°¨íŠ¸ ìƒì„±ì— í•„ìš”í•œ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # ì‚¬ìš©ì ìš”ì²­ì—ì„œ ì°¨íŠ¸ íƒ€ì… ì¶”ì •
        chart_type = _infer_chart_type_from_request(user_request)
        
        # ìƒ‰ìƒ íŒ”ë ˆíŠ¸
        colors = [
            "#3b82f6", "#10b981", "#f59e0b", "#ef4444", "#8b5cf6",
            "#06b6d4", "#84cc16", "#f97316", "#ec4899", "#6b7280"
        ]
        
        # Chart.js í˜¸í™˜ ì„¤ì • ìƒì„±
        chart_config = {
            "chart_type": chart_type,
            "title": chart_data.get("title", "ë°ì´í„° ì°¨íŠ¸"),
            "data": {
                "labels": chart_data["labels"][:10],  # ìµœëŒ€ 10ê°œê¹Œì§€ë§Œ í‘œì‹œ
                "datasets": [{
                    "label": chart_data.get("title", "ë°ì´í„°"),
                    "data": chart_data["data"][:10],
                    "backgroundColor": colors[:len(chart_data["data"][:10])],
                    "borderColor": colors[0],
                    "borderWidth": 2 if chart_type == "line" else 1,
                    "tension": 0.3 if chart_type == "line" else 0
                }]
            },
            "options": {
                "responsive": True,
                "plugins": {
                    "title": {
                        "display": True,
                        "text": chart_data.get("title", "ë°ì´í„° ì°¨íŠ¸"),
                        "font": {"size": 16, "weight": "bold"}
                    },
                    "legend": {
                        "display": True,
                        "position": "top"
                    }
                },
                "scales": {} if chart_type in ["pie", "doughnut"] else {
                    "y": {
                        "beginAtZero": True,
                        "title": {
                            "display": True,
                            "text": "ìˆ˜ëŸ‰"
                        }
                    },
                    "x": {
                        "title": {
                            "display": True,
                            "text": "í•­ëª©"
                        }
                    }
                }
            },
            "query_info": {
                "data_source": f"ì‹¤ì œ ë°ì´í„° ê²€ìƒ‰ ({search_result.get('found_documents', 0)}ê°œ ë¬¸ì„œ)",
                "search_query": user_request,
                "data_type": chart_data.get("type", "unknown")
            }
        }
        
        logger.info(f"âœ… ì‹¤ì œ ë°ì´í„°ë¡œ ì°¨íŠ¸ ì„¤ì • ìƒì„±: {chart_type} - {chart_data.get('title')}")
        
        return {
            "success": True,
            "chart_config": chart_config,
            "message": f"ì‹¤ì œ ë°ì´í„° {search_result.get('found_documents', 0)}ê°œ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì°¨íŠ¸ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.",
            "data_source": "vector_database"
        }
        
    except Exception as e:
        logger.error(f"âŒ ì‹¤ì œ ë°ì´í„° ì°¨íŠ¸ ìƒì„± ì‹¤íŒ¨: {str(e)}")
        return {
            "success": False,
            "error": str(e),
            "message": "ì‹¤ì œ ë°ì´í„°ë¡œ ì°¨íŠ¸ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. AI ìƒì„±ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.",
            "fallback_config": None
        }

def _infer_chart_type_from_request(user_request: str) -> str:
    """ì‚¬ìš©ì ìš”ì²­ì—ì„œ ì°¨íŠ¸ íƒ€ì… ì¶”ì •"""
    request_lower = user_request.lower()
    
    if any(word in request_lower for word in ['íŒŒì´ì°¨íŠ¸', 'pie', 'ì›ê·¸ë˜í”„', 'ë„ë„›']):
        return "doughnut" if 'ë„ë„›' in request_lower else "pie"
    elif any(word in request_lower for word in ['ì„ ê·¸ë˜í”„', 'line', 'ì¶”ì´', 'íŠ¸ë Œë“œ', 'ë³€í™”']):
        return "line"
    elif any(word in request_lower for word in ['ë§‰ëŒ€ì°¨íŠ¸', 'bar', 'ë§‰ëŒ€ê·¸ë˜í”„', 'ë¹„êµ']):
        return "bar"
    elif any(word in request_lower for word in ['ì‚°ì ë„', 'scatter', 'ë¶„í¬']):
        return "scatter"
    else:
        # ê¸°ë³¸ê°’: ë§‰ëŒ€ì°¨íŠ¸
        return "bar"

@app.get("/api/vector-db/status")
async def get_vector_db_status():
    """ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ í™•ì¸"""
    try:
        status = vector_db_service.get_status()
        return status
    except Exception as e:
        logger.error(f"ë²¡í„° DB ìƒíƒœ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
        return {
            "status": "error",
            "error": str(e),
            "message": "ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        }

@app.post("/api/vector-db/reindex")
async def reindex_vector_db():
    """ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì¬ì¸ë±ì‹±"""
    if not data_service.data_loaded:
        raise HTTPException(status_code=404, detail="ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    try:
        logger.info("ğŸ”„ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì¬ì¸ë±ì‹± ì‹œì‘...")
        success = await vector_db_service.index_warehouse_data(force_rebuild=True)
        
        if success:
            return {
                "success": True,
                "message": "ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì¬ì¸ë±ì‹±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
                "status": vector_db_service.get_status()
            }
        else:
            raise HTTPException(status_code=500, detail="ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì¬ì¸ë±ì‹±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            
    except Exception as e:
        logger.error(f"ë²¡í„° DB ì¬ì¸ë±ì‹± ì¤‘ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"ì¬ì¸ë±ì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@app.post("/api/cad/upload")
async def upload_cad_file(file: UploadFile = File(...)):
    """DWG/DXF CAD íŒŒì¼ ì—…ë¡œë“œ ë° ë¶„ì„"""
    logger.info(f"CAD íŒŒì¼ ì—…ë¡œë“œ ìš”ì²­: {file.filename}")
    
    try:
        # íŒŒì¼ ê²€ì¦
        if not file.filename:
            raise HTTPException(status_code=400, detail="íŒŒì¼ëª…ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        # ì§€ì›í•˜ëŠ” íŒŒì¼ í™•ì¥ì í™•ì¸
        allowed_extensions = {'.dwg', '.dxf', '.dwf'}
        file_extension = os.path.splitext(file.filename)[1].lower()
        
        if file_extension not in allowed_extensions:
            raise HTTPException(
                status_code=400, 
                detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. ì§€ì› í˜•ì‹: {', '.join(allowed_extensions)}"
            )
        
        # íŒŒì¼ í¬ê¸° í™•ì¸ (50MB ì œí•œ)
        max_size = 50 * 1024 * 1024  # 50MB
        file_content = await file.read()
        if len(file_content) > max_size:
            raise HTTPException(
                status_code=400, 
                detail=f"íŒŒì¼ í¬ê¸°ê°€ ë„ˆë¬´ í½ë‹ˆë‹¤. ìµœëŒ€ í¬ê¸°: 50MB, í˜„ì¬ í¬ê¸°: {len(file_content) / (1024*1024):.1f}MB"
            )
        
        # ì„ì‹œ íŒŒì¼ ì €ì¥
        temp_dir = "backend/cad_uploads"
        os.makedirs(temp_dir, exist_ok=True)
        
        file_id = datetime.now().strftime("%Y%m%d_%H%M%S")
        temp_filename = f"{file_id}_{file.filename}"
        temp_filepath = os.path.join(temp_dir, temp_filename)
        
        with open(temp_filepath, "wb") as temp_file:
            temp_file.write(file_content)
        
        logger.info(f"ì„ì‹œ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {temp_filepath}")
        
        # CAD íŒŒì¼ ì²˜ë¦¬
        result = await cad_service.process_cad_file(temp_filepath, file.filename)
        
        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        try:
            os.remove(temp_filepath)
        except Exception as cleanup_error:
            logger.warning(f"ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì˜¤ë¥˜: {cleanup_error}")
        
        if result["success"]:
            logger.info(f"CAD íŒŒì¼ ì²˜ë¦¬ ì„±ê³µ: {file.filename}")
            return {
                "success": True,
                "message": f"íŒŒì¼ '{file.filename}'ì´ ì„±ê³µì ìœ¼ë¡œ ë¶„ì„ë˜ì—ˆìŠµë‹ˆë‹¤.",
                "data": result["data"],
                "processing_method": result.get("processing_method"),
                "file_info": {
                    "filename": result["filename"],
                    "size": result["file_size"],
                    "type": result["file_type"]
                }
            }
        else:
            logger.error(f"CAD íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {result['error']}")
            raise HTTPException(status_code=500, detail=result["error"])
    
    except HTTPException:
        # HTTP ì˜ˆì™¸ëŠ” ê·¸ëŒ€ë¡œ ì¬ë°œìƒ
        raise
    except Exception as e:
        logger.error(f"CAD íŒŒì¼ ì—…ë¡œë“œ ì²˜ë¦¬ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(status_code=500, detail=f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

@app.get("/api/cad/status")
async def get_cad_status():
    """CAD ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
    try:
        # í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸
        libraries_status = {
            "ezdxf": False,
            "pillow": False,
            "opencv": False
        }
        
        try:
            import ezdxf
            libraries_status["ezdxf"] = True
        except ImportError:
            pass
        
        try:
            from PIL import Image
            libraries_status["pillow"] = True
        except ImportError:
            pass
        
        try:
            import cv2
            libraries_status["opencv"] = True
        except ImportError:
            pass
        
        # ì—…ë¡œë“œ ë””ë ‰í† ë¦¬ ìƒíƒœ
        upload_dir = "backend/cad_uploads"
        upload_dir_exists = os.path.exists(upload_dir)
        upload_dir_writable = os.access(upload_dir, os.W_OK) if upload_dir_exists else False
        
        return {
            "service_available": True,
            "libraries": libraries_status,
            "upload_directory": {
                "exists": upload_dir_exists,
                "writable": upload_dir_writable,
                "path": upload_dir
            },
            "supported_formats": [".dwg", ".dxf", ".dwf"],
            "max_file_size_mb": 50,
            "ai_service_available": ai_service is not None
        }
        
    except Exception as e:
        logger.error(f"CAD ìƒíƒœ í™•ì¸ ì˜¤ë¥˜: {str(e)}")
        return {
            "service_available": False,
            "error": str(e)
        }

@app.get("/api/warehouse/racks/{rack_id}/stock")
async def get_rack_stock(rack_id: str):
    """íŠ¹ì • ë™ì˜ ì¬ê³  ì •ë³´ ì¡°íšŒ"""
    try:
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¡°íšŒ
        # í˜„ì¬ëŠ” ëª¨ì˜ ë°ì´í„° ë°˜í™˜
        import random
        
        mock_data = {
            "rack_id": rack_id,
            "currentStock": random.randint(10, 150),
            "capacity": random.randint(100, 200),
            "last_updated": "2025-01-20T10:30:00Z",
            "status": "active"
        }
        
        logger.info(f"ë™ {rack_id} ì¬ê³  ì •ë³´ ì¡°íšŒ")
        return mock_data
        
    except Exception as e:
        logger.error(f"ë™ ì¬ê³  ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"ì¬ê³  ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

@app.get("/api/warehouse/data/current")
async def get_current_warehouse_data():
    """í˜„ì¬ ì°½ê³  ì „ì²´ ë°ì´í„° ì¡°íšŒ"""
    try:
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” data_serviceì™€ vector_db_serviceì—ì„œ ë°ì´í„° ì¡°íšŒ
        current_data = data_service.get_current_summary()
        
        # ì¬ê³  ë°ì´í„° í¬í•¨
        warehouse_data = {
            "timestamp": "2025-01-20T10:30:00Z",
            "total_racks": 5,
            "inventory": [
                {"location": "A", "product_name": "ì œí’ˆA", "quantity": 75},
                {"location": "B", "product_name": "ì œí’ˆB", "quantity": 90},
                {"location": "C", "product_name": "ì œí’ˆC", "quantity": 60},
                {"location": "D", "product_name": "ì œí’ˆD", "quantity": 120},
                {"location": "E", "product_name": "ì œí’ˆE", "quantity": 85},
            ],
            "summary": current_data
        }
        
        logger.info("í˜„ì¬ ì°½ê³  ë°ì´í„° ì¡°íšŒ ì™„ë£Œ")
        return warehouse_data
        
    except Exception as e:
        logger.error(f"ì°½ê³  ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

# =============================================================================
# ProductClusterer API ì—”ë“œí¬ì¸íŠ¸ë“¤
# =============================================================================

@app.get("/api/ml/product-clustering/status")
async def get_product_clustering_status():
    """ProductClusterer ëª¨ë¸ ìƒíƒœ í™•ì¸"""
    try:
        # ë””ë²„ê¹… ì •ë³´ ì¶”ê°€
        logger.info(f"ìƒíƒœ í™•ì¸ ìš”ì²­ - ëª¨ë¸ í›ˆë ¨: {model_trained['product_clusterer']}, ë°ì´í„° ì¡´ì¬: {product_cluster_data is not None}")
        
        status = {
            "model_trained": model_trained["product_clusterer"],
            "model_available": product_cluster_data is not None,
            "data_loaded": data_service.data_loaded,
            "debug_info": {
                "product_cluster_data_keys": list(product_cluster_data.keys()) if product_cluster_data else [],
                "model_trained_status": model_trained
            }
        }
        
        if product_cluster_data:
            model_info = product_cluster_data.get("model_info", {})
            cluster_analysis = product_cluster_data.get("cluster_analysis", {})
            
            status.update({
                "model_type": model_info.get("model_type"),
                "n_clusters": model_info.get("n_clusters"),
                "trained_at": model_info.get("trained_at"),
                "total_products": sum(cluster["size"] for cluster in cluster_analysis.values()) if cluster_analysis else 0,
                "available_clusters": list(cluster_analysis.keys()) if cluster_analysis else []
            })
        else:
            logger.warning("product_cluster_dataê°€ Noneì…ë‹ˆë‹¤. ëª¨ë¸ ì¬ë¡œë“œê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        return status
        
    except Exception as e:
        logger.error(f"ProductClusterer ìƒíƒœ í™•ì¸ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {str(e)}")

@app.get("/api/ml/product-clustering/clusters")
async def get_product_clusters():
    """ì œí’ˆ í´ëŸ¬ìŠ¤í„° ë¶„ì„ ê²°ê³¼ ì¡°íšŒ"""
    logger.info(f"í´ëŸ¬ìŠ¤í„° ëª©ë¡ ìš”ì²­ - ëª¨ë¸ í›ˆë ¨: {model_trained['product_clusterer']}, ë°ì´í„° ì¡´ì¬: {product_cluster_data is not None}")
    
    if not model_trained["product_clusterer"]:
        raise HTTPException(status_code=404, detail="ProductClusterer ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    if not product_cluster_data:
        # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì¬ë¡œë“œ ì‹œë„
        logger.warning("í´ëŸ¬ìŠ¤í„° ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì¬ë¡œë“œ ì‹œë„...")
        await train_product_clusterer()
        
        if not product_cluster_data:
            raise HTTPException(status_code=404, detail="í´ëŸ¬ìŠ¤í„° ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. product_clusterer_trainer.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
    
    try:
        cluster_analysis = product_cluster_data["cluster_analysis"]
        cluster_interpretations = product_cluster_data["cluster_interpretations"]
        
        # í´ëŸ¬ìŠ¤í„°ë³„ ìš”ì•½ ì •ë³´ ìƒì„±
        clusters_summary = []
        for cluster_id, analysis in cluster_analysis.items():
            interpretation = cluster_interpretations.get(cluster_id, {})
            
            summary = {
                "cluster_id": cluster_id,
                "cluster_name": interpretation.get("type", "ì•Œ ìˆ˜ ì—†ìŒ"),
                "size": analysis["size"],
                "percentage": round(analysis["percentage"], 1),
                "strategy": interpretation.get("strategy", "í‘œì¤€ ê´€ë¦¬"),
                "color": interpretation.get("color", "gray"),
                "metrics": interpretation.get("metrics", {}),
                "key_products": analysis.get("key_products", [])[:3],  # ìƒìœ„ 3ê°œë§Œ
                "characteristics": analysis.get("characteristics", {})
            }
            clusters_summary.append(summary)
        
        # í¬ê¸° ìˆœìœ¼ë¡œ ì •ë ¬
        clusters_summary.sort(key=lambda x: x["size"], reverse=True)
        
        return {
            "clusters": clusters_summary,
            "model_info": product_cluster_data["model_info"],
            "total_products": sum(c["size"] for c in clusters_summary)
        }
        
    except Exception as e:
        logger.error(f"í´ëŸ¬ìŠ¤í„° ì¡°íšŒ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"í´ëŸ¬ìŠ¤í„° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

@app.get("/api/ml/product-clustering/cluster/{cluster_id}")
async def get_cluster_details(cluster_id: str):
    """íŠ¹ì • í´ëŸ¬ìŠ¤í„°ì˜ ìƒì„¸ ì •ë³´ ì¡°íšŒ"""
    # ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€
    logger.info(f"í´ëŸ¬ìŠ¤í„° ìƒì„¸ ì¡°íšŒ ìš”ì²­: {cluster_id}")
    logger.info(f"ëª¨ë¸ í›ˆë ¨ ìƒíƒœ: {model_trained['product_clusterer']}")
    logger.info(f"í´ëŸ¬ìŠ¤í„° ë°ì´í„° ì¡´ì¬: {product_cluster_data is not None}")
    
    if not model_trained["product_clusterer"]:
        raise HTTPException(status_code=404, detail="ProductClusterer ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    if not product_cluster_data:
        # ë°ì´í„°ê°€ ì—†ë‹¤ë©´ ë‹¤ì‹œ ë¡œë“œ ì‹œë„
        logger.warning("í´ëŸ¬ìŠ¤í„° ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ë¡œë“œ ì‹œë„...")
        await train_product_clusterer()
        
        if not product_cluster_data:
            raise HTTPException(status_code=404, detail="í´ëŸ¬ìŠ¤í„° ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. product_clusterer_trainer.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
    
    try:
        cluster_analysis = product_cluster_data["cluster_analysis"]
        cluster_interpretations = product_cluster_data["cluster_interpretations"]
        
        logger.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ í´ëŸ¬ìŠ¤í„° IDë“¤: {list(cluster_analysis.keys())}")
        
        if cluster_id not in cluster_analysis:
            raise HTTPException(status_code=404, detail=f"í´ëŸ¬ìŠ¤í„° '{cluster_id}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ í´ëŸ¬ìŠ¤í„°: {list(cluster_analysis.keys())}")
        
        analysis = cluster_analysis[cluster_id]
        interpretation = cluster_interpretations.get(cluster_id, {})
        
        return {
            "cluster_id": cluster_id,
            "cluster_name": interpretation.get("type", "ì•Œ ìˆ˜ ì—†ìŒ"),
            "size": analysis["size"],
            "percentage": round(analysis["percentage"], 1),
            "strategy": interpretation.get("strategy", "í‘œì¤€ ê´€ë¦¬"),
            "color": interpretation.get("color", "gray"),
            "metrics": interpretation.get("metrics", {}),
            "characteristics": analysis.get("characteristics", {}),
            "all_products": analysis.get("key_products", [])  # ëª¨ë“  ì£¼ìš” ìƒí’ˆ
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"í´ëŸ¬ìŠ¤í„° ìƒì„¸ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"í´ëŸ¬ìŠ¤í„° ìƒì„¸ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

@app.get("/api/ml/product-clustering/product/{product_code}")
async def get_product_cluster_info(product_code: str):
    """íŠ¹ì • ìƒí’ˆì˜ í´ëŸ¬ìŠ¤í„° ì •ë³´ ì¡°íšŒ"""
    if not model_trained["product_clusterer"] or not product_cluster_data:
        raise HTTPException(status_code=404, detail="ProductClusterer ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    try:
        # integrated_warehouse_data.jsonì—ì„œ í•´ë‹¹ ìƒí’ˆ ì°¾ê¸°
        with open("integrated_warehouse_data.json", 'r', encoding='utf-8') as f:
            warehouse_data = json.load(f)
        
        products = warehouse_data['inventory_analysis']['products']
        target_product = None
        
        for product in products:
            if product['product_code'] == product_code:
                target_product = product
                break
        
        if not target_product:
            raise HTTPException(status_code=404, detail=f"ìƒí’ˆ ì½”ë“œ '{product_code}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # í•´ë‹¹ ìƒí’ˆì´ ì†í•œ í´ëŸ¬ìŠ¤í„° ì°¾ê¸° (íŠ¹ì§• ê¸°ë°˜ ì˜ˆì¸¡)
        # ì‹¤ì œë¡œëŠ” íŠ¹ì§• ì¶”ì¶œ í›„ ëª¨ë¸ë¡œ ì˜ˆì¸¡í•´ì•¼ í•˜ì§€ë§Œ, í˜„ì¬ëŠ” í´ëŸ¬ìŠ¤í„° ê²°ê³¼ì—ì„œ ê²€ìƒ‰
        product_cluster = None
        cluster_analysis = product_cluster_data["cluster_analysis"]
        
        for cluster_id, analysis in cluster_analysis.items():
            for key_product in analysis.get("key_products", []):
                if key_product.get("product_code") == product_code:
                    product_cluster = cluster_id
                    break
            if product_cluster:
                break
        
        if not product_cluster:
            # ê¸°ë³¸ê°’ìœ¼ë¡œ ê°€ì¥ í° í´ëŸ¬ìŠ¤í„° í• ë‹¹
            largest_cluster = max(cluster_analysis.keys(), 
                                key=lambda x: cluster_analysis[x]["size"])
            product_cluster = largest_cluster
        
        cluster_interpretation = product_cluster_data["cluster_interpretations"].get(product_cluster, {})
        
        return {
            "product_code": product_code,
            "product_name": target_product.get("product_name", "ì•Œ ìˆ˜ ì—†ìŒ"),
            "cluster_id": product_cluster,
            "cluster_name": cluster_interpretation.get("type", "ì¼ë°˜ ìƒí’ˆ"),
            "strategy": cluster_interpretation.get("strategy", "í‘œì¤€ ê´€ë¦¬ ì •ì±… ì ìš©"),
            "color": cluster_interpretation.get("color", "gray"),
            "product_metrics": {
                "turnover_ratio": target_product.get("turnover_ratio", 0),
                "current_stock": target_product.get("current_stock", 0),
                "rack_name": target_product.get("rack_name", ""),
                "stock_status": target_product.get("stock_status", "ì•Œ ìˆ˜ ì—†ìŒ")
            },
            "cluster_characteristics": cluster_analysis[product_cluster]["characteristics"]
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ìƒí’ˆ í´ëŸ¬ìŠ¤í„° ì •ë³´ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"ìƒí’ˆ í´ëŸ¬ìŠ¤í„° ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

@app.get("/api/ml/product-clustering/high-turnover")
async def get_high_turnover_products():
    """ê³ íšŒì „ ìƒí’ˆ ëª©ë¡ ì¡°íšŒ (í”„ë¦¬ë¯¸ì—„ ê³ íšŒì „ í´ëŸ¬ìŠ¤í„°)"""
    if not model_trained["product_clusterer"] or not product_cluster_data:
        raise HTTPException(status_code=404, detail="ProductClusterer ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    try:
        cluster_analysis = product_cluster_data["cluster_analysis"]
        cluster_interpretations = product_cluster_data["cluster_interpretations"]
        
        # í”„ë¦¬ë¯¸ì—„ ê³ íšŒì „ í´ëŸ¬ìŠ¤í„°ë“¤ ì°¾ê¸°
        high_turnover_clusters = []
        for cluster_id, interpretation in cluster_interpretations.items():
            if "í”„ë¦¬ë¯¸ì—„ ê³ íšŒì „" in interpretation.get("type", ""):
                high_turnover_clusters.append(cluster_id)
        
        high_turnover_products = []
        for cluster_id in high_turnover_clusters:
            cluster_info = cluster_analysis[cluster_id]
            interpretation = cluster_interpretations[cluster_id]
            
            for product in cluster_info.get("key_products", []):
                product_info = {
                    **product,
                    "cluster_id": cluster_id,
                    "cluster_name": interpretation.get("type", "í”„ë¦¬ë¯¸ì—„ ê³ íšŒì „"),
                    "strategy": interpretation.get("strategy", "ìµœìš°ì„  ê´€ë¦¬")
                }
                high_turnover_products.append(product_info)
        
        # íšŒì „ìœ¨ ìˆœìœ¼ë¡œ ì •ë ¬
        high_turnover_products.sort(key=lambda x: x.get("turnover_ratio", 0), reverse=True)
        
        return {
            "high_turnover_products": high_turnover_products,
            "total_count": len(high_turnover_products),
            "clusters_included": high_turnover_clusters,
            "message": "ì½”ì¹´ì½œë¼ì œë¡œìº” ë“± ê³ íšŒì „ ìƒí’ˆë“¤ì˜ ì¬ê³  ê´€ë¦¬ë¥¼ ìµœìš°ì„ ìœ¼ë¡œ í•˜ì„¸ìš”."
        }
        
    except Exception as e:
        logger.error(f"ê³ íšŒì „ ìƒí’ˆ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"ê³ íšŒì „ ìƒí’ˆ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

@app.post("/api/ml/product-clustering/reload")
async def reload_product_clusterer():
    """ProductClusterer ëª¨ë¸ ìˆ˜ë™ ì¬ë¡œë“œ"""
    global product_cluster_data  # í•¨ìˆ˜ ë§¨ ì²˜ìŒì— global ì„ ì–¸
    
    try:
        logger.info("ğŸ”„ ProductClusterer ìˆ˜ë™ ì¬ë¡œë“œ ì‹œì‘...")
        
        # ê¸°ì¡´ ìƒíƒœ ë¦¬ì…‹
        model_trained["product_clusterer"] = False
        product_cluster_data = None
        
        # ëª¨ë¸ ì¬ë¡œë“œ ì‹œë„
        await train_product_clusterer()
        
        if model_trained["product_clusterer"] and product_cluster_data:
            return {
                "success": True,
                "message": "ProductClusterer ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ì¬ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.",
                "model_info": product_cluster_data.get("model_info", {}),
                "available_clusters": list(product_cluster_data.get("cluster_analysis", {}).keys()),
                "total_products": sum(cluster["size"] for cluster in product_cluster_data.get("cluster_analysis", {}).values())
            }
        else:
            return {
                "success": False,
                "message": "ëª¨ë¸ ì¬ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
                "model_trained": model_trained["product_clusterer"],
                "data_available": product_cluster_data is not None
            }
            
    except Exception as e:
        logger.error(f"ProductClusterer ì¬ë¡œë“œ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"ì¬ë¡œë“œ ì‹¤íŒ¨: {str(e)}")

@app.post("/api/ml/product-clustering/retrain")
async def retrain_product_clusterer():
    """ProductClusterer ëª¨ë¸ ì¬í›ˆë ¨"""
    global product_cluster_data  # í•¨ìˆ˜ ë§¨ ì²˜ìŒì— global ì„ ì–¸
    
    if not data_service.data_loaded:
        raise HTTPException(status_code=404, detail="ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    try:
        logger.info("ğŸ”„ ProductClusterer ì¬í›ˆë ¨ ì‹œì‘...")
        
        # ê¸°ì¡´ ëª¨ë¸ ìƒíƒœ ë¦¬ì…‹
        model_trained["product_clusterer"] = False
        product_cluster_data = None
        
        # ì¬í›ˆë ¨ ì‹¤í–‰ (product_clusterer_trainer.py ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰)
        import subprocess
        import sys
        
        result = subprocess.run([
            sys.executable, "product_clusterer_trainer.py"
        ], capture_output=True, text=True, cwd=".")
        
        if result.returncode == 0:
            # ì¬í›ˆë ¨ ì„±ê³µ ì‹œ ëª¨ë¸ ë‹¤ì‹œ ë¡œë“œ
            await train_product_clusterer()
            
            if model_trained["product_clusterer"]:
                return {
                    "success": True,
                    "message": "ProductClusterer ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ì¬í›ˆë ¨ë˜ì—ˆìŠµë‹ˆë‹¤.",
                    "model_info": product_cluster_data.get("model_info", {}) if product_cluster_data else {},
                    "training_output": result.stdout
                }
            else:
                raise HTTPException(status_code=500, detail="ëª¨ë¸ ì¬ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        else:
            logger.error(f"ì¬í›ˆë ¨ ì‹¤íŒ¨: {result.stderr}")
            raise HTTPException(status_code=500, detail=f"ì¬í›ˆë ¨ ì‹¤íŒ¨: {result.stderr}")
        
    except Exception as e:
        logger.error(f"ProductClusterer ì¬í›ˆë ¨ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"ì¬í›ˆë ¨ ì‹¤íŒ¨: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    
    print("ğŸš€ VSS ìŠ¤ë§ˆíŠ¸ ì°½ê³  ê´€ë¦¬ ì‹œìŠ¤í…œì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    print("ğŸ“ ì„œë²„ ì£¼ì†Œ: http://localhost:8000")
    print("ğŸ’» ëŒ€ì‹œë³´ë“œ: http://localhost:8000")
    print("ğŸ”§ API ë¬¸ì„œ: http://localhost:8000/docs")
    print("=" * 50)
    
    try:
        uvicorn.run(
            app,
            host="0.0.0.0",
            port=8000,
            reload=False,  # í”„ë¡œë•ì…˜ ëª¨ë“œ
            log_level="info"
        )
    except KeyboardInterrupt:
        print("\nâœ… ì„œë²„ê°€ ì •ìƒì ìœ¼ë¡œ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ ì„œë²„ ì‹œì‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        exit(1)
 