import os
import logging
import time
import asyncio
import random
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, field
from collections import deque
from threading import Lock

import google.generativeai as genai

from dotenv import load_dotenv
load_dotenv()
os.environ['GEMINI_API_KEY_1'] = os.getenv('GEMINI_API_KEY_1')
os.environ['GEMINI_API_KEY_2'] = os.getenv('GEMINI_API_KEY_2')
os.environ['GEMINI_API_KEY_3'] = os.getenv('GEMINI_API_KEY_3')
os.environ['GEMINI_API_KEY_4'] = os.getenv('GEMINI_API_KEY_4')

# AI ëª¨ë¸ ì„¤ì • (legacy/crad_lcrag/utils/ai_model_manager.py ì°¸ì¡°)
AI_MODEL_CONFIG = {
    "temperature": 0.1,
    "top_p": 0.8,
    "top_k": 40,
    "max_output_tokens": 2048,
}

@dataclass
class APIKeyStatus:
    """API í‚¤ ìƒíƒœ ì •ë³´ (gemini_client.py ì°¸ì¡°)"""
    key: str
    is_active: bool = True
    request_count: int = 0
    last_request_time: float = 0.0
    error_count: int = 0
    success_count: int = 0
    
    @property
    def success_rate(self) -> float:
        total = self.success_count + self.error_count
        return self.success_count / total if total > 0 else 0.0

@dataclass
class RateLimitConfig:
    """ìš”ì²­ ì œí•œ ì„¤ì • (rate_limiter.py ì°¸ì¡°)"""
    rpm_limit: int = 2000  # ë¶„ë‹¹ ìš”ì²­ ì œí•œ
    tpm_limit: int = 4000000  # ë¶„ë‹¹ í† í° ì œí•œ
    burst_limit: int = 100  # ë²„ìŠ¤íŠ¸ í—ˆìš© í•œë„
    window_size: int = 60  # ì‹œê°„ ìœˆë„ìš° (ì´ˆ)

@dataclass
class TokenBucket:
    """í† í° ë²„í‚· (rate_limiter.py ì°¸ì¡°)"""
    capacity: int
    tokens: float = field(default_factory=lambda: 0)
    last_refill: float = field(default_factory=time.time)
    refill_rate: float = field(default=1.0)  # ì´ˆë‹¹ í† í° ë³´ì¶©ë¥ 
    
    def __post_init__(self):
        self.tokens = self.capacity
        self.refill_rate = self.capacity / 60.0  # ë¶„ë‹¹ ìš©ëŸ‰ì„ ì´ˆë‹¹ìœ¼ë¡œ ë³€í™˜

class RateLimiter:
    """API ìš”ì²­ ì œí•œ ê´€ë¦¬ì (rate_limiter.py ì°¸ì¡°)"""
    def __init__(self, config: RateLimitConfig = None):
        self.config = config or RateLimitConfig()
        self.request_buckets: Dict[str, TokenBucket] = {}
        self.token_buckets: Dict[str, TokenBucket] = {}
        self.request_history: Dict[str, deque] = {}
        self.token_history: Dict[str, deque] = {}
        self.lock = Lock()
        self.logger = logging.getLogger(__name__)

    def _get_or_create_buckets(self, api_key: str) -> tuple[TokenBucket, TokenBucket]:
        with self.lock:
            if api_key not in self.request_buckets:
                self.request_buckets[api_key] = TokenBucket(
                    capacity=self.config.rpm_limit,
                    refill_rate=self.config.rpm_limit / 60.0
                )
                self.token_buckets[api_key] = TokenBucket(
                    capacity=self.config.tpm_limit,
                    refill_rate=self.config.tpm_limit / 60.0
                )
                self.request_history[api_key] = deque(maxlen=self.config.rpm_limit)
                self.token_history[api_key] = deque(maxlen=self.config.tpm_limit)
                self.logger.debug(f"ğŸ”§ API í‚¤ {api_key[:10]}... í† í° ë²„í‚· ìƒì„±")
            return self.request_buckets[api_key], self.token_buckets[api_key]

    def _refill_bucket(self, bucket: TokenBucket):
        current_time = time.time()
        time_passed = current_time - bucket.last_refill
        tokens_to_add = time_passed * bucket.refill_rate
        bucket.tokens = min(bucket.capacity, bucket.tokens + tokens_to_add)
        bucket.last_refill = current_time

    def _check_window_limit(self, api_key: str, estimated_tokens: int = 1) -> bool:
        current_time = time.time()
        window_start = current_time - self.config.window_size
        
        request_history = self.request_history[api_key]
        while request_history and request_history[0] < window_start:
            request_history.popleft()
        
        token_history = self.token_history[api_key]
        while token_history and token_history[0]['timestamp'] < window_start:
            token_history.popleft()
        
        current_requests = len(request_history)
        current_tokens = sum(entry['tokens'] for entry in token_history)
        
        if current_requests >= self.config.rpm_limit:
            self.logger.warning(f"âš ï¸ API í‚¤ {api_key[:10]}... RPM ì œí•œ ì´ˆê³¼: {current_requests}/{self.config.rpm_limit}")
            return False
        
        if current_tokens + estimated_tokens > self.config.tpm_limit:
            self.logger.warning(f"âš ï¸ API í‚¤ {api_key[:10]}... TPM ì œí•œ ì´ˆê³¼: {current_tokens + estimated_tokens}/{self.config.tpm_limit}")
            return False
        
        return True

    async def acquire_permission(
        self,
        api_key: str,
        estimated_tokens: int = 1,
        timeout: float = 10.0
    ) -> bool:
        start_time = time.time()
        request_bucket, token_bucket = self._get_or_create_buckets(api_key)
        
        while time.time() - start_time < timeout:
            with self.lock:
                self._refill_bucket(request_bucket)
                self._refill_bucket(token_bucket)
                
                if not self._check_window_limit(api_key, estimated_tokens):
                    await asyncio.sleep(0.1)
                    continue
                
                if request_bucket.tokens >= 1 and token_bucket.tokens >= estimated_tokens:
                    request_bucket.tokens -= 1
                    token_bucket.tokens -= estimated_tokens
                    current_time = time.time()
                    self.request_history[api_key].append(current_time)
                    self.token_history[api_key].append({'timestamp': current_time, 'tokens': estimated_tokens})
                    self.logger.debug(f"âœ… ìš”ì²­ í—ˆê°€ ìŠ¹ì¸: {api_key[:10]}... (í† í°: {estimated_tokens})")
                    return True
            await asyncio.sleep(0.1)
        self.logger.warning(f"â° ìš”ì²­ í—ˆê°€ íƒ€ì„ì•„ì›ƒ: {api_key[:10]}...")
        return False

    def estimate_tokens(self, text: str) -> int:
        # ê°„ë‹¨í•œ ì¶”ì • (ì‹¤ì œë¡œëŠ” ë” ì •í™•í•œ í† í°í™” í•„ìš”)
        return max(1, len(text) // 4)

    def get_usage_stats(self, api_key: str) -> Dict[str, Any]:
        if api_key not in self.request_buckets:
            return {"requests_available": self.config.rpm_limit,
                    "tokens_available": self.config.tpm_limit,
                    "requests_used": 0,
                    "tokens_used": 0,
                    "usage_rate": 0.0}
        request_bucket, token_bucket = self._get_or_create_buckets(api_key)
        self._refill_bucket(request_bucket)
        self._refill_bucket(token_bucket)
        current_time = time.time()
        window_start = current_time - self.config.window_size
        recent_requests = [req for req in self.request_history[api_key] if req > window_start]
        recent_tokens = sum(entry['tokens'] for entry in self.token_history[api_key] if entry['timestamp'] > window_start)
        return {"requests_available": int(request_bucket.tokens),
                "tokens_available": int(token_bucket.tokens),
                "requests_used": len(recent_requests),
                "tokens_used": recent_tokens,
                "usage_rate": len(recent_requests) / self.config.rpm_limit,
                "request_bucket_capacity": request_bucket.capacity,
                "token_bucket_capacity": token_bucket.capacity}

    def get_best_available_key(self, api_keys: List[str], estimated_tokens: int = 1) -> Optional[str]:
        """ê°€ì¥ ì‚¬ìš© ê°€ëŠ¥í•œ API í‚¤ ë°˜í™˜ (rate_limiter.py ì°¸ì¡°)"""
        best_key = None
        best_score = -1
        
        for api_key in api_keys:
            stats = self.get_usage_stats(api_key)
            
            # ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸
            if (stats["requests_available"] >= 1 and 
                stats["tokens_available"] >= estimated_tokens):
                
                # ì ìˆ˜ ê³„ì‚° (ì‚¬ìš©ë¥ ì´ ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ, ìš”ì²­ ê°€ëŠ¥ í† í°ì´ ë§ì„ìˆ˜ë¡ ì¢‹ìŒ)
                score = (1 - stats["usage_rate"]) * stats["tokens_available"]
                
                if score > best_score:
                    best_score = score
                    best_key = api_key
        
        return best_key

class WarehouseAI:
    def __init__(self, logger=None):
        self.logger = logger or logging.getLogger(__name__)
        self.gemini_models: List[Dict] = []
        self.gemini_config = AI_MODEL_CONFIG
        self.current_model_index = 0
        self.rate_limiter = RateLimiter() # RateLimiter ì¸ìŠ¤í„´ìŠ¤ ì¶”ê°€
        self._setup_models()

    def _setup_models(self):
        """ì—¬ëŸ¬ Gemini API í‚¤ë¡œ ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            api_keys = {
                'GEMINI_1': os.getenv('GEMINI_API_KEY_1'),
                'GEMINI_2': os.getenv('GEMINI_API_KEY_2'),
                'GEMINI_3': os.getenv('GEMINI_API_KEY_3'),
                'GEMINI_4': os.getenv('GEMINI_API_KEY_4')
            }

            valid_keys = {k: v for k, v in api_keys.items() if v}
            if not valid_keys:
                self.logger.warning("ê²½ê³ : GEMINI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. AI ê¸°ëŠ¥ì´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

            for model_name, api_key in valid_keys.items():
                try:
                    genai.configure(api_key=api_key)
                    model = genai.GenerativeModel(
                        "gemini-2.0-flash-lite-001", # ai_model_manager.pyì—ì„œ ì‚¬ìš©ëœ ëª¨ë¸ëª…
                        generation_config=self.gemini_config
                    )
                    self.gemini_models.append({
                        'model': model,
                        'api_key': api_key,
                        'name': model_name,
                        'failures': 0 # ì‹¤íŒ¨ íšŸìˆ˜ ì¶”ì  (ê°„ë‹¨ êµ¬í˜„)
                    })
                    self.logger.info(f"ğŸ¤– {model_name} ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ")
                except Exception as e:
                    self.logger.error(f"âŒ {model_name} ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                    continue

            if not self.gemini_models:
                self.logger.error("ì‚¬ìš© ê°€ëŠ¥í•œ Gemini ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")

            self.logger.info(f"ğŸ‰ ì´ {len(self.gemini_models)}ê°œì˜ Gemini ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")

        except Exception as e:
            self.logger.error(f"âŒ AI ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

    def _get_next_model(self) -> Optional[Dict]:
        """ë‹¤ìŒ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì„ íƒ (RateLimiterë¥¼ í™œìš©í•˜ì—¬ ê°œì„ )"""
        if not self.gemini_models:
            return None
        
        available_api_keys = [m['api_key'] for m in self.gemini_models if m['failures'] < 3]
        if not available_api_keys:
            # ëª¨ë“  ëª¨ë¸ì´ ì‹¤íŒ¨í•œ ê²½ìš° ì‹¤íŒ¨ íšŸìˆ˜ ë¦¬ì…‹
            for model in self.gemini_models:
                model['failures'] = 0
            available_api_keys = [m['api_key'] for m in self.gemini_models]
        
        # RateLimiterì˜ get_best_available_keyë¥¼ í™œìš©í•˜ì—¬ ìµœì ì˜ í‚¤ ì„ íƒ
        best_key = self.rate_limiter.get_best_available_key(available_api_keys, estimated_tokens=1) # í† í° ì¶”ì •ì€ ë‚˜ì¤‘ì— ì§ˆë¬¸ í…ìŠ¤íŠ¸ ê¸°ë°˜ìœ¼ë¡œ ê°œì„ 
        
        if best_key:            
            # ì„ íƒëœ í‚¤ì— í•´ë‹¹í•˜ëŠ” ëª¨ë¸ ì •ë³´ ë°˜í™˜
            for model_info in self.gemini_models:
                if model_info['api_key'] == best_key:
                    return model_info
        return None

    async def answer_query(self, question: str, data_context: dict):
        """Gemini APIë¥¼ í†µí•œ ì§ˆì˜ì‘ë‹µ"""
        if not self.gemini_models:
            return "ì˜¤ë¥˜: ì‚¬ìš© ê°€ëŠ¥í•œ AI ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤."

        max_attempts = len(self.gemini_models)
        estimated_tokens = self.rate_limiter.estimate_tokens(question) # ì§ˆë¬¸ í…ìŠ¤íŠ¸ ê¸°ë°˜ í† í° ì¶”ì •

        for attempt in range(max_attempts):
            current_model_info = self._get_next_model()
            if not current_model_info:
                continue

            api_key = current_model_info['api_key']
            
            # RateLimiterë¥¼ í†µí•´ ê¶Œí•œ íšë“ ì‹œë„
            if not await self.rate_limiter.acquire_permission(api_key, estimated_tokens):
                self.logger.warning(f"âš ï¸ API í‚¤ {api_key[:10]}... ìš”ì²­ ì œí•œìœ¼ë¡œ ì¸í•´ ëŒ€ê¸° ë˜ëŠ” ë‹¤ë¥¸ í‚¤ ì‹œë„.")
                # ì—¬ê¸°ì„œëŠ” ë°”ë¡œ ë‹¤ìŒ í‚¤ë¡œ ë„˜ì–´ê°€ê±°ë‚˜, ì§§ê²Œ ëŒ€ê¸° í›„ ì¬ì‹œë„í•  ìˆ˜ ìˆìŒ
                # ê°„ë‹¨í•˜ê²Œ ë‹¤ìŒ ëª¨ë¸ë¡œ ë„˜ì–´ê°€ë„ë¡ ì²˜ë¦¬
                continue

            try:
                model_instance = current_model_info['model']
                prompt = f"""
        ë‹¹ì‹ ì€ ìŠ¤ë§ˆíŠ¸ ë¬¼ë¥˜ ì°½ê³  ê´€ë¦¬ ì‹œìŠ¤í…œì„ ìœ„í•œ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë°ì´í„° ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì‹¬ì¸µì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤. íŠ¹íˆ, ML ëª¨ë¸ í™œìš© ë° ML Ops ì„¤ê³„ ê´€ì ì—ì„œì˜ ì œì•ˆë„ í¬í•¨í•´ ì£¼ì„¸ìš”.

        ë‹¤ìŒ ë‹¨ê³„ì— ë”°ë¼ ì‚¬ê³ í•˜ê³  ë‹µë³€ì„ êµ¬ì„±í•˜ì„¸ìš”:

        1.  **ë°ì´í„° ìš”ì•½ ë° í•µì‹¬ íŒŒì•…**: ì œê³µëœ `data_context`ë¥¼ ë¹ ë¥´ê²Œ ìŠ¤ìº”í•˜ì—¬ ê° ë¶„ì„ ê²°ê³¼(ê¸°ìˆ  í†µê³„, ì¼ë³„ ë¬¼ë™ëŸ‰, ìƒí’ˆ ì¸ì‚¬ì´íŠ¸, ë™ í™œìš©ë¥ , ì´ìƒ ì§•í›„)ì˜ ì£¼ìš” ë‚´ìš©ì„ ìš”ì•½í•©ë‹ˆë‹¤.
        2.  **íŠ¸ë Œë“œ ë° íŒ¨í„´ ì‹ë³„**: ë°ì´í„°ì—ì„œ ë‚˜íƒ€ë‚˜ëŠ” ëª…í™•í•œ íŠ¸ë Œë“œ(ì˜ˆ: ì•ˆì •ì ì¸ ì…ì¶œê³ , íŠ¹ì • ë™ì˜ ë‚®ì€ í™œìš©ë¥ )ì™€ ë°˜ë³µë˜ëŠ” íŒ¨í„´ì„ ì‹ë³„í•©ë‹ˆë‹¤.
        3.  **ë¬¸ì œì  ë° íŠ¹ì´ì‚¬í•­ ë¶„ì„**: AIê°€ ì´ì „ì— ì§€ì í•œ Date ê²°ì¸¡ì¹˜, Unnamed ì»¬ëŸ¼, ProductCode/ProductName ë¶ˆì¼ì¹˜, í˜„ì¬ê³ ì˜ ì¼ê´€ì„±ê³¼ ê°™ì€ ë°ì´í„° í’ˆì§ˆ ë¬¸ì œì™€ ê°ì§€ëœ ì´ìƒ ì§•í›„(`anomalies`)ë¥¼ ì‹¬ì¸µì ìœ¼ë¡œ ë¶„ì„í•˜ê³ , ê·¸ ì ì¬ì  ì›ì¸ê³¼ ì˜í–¥ì— ëŒ€í•´ ì¶”ë¡ í•©ë‹ˆë‹¤.
        4.  **ê°œì„ ì  ë„ì¶œ ë° ML ëª¨ë¸ ì œì•ˆ**: ì‹ë³„ëœ ë¬¸ì œì ê³¼ íŠ¹ì´ì‚¬í•­ì„ í•´ê²°í•˜ê¸° ìœ„í•œ êµ¬ì²´ì ì¸ ê°œì„  ë°©ì•ˆì„ ì œì‹œí•©ë‹ˆë‹¤. ì´ ê³¼ì •ì—ì„œ í˜„ì¬ êµ¬ì¶•ëœ ML ëª¨ë¸(ìˆ˜ìš” ì˜ˆì¸¡, ì œí’ˆ í´ëŸ¬ìŠ¤í„°ë§) ì™¸ì— ì¶”ê°€ì ìœ¼ë¡œ í™œìš© ê°€ëŠ¥í•œ ML ëª¨ë¸(ì˜ˆ: ì´ìƒ íƒì§€ë¥¼ ìœ„í•œ Isolation Forest)ì„ ì œì•ˆí•˜ê³ , í•´ë‹¹ ëª¨ë¸ì´ ì–´ë–¤ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆëŠ”ì§€ ëª…í™•íˆ ì„¤ëª…í•©ë‹ˆë‹¤.
        5.  **ML Ops ì„¤ê³„ ê³ ë ¤ì‚¬í•­**: ML ëª¨ë¸ì„ ì‹¤ì œ ìš´ì˜ í™˜ê²½ì— ì ìš©í•˜ê³  ì§€ì†ì ìœ¼ë¡œ ê´€ë¦¬í•˜ê¸° ìœ„í•œ ML Ops ê´€ì ì˜ í•„ìš” ì‚¬í•­(ì˜ˆ: ë°ì´í„° íŒŒì´í”„ë¼ì¸, ëª¨ë¸ ëª¨ë‹ˆí„°ë§, ì¬í•™ìŠµ ì „ëµ)ì— ëŒ€í•´ ê°„ëµíˆ ì–¸ê¸‰í•©ë‹ˆë‹¤.
        6.  **ì§ˆë¬¸ì— ëŒ€í•œ ìµœì¢… ë‹µë³€ êµ¬ì„±**: ìœ„ ë‹¨ê³„ë“¤ì˜ ì¶”ë¡ ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ `ì§ˆë¬¸`ì— ëŒ€í•´ í¬ê´„ì ì´ê³  ì²´ê³„ì ì¸ ë‹µë³€ì„ í•œêµ­ì–´ë¡œ ì œê³µí•©ë‹ˆë‹¤. ê° ì„¹ì…˜(ì£¼ìš” íŠ¸ë Œë“œ, íŠ¹ì´ì‚¬í•­, ê°œì„ ì  ë° ML ëª¨ë¸ ì œì•ˆ, ML Ops ê³ ë ¤ì‚¬í•­, ì¶”ê°€ ë¶„ì„ ê°€ëŠ¥ì„± ë“±)ì„ ëª…í™•íˆ êµ¬ë¶„í•˜ì—¬ ê°€ë…ì„±ì„ ë†’ì…ë‹ˆë‹¤.

        **ë°ì´í„°:**
        ```json
        {data_context}
        ```

        **ì§ˆë¬¸:**
        {question}
        """
                # Gemini API í˜¸ì¶œ
                response = await model_instance.generate_content_async(prompt) # generate_content_asyncë¡œ ë³€ê²½
                result_text = response.text

                self.logger.info(f"âœ… {current_model_info['name']} API ì„±ê³µ - ì‘ë‹µ (ì¼ë¶€): {result_text[:200]}...")
                return result_text

            except Exception as e:
                current_model_info['failures'] += 1
                self.logger.warning(f"âš ï¸ {current_model_info['name']} API ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_attempts}): {str(e)}")

                # ì§€ìˆ˜ ë°±ì˜¤í”„ ì ìš© (RateLimiterì˜ acquire_permissionì—ì„œ ì´ë¯¸ ì²˜ë¦¬ë˜ì§€ë§Œ, ì—¬ê¸°ì„œë„ ì¶”ê°€ ê°€ëŠ¥)
                await asyncio.sleep(0.5 * (2 ** attempt)) # ê°„ë‹¨í•œ ë°±ì˜¤í”„

                if attempt < max_attempts - 1:
                    self.logger.info(f"ğŸ”„ ë‹¤ìŒ ëª¨ë¸ë¡œ ì¬ì‹œë„ ì¤‘...")
                    continue
                else:
                    self.logger.error(f"âŒ ëª¨ë“  Gemini ëª¨ë¸ ì‹œë„ ì‹¤íŒ¨")
                    return f"ì˜¤ë¥˜: ëª¨ë“  API í˜¸ì¶œ ì‹¤íŒ¨ - ë§ˆì§€ë§‰ ì˜¤ë¥˜: {str(e)}"

        return "ì˜¤ë¥˜: ëª¨ë“  ëª¨ë¸ ì‹œë„ ì‹¤íŒ¨" 