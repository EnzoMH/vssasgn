import os
import logging
import time
import asyncio
import random
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, field
from collections import deque
from threading import Lock

import google.generativeai as genai

# AI ëª¨ë¸ ì„¤ì • (legacy/crad_lcrag/utils/ai_model_manager.py ì°¸ì¡°)
AI_MODEL_CONFIG = {
    "temperature": 0.1,
    "top_p": 0.8,
    "top_k": 40,
    "max_output_tokens": 2048,
}

@dataclass
class APIKeyStatus:
    """API í‚¤ ìƒíƒœ ì •ë³´ (gemini_client.py ì°¸ì¡°)"""
    key: str
    is_active: bool = True
    request_count: int = 0
    last_request_time: float = 0.0
    error_count: int = 0
    success_count: int = 0
    
    @property
    def success_rate(self) -> float:
        total = self.success_count + self.error_count
        return self.success_count / total if total > 0 else 0.0

@dataclass
class RateLimitConfig:
    """ìš”ì²­ ì œí•œ ì„¤ì • (rate_limiter.py ì°¸ì¡°)"""
    rpm_limit: int = 2000  # ë¶„ë‹¹ ìš”ì²­ ì œí•œ
    tpm_limit: int = 4000000  # ë¶„ë‹¹ í† í° ì œí•œ
    burst_limit: int = 100  # ë²„ìŠ¤íŠ¸ í—ˆìš© í•œë„
    window_size: int = 60  # ì‹œê°„ ìœˆë„ìš° (ì´ˆ)

@dataclass
class TokenBucket:
    """í† í° ë²„í‚· (rate_limiter.py ì°¸ì¡°)"""
    capacity: int
    tokens: float = field(default_factory=lambda: 0)
    last_refill: float = field(default_factory=time.time)
    refill_rate: float = field(default=1.0)  # ì´ˆë‹¹ í† í° ë³´ì¶©ë¥ 
    
    def __post_init__(self):
        self.tokens = self.capacity
        self.refill_rate = self.capacity / 60.0  # ë¶„ë‹¹ ìš©ëŸ‰ì„ ì´ˆë‹¹ìœ¼ë¡œ ë³€í™˜

class RateLimiter:
    """API ìš”ì²­ ì œí•œ ê´€ë¦¬ì (rate_limiter.py ì°¸ì¡°)"""
    def __init__(self, config: RateLimitConfig = None):
        self.config = config or RateLimitConfig()
        self.request_buckets: Dict[str, TokenBucket] = {}
        self.token_buckets: Dict[str, TokenBucket] = {}
        self.request_history: Dict[str, deque] = {}
        self.token_history: Dict[str, deque] = {}
        self.lock = Lock()
        self.logger = logging.getLogger(__name__)

    def _get_or_create_buckets(self, api_key: str) -> tuple[TokenBucket, TokenBucket]:
        with self.lock:
            if api_key not in self.request_buckets:
                self.request_buckets[api_key] = TokenBucket(
                    capacity=self.config.rpm_limit,
                    refill_rate=self.config.rpm_limit / 60.0
                )
                self.token_buckets[api_key] = TokenBucket(
                    capacity=self.config.tpm_limit,
                    refill_rate=self.config.tpm_limit / 60.0
                )
                self.request_history[api_key] = deque(maxlen=self.config.rpm_limit)
                self.token_history[api_key] = deque(maxlen=self.config.tpm_limit)
                self.logger.debug(f"ğŸ”§ API í‚¤ {api_key[:10]}... í† í° ë²„í‚· ìƒì„±")
            return self.request_buckets[api_key], self.token_buckets[api_key]

    def _refill_bucket(self, bucket: TokenBucket):
        current_time = time.time()
        time_passed = current_time - bucket.last_refill
        tokens_to_add = time_passed * bucket.refill_rate
        bucket.tokens = min(bucket.capacity, bucket.tokens + tokens_to_add)
        bucket.last_refill = current_time

    def _check_window_limit(self, api_key: str, estimated_tokens: int = 1) -> bool:
        current_time = time.time()
        window_start = current_time - self.config.window_size
        
        request_history = self.request_history[api_key]
        while request_history and request_history[0] < window_start:
            request_history.popleft()
        
        token_history = self.token_history[api_key]
        while token_history and token_history[0]['timestamp'] < window_start:
            token_history.popleft()
        
        current_requests = len(request_history)
        current_tokens = sum(entry['tokens'] for entry in token_history)
        
        if current_requests >= self.config.rpm_limit:
            self.logger.warning(f"âš ï¸ API í‚¤ {api_key[:10]}... RPM ì œí•œ ì´ˆê³¼: {current_requests}/{self.config.rpm_limit}")
            return False
        
        if current_tokens + estimated_tokens > self.config.tpm_limit:
            self.logger.warning(f"âš ï¸ API í‚¤ {api_key[:10]}... TPM ì œí•œ ì´ˆê³¼: {current_tokens + estimated_tokens}/{self.config.tpm_limit}")
            return False
        
        return True

    async def acquire_permission(
        self,
        api_key: str,
        estimated_tokens: int = 1,
        timeout: float = 10.0
    ) -> bool:
        start_time = time.time()
        request_bucket, token_bucket = self._get_or_create_buckets(api_key)
        
        while time.time() - start_time < timeout:
            with self.lock:
                self._refill_bucket(request_bucket)
                self._refill_bucket(token_bucket)
                
                if not self._check_window_limit(api_key, estimated_tokens):
                    await asyncio.sleep(0.1)
                    continue
                
                if request_bucket.tokens >= 1 and token_bucket.tokens >= estimated_tokens:
                    request_bucket.tokens -= 1
                    token_bucket.tokens -= estimated_tokens
                    current_time = time.time()
                    self.request_history[api_key].append(current_time)
                    self.token_history[api_key].append({'timestamp': current_time, 'tokens': estimated_tokens})
                    self.logger.debug(f"âœ… ìš”ì²­ í—ˆê°€ ìŠ¹ì¸: {api_key[:10]}... (í† í°: {estimated_tokens})")
                    return True
            await asyncio.sleep(0.1)
        self.logger.warning(f"â° ìš”ì²­ í—ˆê°€ íƒ€ì„ì•„ì›ƒ: {api_key[:10]}...")
        return False

    def estimate_tokens(self, text: str) -> int:
        # ê°„ë‹¨í•œ ì¶”ì • (ì‹¤ì œë¡œëŠ” ë” ì •í™•í•œ í† í°í™” í•„ìš”)
        return max(1, len(text) // 4)

    def get_usage_stats(self, api_key: str) -> Dict[str, Any]:
        if api_key not in self.request_buckets:
            return {"requests_available": self.config.rpm_limit,
                    "tokens_available": self.config.tpm_limit,
                    "requests_used": 0,
                    "tokens_used": 0,
                    "usage_rate": 0.0}
        request_bucket, token_bucket = self._get_or_create_buckets(api_key)
        self._refill_bucket(request_bucket)
        self._refill_bucket(token_bucket)
        current_time = time.time()
        window_start = current_time - self.config.window_size
        recent_requests = [req for req in self.request_history[api_key] if req > window_start]
        recent_tokens = sum(entry['tokens'] for entry in self.token_history[api_key] if entry['timestamp'] > window_start)
        return {"requests_available": int(request_bucket.tokens),
                "tokens_available": int(token_bucket.tokens),
                "requests_used": len(recent_requests),
                "tokens_used": recent_tokens,
                "usage_rate": len(recent_requests) / self.config.rpm_limit,
                "request_bucket_capacity": request_bucket.capacity,
                "token_bucket_capacity": token_bucket.capacity}

class WarehouseAI:
    def __init__(self, logger=None):
        self.logger = logger or logging.getLogger(__name__)
        self.gemini_models: List[Dict] = []
        self.gemini_config = AI_MODEL_CONFIG
        self.current_model_index = 0
        self.rate_limiter = RateLimiter() # RateLimiter ì¸ìŠ¤í„´ìŠ¤ ì¶”ê°€
        self._setup_models()

    def _setup_models(self):
        """ì—¬ëŸ¬ Gemini API í‚¤ë¡œ ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            api_keys = {
                'GEMINI_1': os.getenv('GEMINI_API_KEY_1'),
                'GEMINI_2': os.getenv('GEMINI_API_KEY_2'),
                'GEMINI_3': os.getenv('GEMINI_API_KEY_3'),
                'GEMINI_4': os.getenv('GEMINI_API_KEY_4')
            }

            valid_keys = {k: v for k, v in api_keys.items() if v}
            if not valid_keys:
                self.logger.warning("ê²½ê³ : GEMINI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. AI ê¸°ëŠ¥ì´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

            for model_name, api_key in valid_keys.items():
                try:
                    genai.configure(api_key=api_key)
                    model = genai.GenerativeModel(
                        "gemini-2.0-flash-lite-001", # ai_model_manager.pyì—ì„œ ì‚¬ìš©ëœ ëª¨ë¸ëª…
                        generation_config=self.gemini_config
                    )
                    self.gemini_models.append({
                        'model': model,
                        'api_key': api_key,
                        'name': model_name,
                        'failures': 0 # ì‹¤íŒ¨ íšŸìˆ˜ ì¶”ì  (ê°„ë‹¨ êµ¬í˜„)
                    })
                    self.logger.info(f"ğŸ¤– {model_name} ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ")
                except Exception as e:
                    self.logger.error(f"âŒ {model_name} ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                    continue

            if not self.gemini_models:
                self.logger.error("ì‚¬ìš© ê°€ëŠ¥í•œ Gemini ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")

            self.logger.info(f"ğŸ‰ ì´ {len(self.gemini_models)}ê°œì˜ Gemini ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")

        except Exception as e:
            self.logger.error(f"âŒ AI ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

    def _get_next_model(self) -> Optional[Dict]:
        """ë‹¤ìŒ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì„ íƒ (RateLimiterë¥¼ í™œìš©í•˜ì—¬ ê°œì„ )"""
        if not self.gemini_models:
            return None
        
        available_api_keys = [m['api_key'] for m in self.gemini_models if m['failures'] < 3]
        if not available_api_keys:
            # ëª¨ë“  ëª¨ë¸ì´ ì‹¤íŒ¨í•œ ê²½ìš° ì‹¤íŒ¨ íšŸìˆ˜ ë¦¬ì…‹
            for model in self.gemini_models:
                model['failures'] = 0
            available_api_keys = [m['api_key'] for m in self.gemini_models]
        
        # RateLimiterì˜ get_best_available_keyë¥¼ í™œìš©í•˜ì—¬ ìµœì ì˜ í‚¤ ì„ íƒ
        best_key = self.rate_limiter.get_best_available_key(available_api_keys, estimated_tokens=1) # í† í° ì¶”ì •ì€ ë‚˜ì¤‘ì— ì§ˆë¬¸ í…ìŠ¤íŠ¸ ê¸°ë°˜ìœ¼ë¡œ ê°œì„ 
        
        if best_key:            
            # ì„ íƒëœ í‚¤ì— í•´ë‹¹í•˜ëŠ” ëª¨ë¸ ì •ë³´ ë°˜í™˜
            for model_info in self.gemini_models:
                if model_info['api_key'] == best_key:
                    return model_info
        return None

    async def answer_query(self, question: str, data_context: dict):
        """Gemini APIë¥¼ í†µí•œ ì§ˆì˜ì‘ë‹µ"""
        if not self.gemini_models:
            return "ì˜¤ë¥˜: ì‚¬ìš© ê°€ëŠ¥í•œ AI ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤."

        max_attempts = len(self.gemini_models)
        estimated_tokens = self.rate_limiter.estimate_tokens(question) # ì§ˆë¬¸ í…ìŠ¤íŠ¸ ê¸°ë°˜ í† í° ì¶”ì •

        for attempt in range(max_attempts):
            current_model_info = self._get_next_model()
            if not current_model_info:
                continue

            api_key = current_model_info['api_key']
            
            # RateLimiterë¥¼ í†µí•´ ê¶Œí•œ íšë“ ì‹œë„
            if not await self.rate_limiter.acquire_permission(api_key, estimated_tokens):
                self.logger.warning(f"âš ï¸ API í‚¤ {api_key[:10]}... ìš”ì²­ ì œí•œìœ¼ë¡œ ì¸í•´ ëŒ€ê¸° ë˜ëŠ” ë‹¤ë¥¸ í‚¤ ì‹œë„.")
                # ì—¬ê¸°ì„œëŠ” ë°”ë¡œ ë‹¤ìŒ í‚¤ë¡œ ë„˜ì–´ê°€ê±°ë‚˜, ì§§ê²Œ ëŒ€ê¸° í›„ ì¬ì‹œë„í•  ìˆ˜ ìˆìŒ
                # ê°„ë‹¨í•˜ê²Œ ë‹¤ìŒ ëª¨ë¸ë¡œ ë„˜ì–´ê°€ë„ë¡ ì²˜ë¦¬
                continue

            try:
                model_instance = current_model_info['model']
                prompt = f"""
        ì°½ê³  ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”:
        ë°ì´í„°: {data_context}
        ì§ˆë¬¸: {question}
        """
                # Gemini API í˜¸ì¶œ
                response = await model_instance.generate_content(prompt)
                result_text = response.text

                self.logger.info(f"âœ… {current_model_info['name']} API ì„±ê³µ - ì‘ë‹µ (ì¼ë¶€): {result_text[:200]}...")
                return result_text

            except Exception as e:
                current_model_info['failures'] += 1
                self.logger.warning(f"âš ï¸ {current_model_info['name']} API ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_attempts}): {str(e)}")

                # ì§€ìˆ˜ ë°±ì˜¤í”„ ì ìš© (RateLimiterì˜ acquire_permissionì—ì„œ ì´ë¯¸ ì²˜ë¦¬ë˜ì§€ë§Œ, ì—¬ê¸°ì„œë„ ì¶”ê°€ ê°€ëŠ¥)
                await asyncio.sleep(0.5 * (2 ** attempt)) # ê°„ë‹¨í•œ ë°±ì˜¤í”„

                if attempt < max_attempts - 1:
                    self.logger.info(f"ğŸ”„ ë‹¤ìŒ ëª¨ë¸ë¡œ ì¬ì‹œë„ ì¤‘...")
                    continue
                else:
                    self.logger.error(f"âŒ ëª¨ë“  Gemini ëª¨ë¸ ì‹œë„ ì‹¤íŒ¨")
                    return f"ì˜¤ë¥˜: ëª¨ë“  API í˜¸ì¶œ ì‹¤íŒ¨ - ë§ˆì§€ë§‰ ì˜¤ë¥˜: {str(e)}"

        return "ì˜¤ë¥˜: ëª¨ë“  ëª¨ë¸ ì‹œë„ ì‹¤íŒ¨" 