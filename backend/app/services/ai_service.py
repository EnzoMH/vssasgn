import os
import logging
import time
import asyncio
import random
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, field
from collections import deque
from threading import Lock

import google.generativeai as genai

from dotenv import load_dotenv, find_dotenv

# .env íŒŒì¼ì„ í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¶€í„° ìƒìœ„ ë””ë ‰í† ë¦¬ê¹Œì§€ ìë™ìœ¼ë¡œ ì°¾ì•„ì„œ ë¡œë“œ
dotenv_path = find_dotenv()
if dotenv_path:
    load_dotenv(dotenv_path)
    print(f"âœ… .env íŒŒì¼ ë¡œë“œë¨: {dotenv_path}")
else:
    print("âš ï¸ .env íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‹œìŠ¤í…œ í™˜ê²½ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")

# ì•ˆì „í•œ í™˜ê²½ë³€ìˆ˜ ì„¤ì • - None ê°’ ì²´í¬
def safe_set_env_var(key_name: str):
    """í™˜ê²½ë³€ìˆ˜ë¥¼ ì•ˆì „í•˜ê²Œ ì„¤ì • (None ê°’ ì²´í¬)"""
    value = os.getenv(key_name)
    if value is not None:
        os.environ[key_name] = value
        return True
    return False

# GEMINI API í‚¤ë“¤ ì•ˆì „í•˜ê²Œ ì„¤ì •
api_keys_loaded = []
for i in range(1, 5):
    key_name = f'GEMINI_API_KEY_{i}'
    if safe_set_env_var(key_name):
        api_keys_loaded.append(key_name)

if api_keys_loaded:
    print(f"âœ… ë¡œë“œëœ API í‚¤: {', '.join(api_keys_loaded)}")
else:
    print("âš ï¸ GEMINI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. AI ê¸°ëŠ¥ì´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

# AI ëª¨ë¸ ì„¤ì • (legacy/crad_lcrag/utils/ai_model_manager.py ì°¸ì¡°)
AI_MODEL_CONFIG = {
    "temperature": 0.1,
    "top_p": 0.8,
    "top_k": 40,
    "max_output_tokens": 2048,
}

@dataclass
class APIKeyStatus:
    """API í‚¤ ìƒíƒœ ì •ë³´ (gemini_client.py ì°¸ì¡°)"""
    key: str
    is_active: bool = True
    request_count: int = 0
    last_request_time: float = 0.0
    error_count: int = 0
    success_count: int = 0
    
    @property
    def success_rate(self) -> float:
        total = self.success_count + self.error_count
        return self.success_count / total if total > 0 else 0.0

@dataclass
class RateLimitConfig:
    """ìš”ì²­ ì œí•œ ì„¤ì • (rate_limiter.py ì°¸ì¡°)"""
    rpm_limit: int = 2000  # ë¶„ë‹¹ ìš”ì²­ ì œí•œ
    tpm_limit: int = 4000000  # ë¶„ë‹¹ í† í° ì œí•œ
    burst_limit: int = 100  # ë²„ìŠ¤íŠ¸ í—ˆìš© í•œë„
    window_size: int = 60  # ì‹œê°„ ìœˆë„ìš° (ì´ˆ)

@dataclass
class TokenBucket:
    """í† í° ë²„í‚· (rate_limiter.py ì°¸ì¡°)"""
    capacity: int
    tokens: float = field(default_factory=lambda: 0)
    last_refill: float = field(default_factory=time.time)
    refill_rate: float = field(default=1.0)  # ì´ˆë‹¹ í† í° ë³´ì¶©ë¥ 
    
    def __post_init__(self):
        self.tokens = self.capacity
        self.refill_rate = self.capacity / 60.0  # ë¶„ë‹¹ ìš©ëŸ‰ì„ ì´ˆë‹¹ìœ¼ë¡œ ë³€í™˜

class RateLimiter:
    """API ìš”ì²­ ì œí•œ ê´€ë¦¬ì (rate_limiter.py ì°¸ì¡°)"""
    def __init__(self, config: RateLimitConfig = None):
        self.config = config or RateLimitConfig()
        self.request_buckets: Dict[str, TokenBucket] = {}
        self.token_buckets: Dict[str, TokenBucket] = {}
        self.request_history: Dict[str, deque] = {}
        self.token_history: Dict[str, deque] = {}
        self.lock = Lock()
        self.logger = logging.getLogger(__name__)

    def _get_or_create_buckets(self, api_key: str) -> tuple[TokenBucket, TokenBucket]:
        with self.lock:
            if api_key not in self.request_buckets:
                self.request_buckets[api_key] = TokenBucket(
                    capacity=self.config.rpm_limit,
                    refill_rate=self.config.rpm_limit / 60.0
                )
                self.token_buckets[api_key] = TokenBucket(
                    capacity=self.config.tpm_limit,
                    refill_rate=self.config.tpm_limit / 60.0
                )
                self.request_history[api_key] = deque(maxlen=self.config.rpm_limit)
                self.token_history[api_key] = deque(maxlen=self.config.tpm_limit)
                self.logger.debug(f"ğŸ”§ API í‚¤ {api_key[:10]}... í† í° ë²„í‚· ìƒì„±")
            return self.request_buckets[api_key], self.token_buckets[api_key]

    def _refill_bucket(self, bucket: TokenBucket):
        current_time = time.time()
        time_passed = current_time - bucket.last_refill
        tokens_to_add = time_passed * bucket.refill_rate
        bucket.tokens = min(bucket.capacity, bucket.tokens + tokens_to_add)
        bucket.last_refill = current_time

    def _check_window_limit(self, api_key: str, estimated_tokens: int = 1) -> bool:
        current_time = time.time()
        window_start = current_time - self.config.window_size
        
        request_history = self.request_history[api_key]
        while request_history and request_history[0] < window_start:
            request_history.popleft()
        
        token_history = self.token_history[api_key]
        while token_history and token_history[0]['timestamp'] < window_start:
            token_history.popleft()
        
        current_requests = len(request_history)
        current_tokens = sum(entry['tokens'] for entry in token_history)
        
        if current_requests >= self.config.rpm_limit:
            self.logger.warning(f"âš ï¸ API í‚¤ {api_key[:10]}... RPM ì œí•œ ì´ˆê³¼: {current_requests}/{self.config.rpm_limit}")
            return False
        
        if current_tokens + estimated_tokens > self.config.tpm_limit:
            self.logger.warning(f"âš ï¸ API í‚¤ {api_key[:10]}... TPM ì œí•œ ì´ˆê³¼: {current_tokens + estimated_tokens}/{self.config.tpm_limit}")
            return False
        
        return True

    async def acquire_permission(
        self,
        api_key: str,
        estimated_tokens: int = 1,
        timeout: float = 10.0
    ) -> bool:
        start_time = time.time()
        request_bucket, token_bucket = self._get_or_create_buckets(api_key)
        
        while time.time() - start_time < timeout:
            with self.lock:
                self._refill_bucket(request_bucket)
                self._refill_bucket(token_bucket)
                
                if not self._check_window_limit(api_key, estimated_tokens):
                    await asyncio.sleep(0.1)
                    continue
                
                if request_bucket.tokens >= 1 and token_bucket.tokens >= estimated_tokens:
                    request_bucket.tokens -= 1
                    token_bucket.tokens -= estimated_tokens
                    current_time = time.time()
                    self.request_history[api_key].append(current_time)
                    self.token_history[api_key].append({'timestamp': current_time, 'tokens': estimated_tokens})
                    self.logger.debug(f"âœ… ìš”ì²­ í—ˆê°€ ìŠ¹ì¸: {api_key[:10]}... (í† í°: {estimated_tokens})")
                    return True
            await asyncio.sleep(0.1)
        self.logger.warning(f"â° ìš”ì²­ í—ˆê°€ íƒ€ì„ì•„ì›ƒ: {api_key[:10]}...")
        return False

    def estimate_tokens(self, text: str) -> int:
        # ê°„ë‹¨í•œ ì¶”ì • (ì‹¤ì œë¡œëŠ” ë” ì •í™•í•œ í† í°í™” í•„ìš”)
        return max(1, len(text) // 4)

    def get_usage_stats(self, api_key: str) -> Dict[str, Any]:
        if api_key not in self.request_buckets:
            return {"requests_available": self.config.rpm_limit,
                    "tokens_available": self.config.tpm_limit,
                    "requests_used": 0,
                    "tokens_used": 0,
                    "usage_rate": 0.0}
        request_bucket, token_bucket = self._get_or_create_buckets(api_key)
        self._refill_bucket(request_bucket)
        self._refill_bucket(token_bucket)
        current_time = time.time()
        window_start = current_time - self.config.window_size
        recent_requests = [req for req in self.request_history[api_key] if req > window_start]
        recent_tokens = sum(entry['tokens'] for entry in self.token_history[api_key] if entry['timestamp'] > window_start)
        return {"requests_available": int(request_bucket.tokens),
                "tokens_available": int(token_bucket.tokens),
                "requests_used": len(recent_requests),
                "tokens_used": recent_tokens,
                "usage_rate": len(recent_requests) / self.config.rpm_limit,
                "request_bucket_capacity": request_bucket.capacity,
                "token_bucket_capacity": token_bucket.capacity}

    def get_best_available_key(self, api_keys: List[str], estimated_tokens: int = 1) -> Optional[str]:
        """ê°€ì¥ ì‚¬ìš© ê°€ëŠ¥í•œ API í‚¤ ë°˜í™˜ (rate_limiter.py ì°¸ì¡°)"""
        best_key = None
        best_score = -1
        
        for api_key in api_keys:
            stats = self.get_usage_stats(api_key)
            
            # ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸
            if (stats["requests_available"] >= 1 and 
                stats["tokens_available"] >= estimated_tokens):
                
                # ì ìˆ˜ ê³„ì‚° (ì‚¬ìš©ë¥ ì´ ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ, ìš”ì²­ ê°€ëŠ¥ í† í°ì´ ë§ì„ìˆ˜ë¡ ì¢‹ìŒ)
                score = (1 - stats["usage_rate"]) * stats["tokens_available"]
                
                if score > best_score:
                    best_score = score
                    best_key = api_key
        
        return best_key

class WarehouseAI:
    def __init__(self, logger=None):
        self.logger = logger or logging.getLogger(__name__)
        self.gemini_models: List[Dict] = []
        self.gemini_config = AI_MODEL_CONFIG
        self.current_model_index = 0
        self.rate_limiter = RateLimiter() # RateLimiter ì¸ìŠ¤í„´ìŠ¤ ì¶”ê°€
        
        # ì°¨íŠ¸ ìƒì„± ì „ìš© ì„¤ì • (ë” ì¼ê´€ëœ JSON ì¶œë ¥ì„ ìœ„í•´)
        self.chart_config = genai.GenerationConfig(
            temperature=0.1,  # ë” ì¼ê´€ëœ ì¶œë ¥ì„ ìœ„í•´ ë‚®ì€ temperature
            top_p=0.9,
            top_k=20,
            max_output_tokens=2048
        )
        
        self._setup_models()

    def _setup_models(self):
        """ì—¬ëŸ¬ Gemini API í‚¤ë¡œ ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            api_keys = {
                'GEMINI_1': os.getenv('GEMINI_API_KEY_1'),
                'GEMINI_2': os.getenv('GEMINI_API_KEY_2'),
                'GEMINI_3': os.getenv('GEMINI_API_KEY_3'),
                'GEMINI_4': os.getenv('GEMINI_API_KEY_4')
            }

            valid_keys = {k: v for k, v in api_keys.items() if v}
            if not valid_keys:
                self.logger.warning("ê²½ê³ : GEMINI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. AI ê¸°ëŠ¥ì´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

            for model_name, api_key in valid_keys.items():
                try:
                    genai.configure(api_key=api_key)
                    
                    # ì‚¬ê³  ê¸°ëŠ¥ ë¹„í™œì„±í™” ì„¤ì • ì¶”ê°€
                    model_config = self.gemini_config.copy()
                    # 2.5 ëª¨ë¸ì˜ ì‚¬ê³  ê¸°ëŠ¥ìœ¼ë¡œ ì¸í•œ ì‘ë‹µ ë¬¸ì œ ë°©ì§€
                    
                    model = genai.GenerativeModel(
                        "gemini-1.5-flash",  # ë” ì•ˆì •ì ì¸ 1.5 ëª¨ë¸ ì‚¬ìš© (ì‚¬ê³  ê¸°ëŠ¥ ì—†ìŒ)
                        generation_config=model_config
                    )
                    self.gemini_models.append({
                        'model': model,
                        'api_key': api_key,
                        'name': model_name,
                        'failures': 0 # ì‹¤íŒ¨ íšŸìˆ˜ ì¶”ì  (ê°„ë‹¨ êµ¬í˜„)
                    })
                    self.logger.info(f"ğŸ¤– {model_name} ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ")
                except Exception as e:
                    self.logger.error(f"âŒ {model_name} ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                    continue

            if not self.gemini_models:
                self.logger.error("ì‚¬ìš© ê°€ëŠ¥í•œ Gemini ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")

            self.logger.info(f"ğŸ‰ ì´ {len(self.gemini_models)}ê°œì˜ Gemini ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")

        except Exception as e:
            self.logger.error(f"âŒ AI ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

    def _get_next_model(self) -> Optional[Dict]:
        """ë‹¤ìŒ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì„ íƒ (RateLimiterë¥¼ í™œìš©í•˜ì—¬ ê°œì„ )"""
        if not self.gemini_models:
            return None
        
        available_api_keys = [m['api_key'] for m in self.gemini_models if m['failures'] < 3]
        if not available_api_keys:
            # ëª¨ë“  ëª¨ë¸ì´ ì‹¤íŒ¨í•œ ê²½ìš° ì‹¤íŒ¨ íšŸìˆ˜ ë¦¬ì…‹
            for model in self.gemini_models:
                model['failures'] = 0
            available_api_keys = [m['api_key'] for m in self.gemini_models]
        
        # RateLimiterì˜ get_best_available_keyë¥¼ í™œìš©í•˜ì—¬ ìµœì ì˜ í‚¤ ì„ íƒ
        best_key = self.rate_limiter.get_best_available_key(available_api_keys, estimated_tokens=1) # í† í° ì¶”ì •ì€ ë‚˜ì¤‘ì— ì§ˆë¬¸ í…ìŠ¤íŠ¸ ê¸°ë°˜ìœ¼ë¡œ ê°œì„ 
        
        if best_key:            
            # ì„ íƒëœ í‚¤ì— í•´ë‹¹í•˜ëŠ” ëª¨ë¸ ì •ë³´ ë°˜í™˜
            for model_info in self.gemini_models:
                if model_info['api_key'] == best_key:
                    return model_info
        return None

    async def answer_query(self, question: str, data_context: dict):
        """Gemini APIë¥¼ í†µí•œ ì§ˆì˜ì‘ë‹µ"""
        if not self.gemini_models:
            return "ì˜¤ë¥˜: ì‚¬ìš© ê°€ëŠ¥í•œ AI ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤."

        max_attempts = len(self.gemini_models)
        estimated_tokens = self.rate_limiter.estimate_tokens(question) # ì§ˆë¬¸ í…ìŠ¤íŠ¸ ê¸°ë°˜ í† í° ì¶”ì •

        for attempt in range(max_attempts):
            current_model_info = self._get_next_model()
            if not current_model_info:
                continue

            api_key = current_model_info['api_key']
            
            # RateLimiterë¥¼ í†µí•´ ê¶Œí•œ íšë“ ì‹œë„
            if not await self.rate_limiter.acquire_permission(api_key, estimated_tokens):
                self.logger.warning(f"âš ï¸ API í‚¤ {api_key[:10]}... ìš”ì²­ ì œí•œìœ¼ë¡œ ì¸í•´ ëŒ€ê¸° ë˜ëŠ” ë‹¤ë¥¸ í‚¤ ì‹œë„.")
                # ì—¬ê¸°ì„œëŠ” ë°”ë¡œ ë‹¤ìŒ í‚¤ë¡œ ë„˜ì–´ê°€ê±°ë‚˜, ì§§ê²Œ ëŒ€ê¸° í›„ ì¬ì‹œë„í•  ìˆ˜ ìˆìŒ
                # ê°„ë‹¨í•˜ê²Œ ë‹¤ìŒ ëª¨ë¸ë¡œ ë„˜ì–´ê°€ë„ë¡ ì²˜ë¦¬
                continue

            try:
                model_instance = current_model_info['model']
                # ë²¡í„° DB ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆëŠ”ì§€ í™•ì¸
                has_vector_search = data_context and 'vector_search' in data_context and data_context['vector_search'].get('success')
                
                if has_vector_search:
                    # ë²¡í„° DB ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆì„ ë•Œ - ê°„ë‹¨í•˜ê³  ì§ì ‘ì ì¸ ë‹µë³€
                    vector_data = data_context['vector_search']
                    chart_data = vector_data.get('chart_data', {})
                    documents = vector_data.get('results', {}).get('documents', [[]])[0] if vector_data.get('results') else []
                    
                    prompt = f"""
ë‹¹ì‹ ì€ ì°½ê³  ê´€ë¦¬ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì‹¤ì œ ì°½ê³  ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°„ë‹¨í•˜ê³  ëª…í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

**ì‹¤ì œ ë°ì´í„° ê²€ìƒ‰ ê²°ê³¼:**
- ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(documents)}ê°œ
- ì°¨íŠ¸ ë°ì´í„°: {chart_data}
- ê´€ë ¨ ë¬¸ì„œ: {documents[:3]}  // ìƒìœ„ 3ê°œë§Œ í‘œì‹œ

**ì‘ë‹µ ê·œì¹™:**
1. ì§ˆë¬¸ì— ëŒ€í•´ ì§ì ‘ì ì´ê³  ê°„ë‹¨í•œ ë‹µë³€ì„ í•˜ì„¸ìš”
2. êµ¬ì²´ì ì¸ ìˆ«ìê°€ ìˆìœ¼ë©´ ëª…ì‹œí•˜ì„¸ìš”  
3. 3-5ë¬¸ì¥ ì´ë‚´ë¡œ ë‹µë³€í•˜ì„¸ìš”
4. ê¸°ìˆ ì  ë¶„ì„ì€ ìš”ì²­ë°›ì„ ë•Œë§Œ ì œê³µí•˜ì„¸ìš”

**ì§ˆë¬¸:** {question}

**ë‹µë³€ ì˜ˆì‹œ:**
- "ì´ ì¬ê³ ëŸ‰ì€ ì•½ 1,234ê°œì…ë‹ˆë‹¤. í˜„ì¬ Aë™ì— 456ê°œ, Bë™ì— 789ê°œê°€ ìˆìŠµë‹ˆë‹¤."
- "ì˜¤ëŠ˜ ì…ê³ ëŸ‰ì€ 50ê°œ, ì¶œê³ ëŸ‰ì€ 30ê°œë¡œ ìˆœì¦ê°€ 20ê°œì…ë‹ˆë‹¤."
- "ì¬ê³ ê°€ ë¶€ì¡±í•œ ì œí’ˆì€ ì œí’ˆA(5ê°œ ë‚¨ìŒ), ì œí’ˆB(3ê°œ ë‚¨ìŒ)ì…ë‹ˆë‹¤."
"""
                else:
                    # ë²¡í„° DB ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì„ ë•Œ - ê¸°ì¡´ ë°ì´í„° ë¶„ì„ ë°©ì‹
                    prompt = f"""
ë‹¹ì‹ ì€ ì°½ê³  ê´€ë¦¬ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì œê³µëœ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

**ë°ì´í„° ì»¨í…ìŠ¤íŠ¸:**
{data_context}

**ì‘ë‹µ ê·œì¹™:**
1. ë°ì´í„°ê°€ ìˆìœ¼ë©´ êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë¥¼ í¬í•¨í•œ ë‹µë³€
2. ë°ì´í„°ê°€ ë¶€ì¡±í•˜ë©´ ê·¸ ì‚¬ì‹¤ì„ ëª…ì‹œí•˜ê³  ê°„ë‹¨í•œ ê°€ì´ë“œ ì œê³µ
3. 5ë¬¸ì¥ ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ë‹µë³€
4. ë¶ˆí•„ìš”í•œ ê¸°ìˆ ì  ë¶„ì„ì€ í”¼í•˜ì„¸ìš”

**ì§ˆë¬¸:** {question}
"""
                # Gemini API í˜¸ì¶œ
                self.logger.info(f"ğŸ”„ {current_model_info['name']} API í˜¸ì¶œ ì‹œì‘...")
                self.logger.debug(f"ğŸ“¤ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)}")
                
                try:
                    # Gemini API ì•ˆì „ ì„¤ì • ì¶”ê°€
                    safety_settings = [
                        {
                            "category": "HARM_CATEGORY_HARASSMENT",
                            "threshold": "BLOCK_MEDIUM_AND_ABOVE"
                        },
                        {
                            "category": "HARM_CATEGORY_HATE_SPEECH", 
                            "threshold": "BLOCK_MEDIUM_AND_ABOVE"
                        },
                        {
                            "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
                            "threshold": "BLOCK_MEDIUM_AND_ABOVE"
                        },
                        {
                            "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
                            "threshold": "BLOCK_MEDIUM_AND_ABOVE"
                        }
                    ]
                    
                    # async ìš°ì„  ì‹œë„
                    try:
                        response = await model_instance.generate_content_async(
                            prompt,
                            safety_settings=safety_settings
                        )
                    except AttributeError:
                        # generate_content_asyncê°€ ì—†ëŠ” ê²½ìš° sync í˜¸ì¶œ
                        self.logger.info(f"ğŸ”„ Async ë©”ì„œë“œ ì—†ìŒ, sync í˜¸ì¶œë¡œ ëŒ€ì²´")
                        response = model_instance.generate_content(
                            prompt,
                            safety_settings=safety_settings
                        )
                except Exception as api_error:
                    self.logger.warning(f"âš ï¸ API í˜¸ì¶œ ì‹¤íŒ¨: {api_error}")
                    # ì•ˆì „ ì„¤ì • ì—†ì´ ì¬ì‹œë„
                    try:
                        response = model_instance.generate_content(prompt)
                    except Exception as fallback_error:
                        raise Exception(f"ëª¨ë“  API í˜¸ì¶œ ë°©ì‹ ì‹¤íŒ¨: {fallback_error}")
                
                # ì‘ë‹µ ìƒì„¸ ë¡œê¹…
                self.logger.debug(f"ğŸ“¥ ì‘ë‹µ ê°ì²´ íƒ€ì…: {type(response)}")
                self.logger.debug(f"ğŸ“¥ ì‘ë‹µ ê°ì²´ ì†ì„±: {[attr for attr in dir(response) if not attr.startswith('_')]}")
                
                # ì•ˆì „í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                result_text = ""
                if hasattr(response, 'text'):
                    result_text = response.text
                elif hasattr(response, 'content'):
                    result_text = str(response.content)
                elif hasattr(response, 'candidates') and response.candidates:
                    # Gemini ì‘ë‹µ êµ¬ì¡°ì— ë”°ë¥¸ ì²˜ë¦¬
                    candidate = response.candidates[0]
                    if hasattr(candidate, 'content'):
                        if hasattr(candidate.content, 'parts'):
                            result_text = candidate.content.parts[0].text
                        else:
                            result_text = str(candidate.content)
                else:
                    result_text = str(response)
                
                self.logger.info(f"ğŸ“ ì‘ë‹µ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(result_text)}")
                
                if not result_text or result_text.strip() == "":
                    self.logger.error(f"âŒ ë¹ˆ ì‘ë‹µì„ ë°›ì•˜ìŠµë‹ˆë‹¤!")
                    return "ì˜¤ë¥˜: Gemini APIì—ì„œ ë¹ˆ ì‘ë‹µì„ ë°›ì•˜ìŠµë‹ˆë‹¤."
                
                self.logger.info(f"âœ… {current_model_info['name']} API ì„±ê³µ - ì‘ë‹µ (ì¼ë¶€): {result_text[:200]}...")
                return result_text

            except Exception as e:
                current_model_info['failures'] += 1
                self.logger.warning(f"âš ï¸ {current_model_info['name']} API ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_attempts}): {str(e)}")

                # ì§€ìˆ˜ ë°±ì˜¤í”„ ì ìš© (RateLimiterì˜ acquire_permissionì—ì„œ ì´ë¯¸ ì²˜ë¦¬ë˜ì§€ë§Œ, ì—¬ê¸°ì„œë„ ì¶”ê°€ ê°€ëŠ¥)
                await asyncio.sleep(0.5 * (2 ** attempt)) # ê°„ë‹¨í•œ ë°±ì˜¤í”„

                if attempt < max_attempts - 1:
                    self.logger.info(f"ğŸ”„ ë‹¤ìŒ ëª¨ë¸ë¡œ ì¬ì‹œë„ ì¤‘...")
                    continue
                else:
                    self.logger.error(f"âŒ ëª¨ë“  Gemini ëª¨ë¸ ì‹œë„ ì‹¤íŒ¨")
                    return f"ì˜¤ë¥˜: ëª¨ë“  API í˜¸ì¶œ ì‹¤íŒ¨ - ë§ˆì§€ë§‰ ì˜¤ë¥˜: {str(e)}"

        return "ì˜¤ë¥˜: ëª¨ë“  ëª¨ë¸ ì‹œë„ ì‹¤íŒ¨"
    
    async def process_query(self, prompt: str) -> str:
        """ì°¨íŠ¸ ìƒì„±ì„ ìœ„í•œ ë‹¨ìˆœí•œ í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ ë©”ì„œë“œ"""
        # ì°¨íŠ¸ ìƒì„±ì—ì„œëŠ” ë°•í•˜í•œ ë°ì´í„° ì»¨í…ìŠ¤íŠ¸ë¡œ í˜¸ì¶œ
        context_data = {"chart_generation": True, "prompt_only": True}
        return await self.answer_query(prompt, context_data)
    
    async def generate_chart_config(self, user_request: str, available_data: dict) -> dict:
        """ì‚¬ìš©ì ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ì°¨íŠ¸ ì„¤ì •ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„° ìš”ì•½
        data_summary = self._summarize_available_data(available_data)
        
        chart_prompt = f"""
ë‹¹ì‹ ì€ ë°ì´í„° ì‹œê°í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ìì—°ì–´ ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ Chart.js í˜¸í™˜ ì°¨íŠ¸ ì„¤ì •ì„ ìƒì„±í•´ì£¼ì„¸ìš”.

**ì‚¬ìš©ì ìš”ì²­**: {user_request}

**ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°**:
{data_summary}

**ì‘ë‹µ í˜•ì‹**: ë°˜ë“œì‹œ ì•„ë˜ JSON êµ¬ì¡°ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”.

```json
{{
    "chart_type": "bar|line|pie|doughnut|radar|scatter",
    "title": "ì°¨íŠ¸ ì œëª©",
    "data": {{
        "labels": ["ë¼ë²¨1", "ë¼ë²¨2", "ë¼ë²¨3"],
        "datasets": [{{
            "label": "ë°ì´í„°ì…‹ ì´ë¦„",
            "data": [ê°’1, ê°’2, ê°’3],
            "backgroundColor": ["ìƒ‰ìƒ1", "ìƒ‰ìƒ2", "ìƒ‰ìƒ3"],
            "borderColor": "í…Œë‘ë¦¬ìƒ‰ìƒ",
            "borderWidth": 1
        }}]
    }},
    "options": {{
        "responsive": true,
        "plugins": {{
            "title": {{
                "display": true,
                "text": "ì°¨íŠ¸ ì œëª©"
            }},
            "legend": {{
                "display": true,
                "position": "top"
            }}
        }},
        "scales": {{
            "y": {{
                "beginAtZero": true
            }}
        }}
    }},
    "query_info": {{
        "data_source": "ì‚¬ìš©ëœ ë°ì´í„° ì†ŒìŠ¤",
        "filters_applied": "ì ìš©ëœ í•„í„°ë§",
        "aggregation": "ì§‘ê³„ ë°©ì‹"
    }}
}}
```

**ì£¼ì˜ì‚¬í•­**:
1. ì‚¬ìš©ì ìš”ì²­ì— ê°€ì¥ ì í•©í•œ ì°¨íŠ¸ íƒ€ì…ì„ ì„ íƒí•˜ì„¸ìš”
2. ì‹¤ì œ ë°ì´í„°ì— ê¸°ë°˜í•˜ì—¬ realisticí•œ ê°’ì„ ì œê³µí•˜ì„¸ìš”
3. ìƒ‰ìƒì€ ì‹œê°ì ìœ¼ë¡œ êµ¬ë¶„ì´ ì˜ ë˜ë„ë¡ ì„ íƒí•˜ì„¸ìš”
4. JSON í˜•ì‹ì„ ì •í™•íˆ ì§€ì¼œì£¼ì„¸ìš”
"""
        
        try:
            # ì°¨íŠ¸ ì „ìš© ì„¤ì •ìœ¼ë¡œ API í˜¸ì¶œ
            original_config = self.gemini_config
            self.gemini_config = self.chart_config
            
            response = await self.process_query(chart_prompt)
            
            # ì›ë˜ ì„¤ì •ìœ¼ë¡œ ë³µì›
            self.gemini_config = original_config
            
            # JSON íŒŒì‹± ì‹œë„
            import json
            import re
            
            # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
            json_match = re.search(r'```json\s*(\{.*?\})\s*```', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # ```json íƒœê·¸ê°€ ì—†ë‹¤ë©´ ì „ì²´ì—ì„œ JSON ì°¾ê¸°
                json_match = re.search(r'\{.*\}', response, re.DOTALL)
                if json_match:
                    json_str = json_match.group(0)
                else:
                    raise ValueError("JSON í˜•ì‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            chart_config = json.loads(json_str)
            
            # í•„ìˆ˜ í•„ë“œ ê²€ì¦
            required_fields = ['chart_type', 'title', 'data']
            for field in required_fields:
                if field not in chart_config:
                    raise ValueError(f"í•„ìˆ˜ í•„ë“œ '{field}'ê°€ ì—†ìŠµë‹ˆë‹¤")
            
            self.logger.info(f"âœ… ì°¨íŠ¸ ì„¤ì • ìƒì„± ì„±ê³µ: {chart_config['chart_type']} - {chart_config['title']}")
            return {
                "success": True,
                "chart_config": chart_config,
                "message": "ì°¨íŠ¸ ì„¤ì •ì´ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤."
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ì°¨íŠ¸ ì„¤ì • ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "message": "ì°¨íŠ¸ ì„¤ì • ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
                "fallback_config": self._get_fallback_chart_config(user_request)
            }
    
    def _summarize_available_data(self, available_data: dict) -> str:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ë¥¼ ìš”ì•½í•˜ì—¬ ë¬¸ìì—´ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
        summary_lines = []
        
        for data_name, data_info in available_data.items():
            if isinstance(data_info, dict):
                summary_lines.append(f"- {data_name}: {data_info.get('description', 'ì„¤ëª… ì—†ìŒ')}")
                if 'columns' in data_info:
                    summary_lines.append(f"  ì»¬ëŸ¼: {', '.join(data_info['columns'][:5])}{'...' if len(data_info['columns']) > 5 else ''}")
                if 'row_count' in data_info:
                    summary_lines.append(f"  í–‰ ìˆ˜: {data_info['row_count']}")
            else:
                summary_lines.append(f"- {data_name}: {str(data_info)[:100]}...")
        
        return '\n'.join(summary_lines) if summary_lines else "ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„° ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."
    
    def _get_fallback_chart_config(self, user_request: str) -> dict:
        """AI ìƒì„± ì‹¤íŒ¨ ì‹œ ì‚¬ìš©í•  ê¸°ë³¸ ì°¨íŠ¸ ì„¤ì •ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return {
            "chart_type": "bar",
            "title": "ë°ì´í„° ì°¨íŠ¸",
            "data": {
                "labels": ["ë°ì´í„° 1", "ë°ì´í„° 2", "ë°ì´í„° 3"],
                "datasets": [{
                    "label": "ê¸°ë³¸ ë°ì´í„°ì…‹",
                    "data": [10, 20, 30],
                    "backgroundColor": ["#FF6384", "#36A2EB", "#FFCE56"],
                    "borderColor": "#36A2EB",
                    "borderWidth": 1
                }]
            },
            "options": {
                "responsive": True,
                "plugins": {
                    "title": {
                        "display": True,
                        "text": "ê¸°ë³¸ ì°¨íŠ¸"
                    },
                    "legend": {
                        "display": True,
                        "position": "top"
                    }
                },
                "scales": {
                    "y": {
                        "beginAtZero": True
                    }
                }
            },
            "query_info": {
                "data_source": "ê¸°ë³¸ ë°ì´í„°",
                "filters_applied": "ì—†ìŒ",
                "aggregation": "ê¸°ë³¸"
            }
        }
    
    async def analyze_image_with_prompt(self, image_data: str, prompt: str) -> Dict[str, Any]:
        """
        Gemini Vision APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ ë¶„ì„
        
        Args:
            image_data: base64 ì¸ì½”ë”©ëœ ì´ë¯¸ì§€ ë°ì´í„°
            prompt: ë¶„ì„ ìš”ì²­ í”„ë¡¬í”„íŠ¸
        
        Returns:
            ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        try:
            # API í‚¤ ì„ íƒ
            api_key = self._get_best_api_key()
            if not api_key:
                raise Exception("ì‚¬ìš© ê°€ëŠ¥í•œ API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            # Gemini ëª¨ë¸ ì„¤ì •
            genai.configure(api_key=api_key)
            model = genai.GenerativeModel('gemini-1.5-flash')
            
            # ì´ë¯¸ì§€ ë°ì´í„° ì¤€ë¹„
            import base64
            from io import BytesIO
            
            # base64 ë””ì½”ë”©
            image_bytes = base64.b64decode(image_data)
            
            # PIL Image ê°ì²´ ìƒì„±
            try:
                from PIL import Image
                image = Image.open(BytesIO(image_bytes))
            except ImportError:
                raise Exception("PIL(Pillow) ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            # Gemini Vision API í˜¸ì¶œ
            response = model.generate_content([prompt, image])
            
            if response and response.text:
                # API í˜¸ì¶œ ì„±ê³µ ê¸°ë¡
                self._record_api_success(api_key)
                
                return {
                    "success": True,
                    "response": response.text.strip(),
                    "model": "gemini-1.5-flash",
                    "api_key_used": api_key[-10:] if api_key else "unknown"
                }
            else:
                raise Exception("Gemini API ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        
        except Exception as e:
            error_msg = str(e)
            self.logger.error(f"ì´ë¯¸ì§€ ë¶„ì„ ì˜¤ë¥˜: {error_msg}")
            
            # API í‚¤ ì˜¤ë¥˜ ê¸°ë¡
            if api_key:
                self._record_api_error(api_key, error_msg)
            
            return {
                "success": False,
                "error": error_msg,
                "response": None
            } 