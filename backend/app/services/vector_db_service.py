"""
ChromaDBë¥¼ í™œìš©í•œ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì„œë¹„ìŠ¤
"""
import os
import logging
import pandas as pd
from typing import List, Dict, Any, Optional
from datetime import datetime
import json

try:
    import chromadb
    from chromadb.config import Settings
    from sentence_transformers import SentenceTransformer
except ImportError:
    chromadb = None
    SentenceTransformer = None

class VectorDBService:
    """ChromaDBë¥¼ í™œìš©í•œ ì°½ê³  ë°ì´í„° ë²¡í„°í™” ë° ê²€ìƒ‰ ì„œë¹„ìŠ¤"""
    
    def __init__(self, data_service=None, logger=None):
        self.logger = logger or logging.getLogger(__name__)
        self.data_service = data_service
        self.client = None
        self.collection = None
        self.encoder = None
        self.is_initialized = False
        
        if chromadb is None or SentenceTransformer is None:
            self.logger.warning("âš ï¸ ChromaDB ë˜ëŠ” SentenceTransformersê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            self.logger.warning("pip install chromadb sentence-transformersë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
            return
        
        self._initialize_chromadb()
    
    def _initialize_chromadb(self):
        """ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        try:
            # ChromaDB í´ë¼ì´ì–¸íŠ¸ ìƒì„± (ë¡œì»¬ ëª¨ë“œ)
            self.client = chromadb.PersistentClient(
                path="./chromadb_storage",
                settings=Settings(anonymized_telemetry=False)
            )
            
            # ì»¬ë ‰ì…˜ ìƒì„± ë˜ëŠ” ê°€ì ¸ì˜¤ê¸°
            collection_name = "warehouse_data"
            try:
                self.collection = self.client.get_collection(collection_name)
                self.logger.info(f"âœ… ê¸°ì¡´ ChromaDB ì»¬ë ‰ì…˜ ë¡œë“œ: {collection_name}")
            except:
                self.collection = self.client.create_collection(collection_name)
                self.logger.info(f"âœ… ìƒˆ ChromaDB ì»¬ë ‰ì…˜ ìƒì„±: {collection_name}")
            
            # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” (í•œêµ­ì–´ ì§€ì›, fallback í¬í•¨)
            try:
                self.encoder = SentenceTransformer('jhgan/ko-sroberta-multitask')
                self.logger.info("âœ… í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            except Exception as korean_model_error:
                self.logger.warning(f"âš ï¸ í•œêµ­ì–´ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {korean_model_error}")
                try:
                    # ë‹¤êµ­ì–´ ëª¨ë¸ë¡œ fallback
                    self.encoder = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
                    self.logger.info("âœ… ë‹¤êµ­ì–´ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (fallback)")
                except Exception as fallback_error:
                    self.logger.warning(f"âš ï¸ ë‹¤êµ­ì–´ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {fallback_error}")
                    # ê°€ì¥ ê¸°ë³¸ì ì¸ ì˜ì–´ ëª¨ë¸ë¡œ fallback
                    self.encoder = SentenceTransformer('all-MiniLM-L6-v2')
                    self.logger.info("âœ… ì˜ì–´ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (final fallback)")
            
            self.is_initialized = True
            
        except Exception as e:
            self.logger.error(f"âŒ ChromaDB ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            self.is_initialized = False
    
    async def index_warehouse_data(self, force_rebuild=False):
        """ì°½ê³  ë°ì´í„°ë¥¼ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì¸ë±ì‹± (ê°•ì œ ë¦¬ë¹Œë“œ ì§€ì›)"""
        if not self.is_initialized or not self.data_service:
            self.logger.warning("âš ï¸ VectorDB ë˜ëŠ” DataServiceê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return False
        
        try:
            # ë°ì´í„° ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
            self.logger.info(f"ğŸ“Š ë°ì´í„° ë¡œë“œ ìƒíƒœ: {self.data_service.data_loaded}")
            self.logger.info(f"ğŸ“Š ì…ê³  ë°ì´í„°: {len(self.data_service.inbound_data) if self.data_service.inbound_data is not None else 0}ê°œ")
            self.logger.info(f"ğŸ“Š ì¶œê³  ë°ì´í„°: {len(self.data_service.outbound_data) if self.data_service.outbound_data is not None else 0}ê°œ")
            self.logger.info(f"ğŸ“Š ì œí’ˆ ë°ì´í„°: {len(self.data_service.product_master) if self.data_service.product_master is not None else 0}ê°œ")
            
            # ğŸ“… ë‚ ì§œ ë²”ìœ„ ìƒì„¸ í™•ì¸
            if not self.data_service.inbound_data.empty and 'Date' in self.data_service.inbound_data.columns:
                import pandas as pd
                dates = pd.to_datetime(self.data_service.inbound_data['Date'], errors='coerce').dropna()
                unique_dates = sorted(dates.dt.strftime('%Y-%m-%d').unique())
                self.logger.info(f"ğŸ“… [VECTOR_INDEX] ì…ê³  ë°ì´í„° ë‚ ì§œ: {unique_dates}")
            
            if not self.data_service.outbound_data.empty and 'Date' in self.data_service.outbound_data.columns:
                import pandas as pd
                dates = pd.to_datetime(self.data_service.outbound_data['Date'], errors='coerce').dropna()
                unique_dates = sorted(dates.dt.strftime('%Y-%m-%d').unique())
                self.logger.info(f"ğŸ“… [VECTOR_INDEX] ì¶œê³  ë°ì´í„° ë‚ ì§œ: {unique_dates}")
            
            # ê¸°ì¡´ ë°ì´í„° í™•ì¸
            existing_count = self.collection.count()
            
            # ğŸ”¥ ê°•ì œ ë¦¬ë¹Œë“œ ëª¨ë“œ: ë°ì´í„° ì¼ê´€ì„± í™•ë³´
            if force_rebuild or existing_count == 0:
                if existing_count > 0:
                    self.logger.info(f"ğŸ”„ ê°•ì œ ë¦¬ë¹Œë“œ ëª¨ë“œ: ê¸°ì¡´ {existing_count}ê°œ ë¬¸ì„œ ì‚­ì œ")
                    self.collection.delete(where={})
                    self.logger.info("ğŸ—‘ï¸ ê¸°ì¡´ ë²¡í„° ë°ì´í„° ì™„ì „ ì‚­ì œ")
                else:
                    self.logger.info("ğŸ†• ì´ˆê¸° ì¸ë±ì‹± ì‹œì‘")
            else:
                self.logger.info(f"âœ… ê¸°ì¡´ ë²¡í„° ë°ì´í„° ì‚¬ìš©: {existing_count}ê°œ ë¬¸ì„œ")
                return True
            
            documents = []
            metadatas = []
            ids = []
            
            # ì…ê³  ë°ì´í„° ì¸ë±ì‹±
            if self.data_service.inbound_data is not None and len(self.data_service.inbound_data) > 0:
                inbound_docs, inbound_metas, inbound_ids = self._process_inbound_data()
                documents.extend(inbound_docs)
                metadatas.extend(inbound_metas)
                ids.extend(inbound_ids)
            else:
                self.logger.warning("âš ï¸ ì…ê³  ë°ì´í„°ê°€ ì—†ê±°ë‚˜ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            
            # ì¶œê³  ë°ì´í„° ì¸ë±ì‹±
            if self.data_service.outbound_data is not None and len(self.data_service.outbound_data) > 0:
                outbound_docs, outbound_metas, outbound_ids = self._process_outbound_data()
                documents.extend(outbound_docs)
                metadatas.extend(outbound_metas)
                ids.extend(outbound_ids)
            else:
                self.logger.warning("âš ï¸ ì¶œê³  ë°ì´í„°ê°€ ì—†ê±°ë‚˜ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            
            # ìƒí’ˆ ë§ˆìŠ¤í„° ë°ì´í„° ì¸ë±ì‹±
            if self.data_service.product_master is not None and len(self.data_service.product_master) > 0:
                product_docs, product_metas, product_ids = self._process_product_data()
                documents.extend(product_docs)
                metadatas.extend(product_metas)
                ids.extend(product_ids)
            else:
                self.logger.warning("âš ï¸ ì œí’ˆ ë§ˆìŠ¤í„° ë°ì´í„°ê°€ ì—†ê±°ë‚˜ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            
            if documents:
                # ì„ë² ë”© ìƒì„±
                self.logger.info(f"ğŸ”„ {len(documents)}ê°œ ë¬¸ì„œ ì„ë² ë”© ìƒì„± ì¤‘...")
                embeddings = self.encoder.encode(documents).tolist()
                
                # ChromaDBì— ì €ì¥
                self.collection.add(
                    documents=documents,
                    metadatas=metadatas,
                    embeddings=embeddings,
                    ids=ids
                )
                
                # ì¸ë±ì‹± ê²°ê³¼ ìƒì„¸ ì •ë³´
                type_counts = {}
                for meta in metadatas:
                    data_type = meta.get('type', 'unknown')
                    type_counts[data_type] = type_counts.get(data_type, 0) + 1
                
                self.logger.info(f"âœ… ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì¸ë±ì‹± ì™„ë£Œ:")
                self.logger.info(f"  ğŸ“Š ì´ ë¬¸ì„œ: {len(documents)}ê°œ")
                for data_type, count in type_counts.items():
                    self.logger.info(f"  ğŸ“‹ {data_type}: {count}ê°œ")
                
                return True
            else:
                self.logger.warning("âš ï¸ ì¸ë±ì‹±í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ ë²¡í„° ì¸ë±ì‹± ì‹¤íŒ¨: {str(e)}")
            return False
    
    def _process_inbound_data(self):
        """ì…ê³  ë°ì´í„°ë¥¼ ë¬¸ì„œí™”"""
        documents = []
        metadatas = []
        ids = []
        
        df = self.data_service.inbound_data
        self.logger.info(f"ğŸ“¦ ì…ê³  ë°ì´í„° ì²˜ë¦¬ ì‹œì‘: {len(df)}ê°œ í–‰")
        self.logger.info(f"ğŸ“¦ ì…ê³  ë°ì´í„° ì»¬ëŸ¼: {list(df.columns)}")
        
        for idx, row in df.iterrows():
            try:
                # ìì—°ì–´ ë¬¸ì„œ ìƒì„± (ì‹¤ì œ ì»¬ëŸ¼ëª… ì‚¬ìš©)
                doc = f"""
                ì…ê³  ì •ë³´: {row.get('Supplier', 'ì•Œ ìˆ˜ ì—†ìŒ')} ê³µê¸‰ì—…ì²´ì—ì„œ {row.get('ProductName', 'ì•Œ ìˆ˜ ì—†ìŒ')} ìƒí’ˆì„ 
                {row.get('PalleteQty', 0)}ê°œ íŒŒë ˆíŠ¸ ì…ê³ í–ˆìŠµë‹ˆë‹¤. 
                ë‚ ì§œ: {row.get('Date', 'ì•Œ ìˆ˜ ì—†ìŒ')}, 
                ìƒí’ˆì½”ë“œ: {row.get('ProductCode', 'ì•Œ ìˆ˜ ì—†ìŒ')}, 
                ì…ê³ ìœ„ì¹˜: {row.get('InboundPosition', 'ì•Œ ìˆ˜ ì—†ìŒ')},
                ì…ê³ ë¼ì¸: {row.get('InboundLine', 'ì•Œ ìˆ˜ ì—†ìŒ')}
                """
                
                # ë©”íƒ€ë°ì´í„° (ì‹¤ì œ ì°¨íŠ¸ì— ì‚¬ìš©ë  ìˆ˜ì¹˜ ë°ì´í„°)
                metadata = {
                    "type": "inbound",
                    "supplier": str(row.get('Supplier', '')),
                    "product_code": str(row.get('ProductCode', '')),
                    "product_name": str(row.get('ProductName', '')),
                    "quantity": float(row.get('PalleteQty', 0)),
                    "date": str(row.get('Date', '')),
                    "position": str(row.get('InboundPosition', '')),
                    "inbound_line": str(row.get('InboundLine', '')),
                    "row_index": int(idx)
                }
                
                documents.append(doc.strip())
                metadatas.append(metadata)
                ids.append(f"inbound_{idx}")
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ ì…ê³  ë°ì´í„° í–‰ {idx} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue
        
        self.logger.info(f"âœ… ì…ê³  ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ: {len(documents)}ê°œ ë¬¸ì„œ ìƒì„±")
        return documents, metadatas, ids
    
    def _process_outbound_data(self):
        """ì¶œê³  ë°ì´í„°ë¥¼ ë¬¸ì„œí™”"""
        documents = []
        metadatas = []
        ids = []
        
        df = self.data_service.outbound_data
        self.logger.info(f"ğŸ“¤ ì¶œê³  ë°ì´í„° ì²˜ë¦¬ ì‹œì‘: {len(df)}ê°œ í–‰")
        self.logger.info(f"ğŸ“¤ ì¶œê³  ë°ì´í„° ì»¬ëŸ¼: {list(df.columns)}")
        
        for idx, row in df.iterrows():
            try:
                # ìì—°ì–´ ë¬¸ì„œ ìƒì„± (ì‹¤ì œ ì»¬ëŸ¼ëª… ì‚¬ìš©)
                doc = f"""
                ì¶œê³  ì •ë³´: {row.get('Business name', 'ì•Œ ìˆ˜ ì—†ìŒ')} ê³ ê°ì‚¬ë¡œ {row.get('ProductName', 'ì•Œ ìˆ˜ ì—†ìŒ')} ìƒí’ˆì„ 
                {row.get('PalleteQty', 0)}ê°œ íŒŒë ˆíŠ¸ ì¶œê³ í–ˆìŠµë‹ˆë‹¤. 
                ë‚ ì§œ: {row.get('Date', 'ì•Œ ìˆ˜ ì—†ìŒ')}, 
                ìƒí’ˆì½”ë“œ: {row.get('ProductCode', 'ì•Œ ìˆ˜ ì—†ìŒ')}, 
                ì¶œê³ ìœ„ì¹˜: {row.get('ProductPosition', 'ì•Œ ìˆ˜ ì—†ìŒ')},
                ì¶œê³ ë¼ì¸: {row.get('OutboundLine', 'ì•Œ ìˆ˜ ì—†ìŒ')}
                """
                
                # ë©”íƒ€ë°ì´í„°
                metadata = {
                    "type": "outbound",
                    "business_name": str(row.get('Business name', '')),
                    "product_code": str(row.get('ProductCode', '')),
                    "product_name": str(row.get('ProductName', '')),
                    "quantity": float(row.get('PalleteQty', 0)),
                    "date": str(row.get('Date', '')),
                    "position": str(row.get('ProductPosition', '')),
                    "outbound_line": str(row.get('OutboundLine', '')),
                    "row_index": int(idx)
                }
                
                documents.append(doc.strip())
                metadatas.append(metadata)
                ids.append(f"outbound_{idx}")
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ ì¶œê³  ë°ì´í„° í–‰ {idx} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue
        
        self.logger.info(f"âœ… ì¶œê³  ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ: {len(documents)}ê°œ ë¬¸ì„œ ìƒì„±")
        return documents, metadatas, ids
    
    def _process_product_data(self):
        """ìƒí’ˆ ë§ˆìŠ¤í„° ë°ì´í„°ë¥¼ ë¬¸ì„œí™” (ì‹¤ì œ ì»¬ëŸ¼ëª… ê¸°ë°˜ ê°œì„ )"""
        documents = []
        metadatas = []
        ids = []
        
        df = self.data_service.product_master
        self.logger.info(f"ğŸ“‹ ì œí’ˆ ë°ì´í„° ì²˜ë¦¬ ì‹œì‘: {len(df)}ê°œ í–‰")
        self.logger.info(f"ğŸ“‹ ì œí’ˆ ë°ì´í„° ì»¬ëŸ¼: {list(df.columns)}")
        
        # ğŸ”§ ì‹¤ì œ ì»¬ëŸ¼ëª… ë§¤í•‘ (ë¡œê·¸ì—ì„œ í™•ì¸ëœ ì‹¤ì œ ì»¬ëŸ¼ëª… ì‚¬ìš©)
        rack_column_options = ['ë™ìœ„ì¹˜', 'Rack Name', 'Rack Code Name']
        rack_column = None
        for col in rack_column_options:
            if col in df.columns:
                rack_column = col
                self.logger.info(f"ğŸ” ë™ ì •ë³´ ì»¬ëŸ¼ ì‚¬ìš©: {rack_column}")
                break
        
        if not rack_column:
            self.logger.warning(f"âš ï¸ ë™ ì •ë³´ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ. ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {list(df.columns)}")
            rack_column = 'ë™ìœ„ì¹˜'  # ê¸°ë³¸ê°’
        
        for idx, row in df.iterrows():
            try:
                # ğŸ·ï¸ ì‹¤ì œ ë™ ì •ë³´ ì¶”ì¶œ
                rack_info = str(row.get(rack_column, 'ì•Œ ìˆ˜ ì—†ìŒ'))
                product_name = str(row.get('ProductName', 'ì•Œ ìˆ˜ ì—†ìŒ'))
                product_code = str(row.get('ìƒí’ˆì½”ë“œ', row.get('ProductCode', 'ì•Œ ìˆ˜ ì—†ìŒ')))
                current_stock = row.get('í˜„ì¬ê³ ', row.get('Start Pallete Qty', 0))
                unit = str(row.get('Unit', 'ê°œ'))
                
                # ğŸ“ ìì—°ì–´ ë¬¸ì„œ ìƒì„± (ë™ ì •ë³´ ê°•í™”)
                doc = f"""
                ìƒí’ˆëª…: {product_name} (ìƒí’ˆì½”ë“œ: {product_code})
                í˜„ì¬ ì¬ê³ ëŸ‰: {current_stock} {unit}
                ì €ì¥ ìœ„ì¹˜: {rack_info}ë™
                ì‹œì‘ ì¬ê³ ëŸ‰: {row.get('Start Pallete Qty', 0)}
                ë™ ìœ„ì¹˜ ì •ë³´: {rack_info}ë™ì— ì €ì¥ëœ {product_name} ìƒí’ˆ
                """
                
                # ğŸ“Š ë©”íƒ€ë°ì´í„° (ì‹¤ì œ ì»¬ëŸ¼ëª… ê¸°ë°˜)
                metadata = {
                    "type": "product",
                    "product_code": product_code,
                    "product_name": product_name,
                    "current_stock": float(current_stock) if current_stock else 0.0,
                    "unit": unit,
                    "rack_name": rack_info,
                    "rack_location": rack_info,  # ê²€ìƒ‰ìš© ì¶”ê°€ í•„ë“œ
                    "start_qty": float(row.get('Start Pallete Qty', 0)),
                    "row_index": int(idx),
                    "rack_column_used": rack_column  # ë””ë²„ê¹…ìš©
                }
                
                documents.append(doc.strip())
                metadatas.append(metadata)
                ids.append(f"product_{idx}")
                
                # ğŸ” ë””ë²„ê¹…: ì²˜ìŒ 5ê°œ í•­ëª© ë¡œê·¸
                if idx < 5:
                    self.logger.info(f"ğŸ“¦ ìƒí’ˆ {idx}: {product_name} â†’ {rack_info}ë™ ({current_stock} {unit})")
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ ì œí’ˆ ë°ì´í„° í–‰ {idx} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue
        
        # ğŸ“Š ë™ë³„ í†µê³„ ìƒì„±
        rack_stats = {}
        for meta in metadatas:
            rack = meta['rack_name']
            if rack not in rack_stats:
                rack_stats[rack] = {'count': 0, 'total_stock': 0}
            rack_stats[rack]['count'] += 1
            rack_stats[rack]['total_stock'] += meta['current_stock']
        
        self.logger.info(f"âœ… ì œí’ˆ ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ: {len(documents)}ê°œ ë¬¸ì„œ ìƒì„±")
        self.logger.info(f"ğŸ“Š ë™ë³„ í†µê³„: {dict(sorted(rack_stats.items()))}")
        return documents, metadatas, ids
    
    async def search_relevant_data(self, query: str, n_results: int = 20) -> Dict[str, Any]:
        """ì‚¬ìš©ì ì¿¼ë¦¬ì™€ ê´€ë ¨ëœ ë°ì´í„° ê²€ìƒ‰"""
        self.logger.info(f"ğŸ” [VECTOR_SEARCH] ê²€ìƒ‰ ì‹œì‘: '{query}' (ìµœëŒ€ {n_results}ê°œ)")
        
        if not self.is_initialized:
            self.logger.error("âŒ [VECTOR_ERROR] ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return {"error": "ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
        
        try:
            # ì¿¼ë¦¬ ì„ë² ë”©
            self.logger.info("ğŸ”„ [VECTOR_EMBEDDING] ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±")
            query_embedding = self.encoder.encode([query]).tolist()[0]
            self.logger.info(f"ğŸ“Š [VECTOR_EMBEDDING] ì„ë² ë”© ì°¨ì›: {len(query_embedding)}")
            
            # ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰
            self.logger.info(f"ğŸ” [VECTOR_QUERY] ChromaDB ê²€ìƒ‰ ìˆ˜í–‰ (n_results={n_results})")
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=n_results,
                include=['documents', 'metadatas', 'distances']
            )
            
            if not results['documents'] or not results['documents'][0]:
                self.logger.warning("âš ï¸ [VECTOR_EMPTY] ê´€ë ¨ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return {"error": "ê´€ë ¨ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
            
            # ê²€ìƒ‰ ê²°ê³¼ ì •ë¦¬
            documents = results['documents'][0]
            metadatas = results['metadatas'][0]
            distances = results['distances'][0]
            
            self.logger.info(f"âœ… [VECTOR_SUCCESS] ê²€ìƒ‰ ì™„ë£Œ: {len(documents)}ê°œ ë¬¸ì„œ ë°œê²¬")
            self.logger.info(f"ğŸ“Š [VECTOR_STATS] í‰ê·  ê±°ë¦¬: {sum(distances)/len(distances):.3f}" if distances else "ğŸ“Š [VECTOR_STATS] ê±°ë¦¬ ì •ë³´ ì—†ìŒ")
            
            # ë©”íƒ€ë°ì´í„°ì—ì„œ ì‹¤ì œ ì°¨íŠ¸ ë°ì´í„° ì¶”ì¶œ
            self.logger.info("ğŸ“ˆ [VECTOR_CHART] ì°¨íŠ¸ ë°ì´í„° ì¶”ì¶œ ì‹œë„")
            chart_data = self._extract_chart_data_from_metadata(metadatas, query)
            self.logger.info(f"ğŸ“ˆ [VECTOR_CHART] ì°¨íŠ¸ ë°ì´í„° ì¶”ì¶œ ê²°ê³¼: {bool(chart_data)}")
            
            # ë©”íƒ€ë°ì´í„° ìš”ì•½
            self.logger.info("ğŸ“‹ [VECTOR_META] ë©”íƒ€ë°ì´í„° ìš”ì•½ ìƒì„±")
            metadata_summary = self._summarize_metadata(metadatas)
            self.logger.info(f"ğŸ“‹ [VECTOR_META] ë©”íƒ€ë°ì´í„° ìš”ì•½: {list(metadata_summary.keys()) if metadata_summary else 'None'}")
            
            return {
                "success": True,
                "query": query,
                "found_documents": len(documents),
                "documents": documents[:5],  # ìƒìœ„ 5ê°œ ë¬¸ì„œë§Œ ë°˜í™˜
                "chart_data": chart_data,
                "metadata_summary": metadata_summary
            }
            
        except Exception as e:
            self.logger.error(f"âŒ [VECTOR_ERROR] ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return {"error": f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}
    
    def _extract_chart_data_from_metadata(self, metadatas: List[Dict], query: str) -> Dict[str, Any]:
        """ë©”íƒ€ë°ì´í„°ì—ì„œ ì°¨íŠ¸ ë°ì´í„° ì¶”ì¶œ"""
        try:
            # ì¿¼ë¦¬ ë¶„ì„í•˜ì—¬ ì°¨íŠ¸ íƒ€ì… ì¶”ì •
            query_lower = query.lower()
            
            # ë°ì´í„° íƒ€ì…ë³„ ë¶„ë¥˜
            inbound_data = [m for m in metadatas if m.get('type') == 'inbound']
            outbound_data = [m for m in metadatas if m.get('type') == 'outbound']
            product_data = [m for m in metadatas if m.get('type') == 'product']
            
            chart_data = {}
            
            # ì…ê³ /ê³µê¸‰ì—…ì²´ ê´€ë ¨ ì¿¼ë¦¬ (í™•ì¥ëœ í‚¤ì›Œë“œ)
            inbound_keywords = ['ì…ê³ ', 'ê³µê¸‰ì—…ì²´', 'inbound', 'supplier', 'ë‚©í’ˆì—…ì²´', 'ì—…ì²´', 'ê³µê¸‰', 
                              'inboundline', 'inboundposition', 'ì…ê³ ë¼ì¸', 'ì…ê³ ìœ„ì¹˜', 'pallete']
            if any(word in query_lower for word in inbound_keywords):
                if inbound_data:
                    chart_data.update(self._process_inbound_chart_data(inbound_data, query_lower))
            
            # ì¶œê³ /ê³ ê° ê´€ë ¨ ì¿¼ë¦¬ (í™•ì¥ëœ í‚¤ì›Œë“œ)
            outbound_keywords = ['ì¶œê³ ', 'ê³ ê°', 'outbound', 'customer', 'business name', 'ê³ ê°ì‚¬',
                               'outboundline', 'productposition', 'ì¶œê³ ë¼ì¸', 'ì¶œê³ ìœ„ì¹˜']
            if any(word in query_lower for word in outbound_keywords):
                if outbound_data:
                    chart_data.update(self._process_outbound_chart_data(outbound_data, query_lower))
            
            # ì¬ê³ /ìƒí’ˆ/ë™ ê´€ë ¨ ì¿¼ë¦¬ (í™•ì¥ëœ í‚¤ì›Œë“œ)
            product_keywords = ['ì¬ê³ ', 'ìƒí’ˆ', 'ì œí’ˆ', 'ë™', 'inventory', 'product', 'rack', 'productcode', 
                              'productname', 'rack name', 'unit', 'start pallete qty', 'ìƒí’ˆì½”ë“œ', 'ì œí’ˆì½”ë“œ', 
                              'ë™ëª…', 'ë™ìœ„ì¹˜', 'ë‹¨ìœ„', 'ì‹œì‘ì¬ê³ ']
            if any(word in query_lower for word in product_keywords):
                if product_data:
                    chart_data.update(self._process_product_chart_data(product_data, query_lower))
            
            # ì „ì²´ ë°ì´í„°ê°€ í•„ìš”í•œ ê²½ìš°
            if not chart_data and metadatas:
                chart_data = self._process_general_chart_data(metadatas, query_lower)
            
            return chart_data
            
        except Exception as e:
            self.logger.error(f"âŒ ì°¨íŠ¸ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return {}
    
    def _process_inbound_chart_data(self, inbound_data: List[Dict], query: str) -> Dict[str, Any]:
        """ì…ê³  ë°ì´í„°ë¡œ ì°¨íŠ¸ ë°ì´í„° ìƒì„±"""
        if any(word in query for word in ['ê³µê¸‰ì—…ì²´', 'supplier', 'ë‚©í’ˆì—…ì²´', 'ì—…ì²´', 'ê³µê¸‰']):
            # ê³µê¸‰ì—…ì²´ë³„ ì§‘ê³„
            supplier_counts = {}
            for item in inbound_data:
                supplier = item.get('supplier', 'ì•Œ ìˆ˜ ì—†ìŒ')
                quantity = item.get('quantity', 0)
                supplier_counts[supplier] = supplier_counts.get(supplier, 0) + quantity
            
            # ìƒìœ„ 10ê°œ ê³µê¸‰ì—…ì²´ë¡œ ì •ë ¬
            sorted_suppliers = sorted(supplier_counts.items(), key=lambda x: x[1], reverse=True)[:10]
            
            return {
                "labels": [item[0] for item in sorted_suppliers],
                "data": [item[1] for item in sorted_suppliers],
                "title": "ì£¼ìš” ê³µê¸‰ì—…ì²´ë³„ ì…ê³ ëŸ‰ (ìƒìœ„ 10ê°œ)",
                "type": "inbound_by_supplier",
                "total_suppliers": len(supplier_counts),
                "top_supplier": sorted_suppliers[0] if sorted_suppliers else None
            }
        
        elif 'ë‚ ì§œ' in query or 'ì¼ë³„' in query or 'daily' in query:
            # ë‚ ì§œë³„ ì§‘ê³„
            date_counts = {}
            for item in inbound_data:
                date = item.get('date', 'ì•Œ ìˆ˜ ì—†ìŒ')
                quantity = item.get('quantity', 0)
                date_counts[date] = date_counts.get(date, 0) + quantity
            
            return {
                "labels": list(date_counts.keys()),
                "data": list(date_counts.values()),
                "title": "ì¼ë³„ ì…ê³ ëŸ‰",
                "type": "inbound_by_date"
            }
        
        return {}
    
    def _process_outbound_chart_data(self, outbound_data: List[Dict], query: str) -> Dict[str, Any]:
        """ì¶œê³  ë°ì´í„°ë¡œ ì°¨íŠ¸ ë°ì´í„° ìƒì„±"""
        if any(word in query for word in ['ê³ ê°', 'customer', 'business', 'ê³ ê°ì‚¬', 'business name']):
            # ê³ ê°ì‚¬ë³„ ì§‘ê³„
            business_counts = {}
            for item in outbound_data:
                business = item.get('business_name', 'ì•Œ ìˆ˜ ì—†ìŒ')
                quantity = item.get('quantity', 0)
                business_counts[business] = business_counts.get(business, 0) + quantity
            
            # ìƒìœ„ 10ê°œ ê³ ê°ì‚¬ë¡œ ì •ë ¬
            sorted_businesses = sorted(business_counts.items(), key=lambda x: x[1], reverse=True)[:10]
            
            return {
                "labels": [item[0] for item in sorted_businesses],
                "data": [item[1] for item in sorted_businesses],
                "title": "ì£¼ìš” ê³ ê°ì‚¬ë³„ ì¶œê³ ëŸ‰ (ìƒìœ„ 10ê°œ)",
                "type": "outbound_by_business",
                "total_customers": len(business_counts),
                "top_customer": sorted_businesses[0] if sorted_businesses else None
            }
        
        elif any(word in query for word in ['ìƒí’ˆ', 'product', 'productname', 'productcode', 'ì œí’ˆ']):
            # ìƒí’ˆë³„ ì§‘ê³„
            product_counts = {}
            for item in outbound_data:
                product = item.get('product_name', 'ì•Œ ìˆ˜ ì—†ìŒ')
                quantity = item.get('quantity', 0)
                product_counts[product] = product_counts.get(product, 0) + quantity
            
            # ìƒìœ„ 10ê°œ ìƒí’ˆìœ¼ë¡œ ì •ë ¬
            sorted_products = sorted(product_counts.items(), key=lambda x: x[1], reverse=True)[:10]
            
            return {
                "labels": [item[0] for item in sorted_products],
                "data": [item[1] for item in sorted_products],
                "title": "ì£¼ìš” ì¶œê³  ìƒí’ˆ (ìƒìœ„ 10ê°œ)",
                "type": "outbound_by_product",
                "total_products": len(product_counts),
                "top_product": sorted_products[0] if sorted_products else None
            }
        
        elif any(word in query for word in ['ë‚ ì§œ', 'ì¼ë³„', 'daily', 'date', 'ê¸°ê°„']):
            # ë‚ ì§œë³„ ì§‘ê³„
            date_counts = {}
            for item in outbound_data:
                date = item.get('date', 'ì•Œ ìˆ˜ ì—†ìŒ')
                quantity = item.get('quantity', 0)
                date_counts[date] = date_counts.get(date, 0) + quantity
            
            return {
                "labels": list(date_counts.keys()),
                "data": list(date_counts.values()),
                "title": "ì¼ë³„ ì¶œê³ ëŸ‰",
                "type": "outbound_by_date"
            }
        
        return {}
    
    def _process_product_chart_data(self, product_data: List[Dict], query: str) -> Dict[str, Any]:
        """ìƒí’ˆ ë°ì´í„°ë¡œ ì°¨íŠ¸ ë°ì´í„° ìƒì„±"""
        if any(word in query for word in ['ë™', 'rack', 'rack name', 'ë™ëª…', 'ë™ìœ„ì¹˜', 'ìœ„ì¹˜']):
            # ë™ë³„ ì§‘ê³„
            rack_counts = {}
            for item in product_data:
                rack = item.get('rack_name', 'ì•Œ ìˆ˜ ì—†ìŒ')
                stock = item.get('current_stock', 0)
                rack_counts[rack] = rack_counts.get(rack, 0) + stock
            
            # ë™ë³„ ì •ë ¬ (ì•ŒíŒŒë²³ ìˆœ)
            sorted_racks = sorted(rack_counts.items())
            
            return {
                "labels": [item[0] for item in sorted_racks],
                "data": [item[1] for item in sorted_racks],
                "title": "ë™ë³„ ì¬ê³ ëŸ‰",
                "type": "inventory_by_rack",
                "total_racks": len(rack_counts)
            }
        
        elif any(word in query for word in ['ìƒí’ˆ', 'product', 'productname', 'productcode', 'ì œí’ˆ', 'í’ˆëª©']):
            # ìƒí’ˆë³„ ì¬ê³ ëŸ‰ ì§‘ê³„
            product_counts = {}
            for item in product_data:
                product = item.get('product_name', 'ì•Œ ìˆ˜ ì—†ìŒ')
                stock = item.get('current_stock', 0)
                product_counts[product] = stock  # ì¬ê³ ëŠ” í•©ê³„ê°€ ì•„ë‹Œ ê°œë³„ ê°’
            
            # ìƒìœ„ 10ê°œ ìƒí’ˆìœ¼ë¡œ ì •ë ¬
            sorted_products = sorted(product_counts.items(), key=lambda x: x[1], reverse=True)[:10]
            
            return {
                "labels": [item[0] for item in sorted_products],
                "data": [item[1] for item in sorted_products],
                "title": "ìƒí’ˆë³„ ì¬ê³ ëŸ‰ (ìƒìœ„ 10ê°œ)",
                "type": "inventory_by_product",
                "total_products": len(product_counts)
            }
        
        elif any(word in query for word in ['unit', 'ë‹¨ìœ„', 'box', 'pac', 'kg', 'ea']):
            # ë‹¨ìœ„ë³„ ì§‘ê³„
            unit_counts = {}
            for item in product_data:
                unit = item.get('unit', 'ì•Œ ìˆ˜ ì—†ìŒ')
                unit_counts[unit] = unit_counts.get(unit, 0) + 1
            
            return {
                "labels": list(unit_counts.keys()),
                "data": list(unit_counts.values()),
                "title": "ë‹¨ìœ„ë³„ ìƒí’ˆ ê°œìˆ˜",
                "type": "products_by_unit"
            }
        
        return {}
    
    def _process_general_chart_data(self, metadatas: List[Dict], query: str) -> Dict[str, Any]:
        """ì¼ë°˜ì ì¸ ì°¨íŠ¸ ë°ì´í„° ìƒì„±"""
        # ë°ì´í„° íƒ€ì…ë³„ ê°œìˆ˜ ì§‘ê³„
        type_counts = {}
        for item in metadatas:
            data_type = item.get('type', 'ì•Œ ìˆ˜ ì—†ìŒ')
            type_counts[data_type] = type_counts.get(data_type, 0) + 1
        
        return {
            "labels": ["ì…ê³ ", "ì¶œê³ ", "ìƒí’ˆ"],
            "data": [
                type_counts.get('inbound', 0),
                type_counts.get('outbound', 0),
                type_counts.get('product', 0)
            ],
            "title": "ë°ì´í„° ìœ í˜•ë³„ ë¶„í¬",
            "type": "general_overview"
        }
    
    def _summarize_metadata(self, metadatas: List[Dict]) -> Dict[str, Any]:
        """ë©”íƒ€ë°ì´í„° ìš”ì•½"""
        summary = {
            "total_records": len(metadatas),
            "data_types": {},
            "date_range": {"min": None, "max": None},
            "quantity_stats": {"min": 0, "max": 0, "total": 0}
        }
        
        quantities = []
        dates = []
        
        for item in metadatas:
            # ë°ì´í„° íƒ€ì… ì§‘ê³„
            data_type = item.get('type', 'unknown')
            summary["data_types"][data_type] = summary["data_types"].get(data_type, 0) + 1
            
            # ìˆ˜ëŸ‰ í†µê³„
            quantity = item.get('quantity', 0)
            if quantity > 0:
                quantities.append(quantity)
            
            # ë‚ ì§œ ìˆ˜ì§‘
            date_str = item.get('date', '')
            if date_str and date_str != '':
                dates.append(date_str)
        
        # ìˆ˜ëŸ‰ í†µê³„ ê³„ì‚°
        if quantities:
            summary["quantity_stats"] = {
                "min": min(quantities),
                "max": max(quantities),
                "total": sum(quantities),
                "average": sum(quantities) / len(quantities)
            }
        
        # ë‚ ì§œ ë²”ìœ„ ê³„ì‚°
        if dates:
            summary["date_range"] = {
                "min": min(dates),
                "max": max(dates),
                "unique_dates": len(set(dates))
            }
        
        return summary
    
    def get_status(self) -> Dict[str, Any]:
        """ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ ë°˜í™˜"""
        if not self.is_initialized:
            return {
                "status": "not_initialized",
                "message": "ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            }
        
        try:
            count = self.collection.count()
            encoder_name = "ko-sroberta-multitask" if hasattr(self.encoder, 'model_name') else "unknown"
            
            return {
                "status": "ready",
                "document_count": count,
                "collection_name": self.collection.name,
                "encoder_model": encoder_name,
                "is_initialized": self.is_initialized,
                "collection_exists": self.collection is not None
            }
        except Exception as e:
            return {
                "status": "error",
                "error": str(e)
            }