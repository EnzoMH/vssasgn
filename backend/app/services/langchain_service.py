"""
LangChain Tools + SELF-RAG êµ¬í˜„
í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€ ë° ì •í™•í•œ ì •ë³´ ì œê³µì„ ìœ„í•œ ê³ ê¸‰ RAG ì‹œìŠ¤í…œ
"""

import datetime
import logging
import asyncio
import json
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass
from enum import Enum

# LangChain ê´€ë ¨ import (í•„ìš”ì‹œ ì„¤ì¹˜: pip install langchain langchain-community)
try:
    from langchain.tools import Tool
    from langchain.agents import initialize_agent, AgentType
    from langchain.schema import Document
    LANGCHAIN_AVAILABLE = True
except ImportError:
    # LangChainì´ ì—†ì–´ë„ ë™ì‘í•˜ë„ë¡ fallback
    LANGCHAIN_AVAILABLE = False
    Tool = object
    Document = dict

class RAGMode(Enum):
    """RAG ì²˜ë¦¬ ëª¨ë“œ"""
    SIMPLE = "simple"           # ê¸°ë³¸ RAG
    SELF_RAG = "self_rag"      # SELF-RAG (ìì²´ ê²€ì¦)
    TOOL_ENHANCED = "tool_enhanced"  # LangChain Tools í™œìš©

@dataclass
class RetrievalResult:
    """ê²€ìƒ‰ ê²°ê³¼ êµ¬ì¡°"""
    documents: List[Dict]
    scores: List[float]
    metadata: Dict[str, Any]
    total_found: int
    search_quality: float  # 0.0 ~ 1.0

@dataclass
class CritiqueResult:
    """ê²€ìƒ‰ ê²°ê³¼ ë¹„í‰/ê²€ì¦"""
    relevance_score: float      # ê´€ë ¨ì„± ì ìˆ˜ (0.0 ~ 1.0)
    confidence_score: float     # ì‹ ë¢°ë„ ì ìˆ˜ (0.0 ~ 1.0)
    missing_info: List[str]     # ë¶€ì¡±í•œ ì •ë³´
    hallucination_risk: float  # í• ë£¨ì‹œë„¤ì´ì…˜ ìœ„í—˜ë„ (0.0 ~ 1.0)
    needs_additional_search: bool

class LangChainRAGService:
    """LangChain Tools + SELF-RAG ì„œë¹„ìŠ¤"""
    
    def __init__(self, vector_db_service=None, ai_client=None, data_service=None, 
                 demand_predictor=None, product_clusterer=None, anomaly_detector=None):
        self.vector_db_service = vector_db_service
        self.ai_client = ai_client
        self.data_service = data_service
        self.logger = logging.getLogger(__name__)
        
        # ğŸ¤– ML ëª¨ë¸ë“¤ ì£¼ì…
        self.ml_models = {
            'demand_predictor': demand_predictor,
            'product_clusterer': product_clusterer,
            'anomaly_detector': anomaly_detector
        }
        self.logger.info(f"ğŸ¤– ML ëª¨ë¸ ì£¼ì… ì™„ë£Œ: {[k for k, v in self.ml_models.items() if v is not None]}")
        
        # í˜„ì¬ ë‚ ì§œ/ì‹œê°„ ìºì‹œ (í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€ìš©)
        self.current_datetime = datetime.datetime.now()
        self.system_context = self._build_system_context()
        
        # LangChain Tools ì´ˆê¸°í™”
        self.tools = self._create_langchain_tools()
        
        # SELF-RAG ì„¤ì •
        self.critique_threshold = 0.7  # ê²€ì¦ í†µê³¼ ì„ê³„ê°’
        self.max_retrieval_attempts = 3  # ìµœëŒ€ ì¬ê²€ìƒ‰ íšŸìˆ˜
        
        self.logger.info("ğŸ”§ LangChain SELF-RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _build_system_context(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€)"""
        return {
            "current_datetime": self.current_datetime.strftime("%Yë…„ %mì›” %dì¼ %Hì‹œ %Më¶„"),
            "current_date": self.current_datetime.strftime("%Yë…„ %mì›” %dì¼"),
            "current_time": self.current_datetime.strftime("%Hì‹œ %Më¶„"),
            "data_range": "2025ë…„ 1ì›” 1ì¼ ~ 2025ë…„ 1ì›” 7ì¼ (ê³¼ê±° ë°ì´í„°)",
            "system_status": "ë²¡í„° ê²€ìƒ‰ ê°€ëŠ¥ (2,900ê°œ ë¬¸ì„œ ì¸ë±ì‹± ì™„ë£Œ)",
            "warning": "ë°ì´í„°ëŠ” ê³¼ê±° ê¸°ë¡ì´ë©°, í˜„ì¬ ë‚ ì§œì™€ ë‹¤ë¦„ì„ ëª…ì‹œí•  ê²ƒ"
        }
    
    def _create_langchain_tools(self) -> List:
        """LangChain Tools ìƒì„±"""
        if not LANGCHAIN_AVAILABLE:
            self.logger.warning("âš ï¸ LangChainì´ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ. ê¸°ë³¸ ë„êµ¬ë§Œ ì‚¬ìš©")
            return []
        
        tools = [
            Tool(
                name="get_current_datetime",
                description="í˜„ì¬ ì •í™•í•œ ë‚ ì§œì™€ ì‹œê°„ì„ ê°€ì ¸ì˜µë‹ˆë‹¤. í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€ìš©.",
                func=self._get_current_datetime
            ),
            Tool(
                name="get_system_context", 
                description="ì‹œìŠ¤í…œ ìƒíƒœì™€ ë°ì´í„° ë²”ìœ„ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.",
                func=self._get_system_context
            ),
            Tool(
                name="search_vector_database",
                description="ì°½ê³  ë°ì´í„°ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤. ìƒí’ˆ, ì…ì¶œê³ , ë™ ì •ë³´ ë“±.",
                func=self._search_vector_database
            ),
            Tool(
                name="calculate_warehouse_statistics",
                description="ì°½ê³  í†µê³„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. ì´ ì¬ê³ ëŸ‰, ì…ì¶œê³ ëŸ‰ ë“±. (í†µí•© ê³„ì‚° ê¸°ë°˜)",
                func=self._calculate_warehouse_statistics
            ),
            Tool(
                name="validate_information",
                description="ì œê³µëœ ì •ë³´ì˜ ì •í™•ì„±ì„ ê²€ì¦í•©ë‹ˆë‹¤.",
                func=self._validate_information
            ),
            # ğŸ†• ìƒˆë¡œìš´ Toolë“¤ ì¶”ê°€
            Tool(
                name="get_rack_specific_info",
                description="íŠ¹ì • ë™(A~Z)ì˜ ìƒì„¸ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤. ì¬ê³ ëŸ‰, í™œìš©ë¥ , ì €ì¥ ìƒí’ˆ ë“±.",
                func=self._get_rack_specific_info
            ),
            Tool(
                name="analyze_inventory_trends",
                description="ì¬ê³  íŠ¸ë Œë“œë¥¼ ë¶„ì„í•©ë‹ˆë‹¤. ì…ì¶œê³  íŒ¨í„´, ì¼ë³„ ë³€í™” ë“±.",
                func=self._analyze_inventory_trends
            ),
            Tool(
                name="get_low_stock_alerts",
                description="ì¬ê³  ë¶€ì¡± ìƒí’ˆê³¼ ìœ„í—˜ ìƒí’ˆì„ ë¶„ì„í•©ë‹ˆë‹¤.",
                func=self._get_low_stock_alerts
            ),
            Tool(
                name="calculate_rack_utilization",
                description="ë™ë³„ í™œìš©ë¥ ê³¼ ì „ì²´ ì°½ê³  íš¨ìœ¨ì„±ì„ ê³„ì‚°í•©ë‹ˆë‹¤.",
                func=self._calculate_rack_utilization
            ),
            Tool(
                name="get_date_specific_data",
                description="íŠ¹ì • ë‚ ì§œì˜ ì…ì¶œê³  ë°ì´í„°ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤. (2025-01-01 ~ 2025-01-07)",
                func=self._get_date_specific_data
            ),
            # ğŸ¤– ML Tools ì¶”ê°€
            Tool(
                name="ml_demand_prediction",
                description="ìˆ˜ìš” ì˜ˆì¸¡ ë° íŠ¸ë Œë“œ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ë¯¸ë˜ ì¶œê³ ëŸ‰, íŒë§¤ ì „ë§ ë“±.",
                func=self._tool_ml_prediction
            ),
            Tool(
                name="ml_anomaly_detection",
                description="ì´ìƒ íŒ¨í„´ ë° ìœ„í—˜ ìš”ì†Œë¥¼ ê°ì§€í•©ë‹ˆë‹¤. ë¹„ì •ìƒì ì¸ ì…ì¶œê³  íŒ¨í„´ ë“±.",
                func=self._tool_ml_anomaly
            ),
            Tool(
                name="ml_product_clustering",
                description="ìƒí’ˆ ë¶„ë¥˜ ë° í´ëŸ¬ìŠ¤í„° ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ìœ ì‚¬í•œ ìƒí’ˆ ê·¸ë£¹í™” ë“±.",
                func=self._tool_ml_clustering
            )
        ]
        
        self.logger.info(f"âœ… {len(tools)}ê°œ LangChain Tools ìƒì„± ì™„ë£Œ")
        return tools
    
    def _get_current_datetime(self, query: str = "") -> str:
        """í˜„ì¬ ì •í™•í•œ ë‚ ì§œ/ì‹œê°„ ë°˜í™˜"""
        current = datetime.datetime.now()
        return f"""í˜„ì¬ ë‚ ì§œì™€ ì‹œê°„: {current.strftime('%Yë…„ %mì›” %dì¼ %Hì‹œ %Më¶„')}

âš ï¸ ì¤‘ìš” ì•Œë¦¼: 
- í˜„ì¬ ë‚ ì§œ: {current.strftime('%Yë…„ %mì›” %dì¼')}
- ì°½ê³  ë°ì´í„° ë²”ìœ„: 2025ë…„ 1ì›” 1ì¼ ~ 2025ë…„ 1ì›” 7ì¼ (ê³¼ê±° ë°ì´í„°)
- ë°ì´í„°ëŠ” ê³¼ê±° ê¸°ë¡ì´ë¯€ë¡œ "í˜„ì¬"ê°€ ì•„ë‹˜ì„ ëª…ì‹œí•  ê²ƒ"""
    
    def _get_system_context(self, query: str = "") -> str:
        """ì‹œìŠ¤í…œ ì»¨í…ìŠ¤íŠ¸ ë°˜í™˜"""
        context = self.system_context.copy()
        
        # ì‹¤ì‹œê°„ ì‹œìŠ¤í…œ ìƒíƒœ ì—…ë°ì´íŠ¸
        if self.vector_db_service and hasattr(self.vector_db_service, 'is_initialized'):
            if self.vector_db_service.is_initialized:
                context["vector_db_status"] = "âœ… ì •ìƒ ì‘ë™"
            else:
                context["vector_db_status"] = "âš ï¸ ì—°ê²° ë¶ˆê°€"
        
        return json.dumps(context, ensure_ascii=False, indent=2)
    
    def _search_vector_database(self, query: str) -> str:
        """ğŸ” ê°•í™”ëœ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ - AI ì²˜ë¦¬ìš© êµ¬ì¡°í™”ëœ ê²°ê³¼"""
        if not self.vector_db_service:
            return "âŒ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ"
        
        try:
            # ë™ê¸° í˜¸ì¶œì„ ìœ„í•œ ë˜í¼
            import asyncio
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            try:
                result = loop.run_until_complete(
                    self.vector_db_service.search_relevant_data(query, n_results=20)  # ë” ë§ì€ ê²°ê³¼
                )
            finally:
                loop.close()
            
            if result.get("success"):
                # êµ¬ì¡°í™”ëœ ê²€ìƒ‰ ê²°ê³¼ ìƒì„±
                enhanced_result = self._format_vector_search_result(result, query)
                return enhanced_result
            else:
                return f"âŒ ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}"
                
        except Exception as e:
            return f"âŒ ë²¡í„° ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}"
    
    def _format_vector_search_result(self, result: Dict[str, Any], query: str) -> str:
        """ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ë¥¼ AI ì²˜ë¦¬ìš©ìœ¼ë¡œ êµ¬ì¡°í™”"""
        docs = result.get("documents", [])
        found = result.get("found_documents", 0)
        metadata = result.get("metadata_summary", {})
        chart_data = result.get("chart_data", {})
        
        formatted_result = []
        
        # ğŸ” ê²€ìƒ‰ ìš”ì•½
        formatted_result.append(f"ğŸ” **ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ (ì§ˆì˜: {query})**")
        formatted_result.append(f"- ì´ ê²€ìƒ‰ëœ ë¬¸ì„œ: {found}ê°œ (2,900ê°œ ì¤‘)")
        formatted_result.append(f"- ê´€ë ¨ë„: {'ë†’ìŒ' if found >= 10 else 'ë³´í†µ' if found >= 5 else 'ë‚®ìŒ'}")
        formatted_result.append("")
        
        # ğŸ“Š í•µì‹¬ ë¬¸ì„œ ë‚´ìš© (ìƒìœ„ 5ê°œ)
        if docs:
            formatted_result.append("ğŸ“Š **í•µì‹¬ ê´€ë ¨ ë¬¸ì„œ:**")
            for i, doc in enumerate(docs[:5], 1):
                # ë¬¸ì„œ ë‚´ìš©ì„ AIê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ ì •ë¦¬
                cleaned_doc = doc.strip()
                if len(cleaned_doc) > 300:
                    cleaned_doc = cleaned_doc[:300] + "..."
                formatted_result.append(f"{i}. {cleaned_doc}")
            formatted_result.append("")
        
        # ğŸ“ˆ ë©”íƒ€ë°ì´í„° ì •ë³´
        if metadata:
            formatted_result.append("ğŸ“ˆ **ë°ì´í„° ë©”íƒ€ì •ë³´:**")
            for key, value in metadata.items():
                if key in ['total_records', 'date_range', 'data_types', 'quantity_stats']:
                    formatted_result.append(f"- {key}: {value}")
            formatted_result.append("")
        
        # ğŸ“Š ì°¨íŠ¸ ë°ì´í„° (ì§‘ê³„ ê²°ê³¼)
        if chart_data:
            formatted_result.append("ğŸ“Š **ì§‘ê³„ ë°ì´í„°:**")
            if 'title' in chart_data:
                formatted_result.append(f"- ì œëª©: {chart_data['title']}")
            if 'data' in chart_data and 'labels' in chart_data:
                formatted_result.append(f"- ë°ì´í„°: {dict(zip(chart_data.get('labels', []), chart_data.get('data', [])))}")
            formatted_result.append("")
        
        # ğŸ¯ AI ì²˜ë¦¬ ê°€ì´ë“œ
        formatted_result.append("ğŸ¯ **AI ì²˜ë¦¬ ì§€ì¹¨:**")
        formatted_result.append("- ìœ„ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ êµ¬ì²´ì ì´ê³  ìƒì„¸í•œ ë‹µë³€ ìƒì„±")
        formatted_result.append("- íšŒì‚¬ëª…, ìƒí’ˆëª…, ìˆ˜ëŸ‰ ë“± êµ¬ì²´ì  ì •ë³´ í™œìš©")
        formatted_result.append("- ë°ì´í„°ëŠ” 2025ë…„ 1ì›” 1ì¼~7ì¼ ê³¼ê±° ê¸°ë¡ì„ì„ ëª…ì‹œ")
        formatted_result.append("- í˜„ì¬ ë‚ ì§œì™€ êµ¬ë¶„í•˜ì—¬ ë‹µë³€")
        
        return "\n".join(formatted_result)
    
    def _calculate_warehouse_statistics(self, query: str) -> str:
        """ì°½ê³  í†µê³„ ê³„ì‚° - í†µí•© ê³„ì‚° ê¸°ë°˜ìœ¼ë¡œ ê°œì„ """
        if not self.data_service:
            return "âŒ ë°ì´í„° ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ"
        
        try:
            # ğŸ”„ í†µí•© ê³„ì‚° ë©”ì„œë“œ ì‚¬ìš©
            if hasattr(self.data_service, 'get_unified_inventory_stats'):
                unified_stats = self.data_service.get_unified_inventory_stats()
                
                if "error" not in unified_stats:
                    return f"""ğŸ“Š ì°½ê³  í†µê³„ (í†µí•© ê³„ì‚° ê¸°ë°˜, 2025ë…„ 1ì›” 1ì¼-7ì¼):
- ì´ ì¬ê³ ëŸ‰: {unified_stats.get('total_inventory', 0):,}ê°œ
- ìƒí’ˆ ì¢…ë¥˜: {unified_stats.get('total_products', 0)}ê°œ
- ì´ ì…ê³ ëŸ‰: {unified_stats.get('total_inbound_qty', 0):,}ê°œ
- ì´ ì¶œê³ ëŸ‰: {unified_stats.get('total_outbound_qty', 0):,}ê°œ
- ì¼í‰ê·  ì…ê³ : {unified_stats.get('daily_inbound_avg', 0):,}ê°œ
- ì¼í‰ê·  ì¶œê³ : {unified_stats.get('daily_outbound_avg', 0):,}ê°œ
- í™œì„± ë™ ìˆ˜: {len(unified_stats.get('rack_distribution', {})):,}ê°œ
- ê³„ì‚° ë°©ì‹: {unified_stats.get('calculation_method', 'unknown')}

âš ï¸ ì£¼ì˜: ì´ ë°ì´í„°ëŠ” ê³¼ê±° 7ì¼ê°„ì˜ ê¸°ë¡ì´ë©°, ëª¨ë“  ì‹œìŠ¤í…œì—ì„œ ë™ì¼í•œ ìˆ˜ì¹˜ë¥¼ ì œê³µí•©ë‹ˆë‹¤."""
            
            # Fallback: ê¸°ì¡´ ë°©ì‹
            stats = {}
            
            # ì´ ì¬ê³ ëŸ‰
            if hasattr(self.data_service, 'product_master') and self.data_service.product_master is not None:
                if 'í˜„ì¬ê³ ' in self.data_service.product_master.columns:
                    stats['total_inventory'] = int(self.data_service.product_master['í˜„ì¬ê³ '].sum())
                    stats['product_count'] = len(self.data_service.product_master)
            
            # ì…ê³ ëŸ‰
            if hasattr(self.data_service, 'inbound_data') and self.data_service.inbound_data is not None:
                if 'PalleteQty' in self.data_service.inbound_data.columns:
                    stats['total_inbound'] = int(self.data_service.inbound_data['PalleteQty'].sum())
                    stats['inbound_records'] = len(self.data_service.inbound_data)
            
            # ì¶œê³ ëŸ‰
            if hasattr(self.data_service, 'outbound_data') and self.data_service.outbound_data is not None:
                if 'PalleteQty' in self.data_service.outbound_data.columns:
                    stats['total_outbound'] = int(self.data_service.outbound_data['PalleteQty'].sum())
                    stats['outbound_records'] = len(self.data_service.outbound_data)
            
            return f"""ğŸ“Š ì°½ê³  í†µê³„ (ë ˆê±°ì‹œ ê³„ì‚°, 2025ë…„ 1ì›” 1ì¼-7ì¼):
- ì´ ì¬ê³ ëŸ‰: {stats.get('total_inventory', 0):,}ê°œ
- ìƒí’ˆ ì¢…ë¥˜: {stats.get('product_count', 0)}ê°œ
- ì´ ì…ê³ ëŸ‰: {stats.get('total_inbound', 0):,}ê°œ ({stats.get('inbound_records', 0)}ê±´)
- ì´ ì¶œê³ ëŸ‰: {stats.get('total_outbound', 0):,}ê°œ ({stats.get('outbound_records', 0)}ê±´)

âš ï¸ ì£¼ì˜: ì´ ë°ì´í„°ëŠ” ê³¼ê±° 7ì¼ê°„ì˜ ê¸°ë¡ì…ë‹ˆë‹¤. (ë ˆê±°ì‹œ ê³„ì‚° ë°©ì‹ ì‚¬ìš©)"""
            
        except Exception as e:
            return f"âŒ í†µê³„ ê³„ì‚° ì˜¤ë¥˜: {str(e)}"
    
    def _get_rack_specific_info(self, query: str) -> str:
        """ğŸ¢ íŠ¹ì • ë™ì˜ ìƒì„¸ ì •ë³´ ì¡°íšŒ"""
        if not self.data_service or not hasattr(self.data_service, 'get_unified_inventory_stats'):
            return "âŒ ë°ì´í„° ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ"
        
        try:
            # ì¿¼ë¦¬ì—ì„œ ë™ ì´ë¦„ ì¶”ì¶œ
            import re
            rack_match = re.search(r'([A-Za-z])ë™?', query)
            if not rack_match:
                rack_match = re.search(r'rack[_\s]*([A-Za-z])', query, re.IGNORECASE)
            
            if not rack_match:
                return "âŒ ë™ ì´ë¦„ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì˜ˆ: Aë™, Bë™, Cë™"
            
            rack_letter = rack_match.group(1).upper()
            
            # í†µí•© ê³„ì‚°ì—ì„œ ë™ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            unified_stats = self.data_service.get_unified_inventory_stats()
            
            if "error" in unified_stats:
                return f"âŒ í†µí•© ê³„ì‚° ì‹¤íŒ¨: {unified_stats['error']}"
            
            rack_distribution = unified_stats.get("rack_distribution", {})
            
            # ë™ ì´ë¦„ ë§¤ì¹­
            target_rack = None
            for rack_name in rack_distribution.keys():
                if rack_letter in rack_name.upper():
                    target_rack = rack_name
                    break
            
            if not target_rack:
                return f"âŒ {rack_letter}ë™ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\nì‚¬ìš© ê°€ëŠ¥í•œ ë™: {list(rack_distribution.keys())}"
            
            current_stock = rack_distribution[target_rack]
            avg_capacity = 50  # ë™ë‹¹ í‰ê·  ìš©ëŸ‰
            utilization = (current_stock / avg_capacity) * 100
            
            status = "âœ… ì •ìƒ" if utilization < 80 else "âš ï¸ ì£¼ì˜" if utilization < 95 else "ğŸš¨ í¬í™”"
            
            return f"""ğŸ¢ {target_rack} ìƒì„¸ ì •ë³´:
- í˜„ì¬ ì¬ê³ ëŸ‰: {int(current_stock):,}ê°œ
- ìµœëŒ€ ìš©ëŸ‰: {avg_capacity}ê°œ
- í™œìš©ë¥ : {utilization:.1f}%
- ìƒíƒœ: {status}
- ë°ì´í„° ì†ŒìŠ¤: í†µí•© ê³„ì‚° (ì¼ê´€ì„± ë³´ì¥)

âš ï¸ ì£¼ì˜: 2025ë…„ 1ì›” 1ì¼-7ì¼ ë°ì´í„° ê¸°ì¤€"""
            
        except Exception as e:
            return f"âŒ ë™ ì •ë³´ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}"
    
    def _analyze_inventory_trends(self, query: str) -> str:
        """ğŸ“ˆ ì¬ê³  íŠ¸ë Œë“œ ë¶„ì„"""
        if not self.data_service:
            return "âŒ ë°ì´í„° ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ"
        
        try:
            # í†µí•© ê³„ì‚° ë°ì´í„° í™œìš©
            unified_stats = self.data_service.get_unified_inventory_stats()
            
            if "error" in unified_stats:
                return f"âŒ í†µí•© ê³„ì‚° ì‹¤íŒ¨: {unified_stats['error']}"
            
            # ì¼ë³„ íŠ¸ë Œë“œ ë°ì´í„° ì‹œë„
            trend_data = None
            if hasattr(self.data_service, 'get_daily_trends_summary'):
                try:
                    trend_data = self.data_service.get_daily_trends_summary()
                except Exception as e:
                    self.logger.warning(f"ì¼ë³„ íŠ¸ë Œë“œ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            
            total_inbound = unified_stats.get('total_inbound_qty', 0)
            total_outbound = unified_stats.get('total_outbound_qty', 0)
            daily_inbound = unified_stats.get('daily_inbound_avg', 0)
            daily_outbound = unified_stats.get('daily_outbound_avg', 0)
            
            # íŠ¸ë Œë“œ ë¶„ì„
            net_flow = total_inbound - total_outbound
            daily_net = daily_inbound - daily_outbound
            
            trend_direction = "ğŸ“ˆ ì¦ê°€" if daily_net > 0 else "ğŸ“‰ ê°ì†Œ" if daily_net < 0 else "â¡ï¸ ê· í˜•"
            
            return f"""ğŸ“ˆ ì¬ê³  íŠ¸ë Œë“œ ë¶„ì„ (2025ë…„ 1ì›” 1ì¼-7ì¼):

ğŸ“Š **ì „ì²´ íë¦„:**
- ì´ ì…ê³ ëŸ‰: {total_inbound:,}ê°œ
- ì´ ì¶œê³ ëŸ‰: {total_outbound:,}ê°œ
- ìˆœ ì¦ê°: {net_flow:,}ê°œ

ğŸ“… **ì¼ë³„ í‰ê· :**
- ì¼í‰ê·  ì…ê³ : {daily_inbound:,}ê°œ
- ì¼í‰ê·  ì¶œê³ : {daily_outbound:,}ê°œ
- ì¼í‰ê·  ìˆœì¦ê°: {daily_net:,}ê°œ

ğŸ¯ **íŠ¸ë Œë“œ ë°©í–¥:** {trend_direction}

ğŸ’¡ **í•´ì„:**
- {"ì¬ê³ ê°€ ì§€ì†ì ìœ¼ë¡œ ì¦ê°€í•˜ëŠ” ì¶”ì„¸" if daily_net > 0 else "ì¬ê³ ê°€ ì§€ì†ì ìœ¼ë¡œ ê°ì†Œí•˜ëŠ” ì¶”ì„¸" if daily_net < 0 else "ì…ì¶œê³ ê°€ ê· í˜•ì„ ì´ë£¨ê³  ìˆìŒ"}

âš ï¸ ì£¼ì˜: ê³¼ê±° 7ì¼ê°„ì˜ ë°ì´í„° ê¸°ë°˜ ë¶„ì„"""
            
        except Exception as e:
            return f"âŒ íŠ¸ë Œë“œ ë¶„ì„ ì˜¤ë¥˜: {str(e)}"
    
    def _get_low_stock_alerts(self, query: str) -> str:
        """âš ï¸ ì¬ê³  ë¶€ì¡± ê²½ê³  ë¶„ì„"""
        if not self.data_service:
            return "âŒ ë°ì´í„° ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ"
        
        try:
            # ì œí’ˆë³„ ì¬ê³  ë°ì´í„° ë¶„ì„
            if not hasattr(self.data_service, 'product_master') or self.data_service.product_master is None:
                return "âŒ ìƒí’ˆ ë§ˆìŠ¤í„° ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤"
            
            df = self.data_service.product_master
            stock_column = 'í˜„ì¬ê³ ' if 'í˜„ì¬ê³ ' in df.columns else 'Start Pallete Qty'
            
            if stock_column not in df.columns:
                return "âŒ ì¬ê³  ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
            
            # ì„ê³„ê°’ ì„¤ì •
            critical_threshold = 10  # ê¸´ê¸‰ ì¬ê³  ë¶€ì¡±
            warning_threshold = 20   # ì£¼ì˜ í•„ìš”
            
            # ë¶„ë¥˜
            critical_products = df[df[stock_column] <= critical_threshold]
            warning_products = df[(df[stock_column] > critical_threshold) & (df[stock_column] <= warning_threshold)]
            
            result = f"""âš ï¸ ì¬ê³  ë¶€ì¡± ê²½ê³  ë¶„ì„:

ğŸš¨ **ê¸´ê¸‰ ì¬ê³  ë¶€ì¡±** ({critical_threshold}ê°œ ì´í•˜):
- ëŒ€ìƒ ìƒí’ˆ: {len(critical_products)}ê°œ"""
            
            if len(critical_products) > 0:
                for _, product in critical_products.head(5).iterrows():
                    product_name = product.get('ProductName', 'ì´ë¦„ ì—†ìŒ')
                    stock = product.get(stock_column, 0)
                    result += f"\n  â€¢ {product_name}: {int(stock)}ê°œ"
                
                if len(critical_products) > 5:
                    result += f"\n  â€¢ ì™¸ {len(critical_products) - 5}ê°œ ìƒí’ˆ"
            
            result += f"""

âš ï¸ **ì£¼ì˜ ì¬ê³ ** ({warning_threshold}ê°œ ì´í•˜):
- ëŒ€ìƒ ìƒí’ˆ: {len(warning_products)}ê°œ"""
            
            if len(warning_products) > 0:
                for _, product in warning_products.head(3).iterrows():
                    product_name = product.get('ProductName', 'ì´ë¦„ ì—†ìŒ')
                    stock = product.get(stock_column, 0)
                    result += f"\n  â€¢ {product_name}: {int(stock)}ê°œ"
                
                if len(warning_products) > 3:
                    result += f"\n  â€¢ ì™¸ {len(warning_products) - 3}ê°œ ìƒí’ˆ"
            
            # ì „ì²´ ìš”ì•½
            total_products = len(df)
            at_risk_products = len(critical_products) + len(warning_products)
            risk_percentage = (at_risk_products / total_products) * 100 if total_products > 0 else 0
            
            result += f"""

ğŸ“Š **ì „ì²´ ìš”ì•½:**
- ì „ì²´ ìƒí’ˆ ìˆ˜: {total_products}ê°œ
- ìœ„í—˜ ìƒí’ˆ ìˆ˜: {at_risk_products}ê°œ ({risk_percentage:.1f}%)
- ì•ˆì „ ìƒí’ˆ ìˆ˜: {total_products - at_risk_products}ê°œ

ğŸ’¡ **ê¶Œì¥ì‚¬í•­:**
- ê¸´ê¸‰ ì¬ê³  ë¶€ì¡± ìƒí’ˆì€ ì¦‰ì‹œ ë°œì£¼ í•„ìš”
- ì£¼ì˜ ì¬ê³  ìƒí’ˆì€ 1-2ì¼ ë‚´ ë°œì£¼ ê²€í† 

âš ï¸ ì£¼ì˜: 2025ë…„ 1ì›” ë°ì´í„° ê¸°ì¤€"""
            
            return result
            
        except Exception as e:
            return f"âŒ ì¬ê³  ë¶€ì¡± ë¶„ì„ ì˜¤ë¥˜: {str(e)}"
    
    def _calculate_rack_utilization(self, query: str) -> str:
        """ğŸ—ï¸ ë™ í™œìš©ë¥  ê³„ì‚°"""
        if not self.data_service:
            return "âŒ ë°ì´í„° ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ"
        
        try:
            # í†µí•© ê³„ì‚°ì—ì„œ ë™ í™œìš©ë¥  ê°€ì ¸ì˜¤ê¸°
            if hasattr(self.data_service, 'calculate_rack_utilization'):
                rack_util = self.data_service.calculate_rack_utilization()
                
                if not rack_util:
                    return "âŒ ë™ í™œìš©ë¥  ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
                
                result = "ğŸ—ï¸ ë™ í™œìš©ë¥  ë¶„ì„:\n\n"
                
                # í™œìš©ë¥  ìˆœìœ¼ë¡œ ì •ë ¬
                sorted_racks = sorted(rack_util.items(), key=lambda x: x[1].get('utilization_rate', 0), reverse=True)
                
                high_util_count = 0
                normal_util_count = 0
                low_util_count = 0
                
                for rack_name, rack_info in sorted_racks:
                    current_stock = rack_info.get('current_stock', 0)
                    max_capacity = rack_info.get('max_capacity', 50)
                    utilization_rate = rack_info.get('utilization_rate', 0)
                    
                    if utilization_rate >= 95:
                        status_icon = "ğŸš¨"
                        high_util_count += 1
                    elif utilization_rate >= 80:
                        status_icon = "âš ï¸"
                        normal_util_count += 1
                    else:
                        status_icon = "âœ…"
                        low_util_count += 1
                    
                    result += f"{status_icon} **{rack_name}**: {current_stock}ê°œ/{max_capacity}ê°œ ({utilization_rate:.1f}%)\n"
                
                # ì „ì²´ ìš”ì•½
                total_racks = len(rack_util)
                avg_utilization = sum(r.get('utilization_rate', 0) for r in rack_util.values()) / total_racks if total_racks > 0 else 0
                
                result += f"""
ğŸ“Š **ì „ì²´ ìš”ì•½:**
- ì´ ë™ ìˆ˜: {total_racks}ê°œ
- í‰ê·  í™œìš©ë¥ : {avg_utilization:.1f}%
- í¬í™” ìƒíƒœ (95%+): {high_util_count}ê°œ
- ì£¼ì˜ ìƒíƒœ (80-95%): {normal_util_count}ê°œ  
- ì—¬ìœ  ìƒíƒœ (80% ë¯¸ë§Œ): {low_util_count}ê°œ

ğŸ’¡ **íš¨ìœ¨ì„± ë¶„ì„:**
- {"ì°½ê³  ê³µê°„ì´ íš¨ìœ¨ì ìœ¼ë¡œ í™œìš©ë˜ê³  ìˆìŒ" if avg_utilization > 70 else "ì°½ê³  ê³µê°„ í™œìš©ë„ê°€ ë‚®ìŒ - ì¬ë°°ì¹˜ ê²€í†  í•„ìš”" if avg_utilization < 50 else "ë³´í†µ ìˆ˜ì¤€ì˜ í™œìš©ë¥ "}

âš ï¸ ì£¼ì˜: í†µí•© ê³„ì‚° ê¸°ë°˜ ë°ì´í„°"""
                
                return result
            else:
                return "âŒ ë™ í™œìš©ë¥  ê³„ì‚° ê¸°ëŠ¥ì´ ì—†ìŠµë‹ˆë‹¤"
                
        except Exception as e:
            return f"âŒ ë™ í™œìš©ë¥  ê³„ì‚° ì˜¤ë¥˜: {str(e)}"
    
    def _get_date_specific_data(self, query: str) -> str:
        """ğŸ“… íŠ¹ì • ë‚ ì§œ ë°ì´í„° ë¶„ì„"""
        if not self.data_service:
            return "âŒ ë°ì´í„° ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ"
        
        try:
            # ì¿¼ë¦¬ì—ì„œ ë‚ ì§œ ì¶”ì¶œ
            import re
            date_patterns = [
                r'2025[.-]01[.-](\d{1,2})',
                r'1ì›”\s*(\d{1,2})ì¼?',
                r'(\d{1,2})ì¼',
                r'01[.-](\d{1,2})'
            ]
            
            target_day = None
            for pattern in date_patterns:
                match = re.search(pattern, query)
                if match:
                    target_day = int(match.group(1))
                    break
            
            if not target_day or target_day < 1 or target_day > 7:
                return """âŒ ìœ íš¨í•œ ë‚ ì§œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

ì‚¬ìš© ê°€ëŠ¥í•œ ë‚ ì§œ: 2025ë…„ 1ì›” 1ì¼ ~ 7ì¼
ì˜ˆì‹œ: "1ì›” 3ì¼", "2025-01-05", "6ì¼" ë“±"""
            
            target_date = f"2025-01-{target_day:02d}"
            display_date = f"2025ë…„ 1ì›” {target_day}ì¼"
            
            result = f"ğŸ“… {display_date} ë°ì´í„° ë¶„ì„:\n\n"
            
            # ì…ê³  ë°ì´í„° ë¶„ì„
            inbound_count = 0
            inbound_qty = 0
            if hasattr(self.data_service, 'inbound_data') and self.data_service.inbound_data is not None:
                df = self.data_service.inbound_data
                if 'Date' in df.columns:
                    # ë‚ ì§œ í•„í„°ë§
                    date_matches = df['Date'].astype(str).str.contains(f"2025.01.{target_day:02d}|2025-01-{target_day:02d}|01/{target_day:02d}/2025", na=False)
                    day_data = df[date_matches]
                    inbound_count = len(day_data)
                    if 'PalleteQty' in day_data.columns:
                        inbound_qty = day_data['PalleteQty'].sum()
            
            # ì¶œê³  ë°ì´í„° ë¶„ì„
            outbound_count = 0
            outbound_qty = 0
            if hasattr(self.data_service, 'outbound_data') and self.data_service.outbound_data is not None:
                df = self.data_service.outbound_data
                if 'Date' in df.columns:
                    date_matches = df['Date'].astype(str).str.contains(f"2025.01.{target_day:02d}|2025-01-{target_day:02d}|01/{target_day:02d}/2025", na=False)
                    day_data = df[date_matches]
                    outbound_count = len(day_data)
                    if 'PalleteQty' in day_data.columns:
                        outbound_qty = day_data['PalleteQty'].sum()
            
            net_qty = inbound_qty - outbound_qty
            net_direction = "ğŸ“ˆ ì¦ê°€" if net_qty > 0 else "ğŸ“‰ ê°ì†Œ" if net_qty < 0 else "â¡ï¸ ê· í˜•"
            
            result += f"""ğŸ“¦ **ì…ê³  í˜„í™©:**
- ì…ê³  ê±´ìˆ˜: {inbound_count}ê±´
- ì…ê³  ìˆ˜ëŸ‰: {int(inbound_qty):,}ê°œ

ğŸšš **ì¶œê³  í˜„í™©:**
- ì¶œê³  ê±´ìˆ˜: {outbound_count}ê±´
- ì¶œê³  ìˆ˜ëŸ‰: {int(outbound_qty):,}ê°œ

ğŸ“Š **ì¼ì¼ ìš”ì•½:**
- ìˆœ ì¦ê°: {int(net_qty):,}ê°œ
- íŠ¸ë Œë“œ: {net_direction}
- ì´ ê±°ë˜: {inbound_count + outbound_count}ê±´

ğŸ’¡ **ë¶„ì„:**
- {"í™œë°œí•œ ì…ì¶œê³  í™œë™" if (inbound_count + outbound_count) > 10 else "ë³´í†µ ìˆ˜ì¤€ì˜ í™œë™" if (inbound_count + outbound_count) > 5 else "ë‚®ì€ ìˆ˜ì¤€ì˜ í™œë™"}
- {"ì¬ê³  ì¦ê°€ì¼" if net_qty > 0 else "ì¬ê³  ê°ì†Œì¼" if net_qty < 0 else "ì…ì¶œê³  ê· í˜•ì¼"}

âš ï¸ ì£¼ì˜: {display_date} ê³¼ê±° ë°ì´í„° ê¸°ì¤€"""
            
            return result
            
        except Exception as e:
            return f"âŒ ë‚ ì§œë³„ ë°ì´í„° ë¶„ì„ ì˜¤ë¥˜: {str(e)}"
    
    def _validate_information(self, information: str) -> str:
        """ì •ë³´ ê²€ì¦"""
        validation_issues = []
        
        # ë‚ ì§œ ê´€ë ¨ í• ë£¨ì‹œë„¤ì´ì…˜ ì²´í¬
        if "2025ë…„ 1ì›”" in information and "í˜„ì¬" in information:
            validation_issues.append("âš ï¸ 2025ë…„ 1ì›” ë°ì´í„°ë¥¼ 'í˜„ì¬'ë¼ê³  í‘œí˜„í•˜ë©´ ì•ˆë¨")
        
        # í™•ì •ì  í‘œí˜„ ì²´í¬
        uncertain_patterns = ["í™•ì‹¤íˆ", "ë¶„ëª…íˆ", "í‹€ë¦¼ì—†ì´", "ë°˜ë“œì‹œ"]
        for pattern in uncertain_patterns:
            if pattern in information:
                validation_issues.append(f"âš ï¸ ê³¼ë„í•œ í™•ì‹  í‘œí˜„ ë°œê²¬: '{pattern}'")
        
        # ìˆ«ì ì •í™•ì„± ì²´í¬ (ê¸°ë³¸ì ì¸ ë²”ìœ„ ê²€ì¦)
        import re
        numbers = re.findall(r'(\d{1,3}(?:,\d{3})*)', information)
        large_numbers = [n for n in numbers if int(n.replace(',', '')) > 100000]
        if large_numbers:
            validation_issues.append(f"âš ï¸ í° ìˆ«ì ê²€ì¦ í•„ìš”: {large_numbers}")
        
        if validation_issues:
            return f"ğŸ” ê²€ì¦ ê²°ê³¼:\n" + "\n".join(validation_issues)
        else:
            return "âœ… ê²€ì¦ í†µê³¼: ë¬¸ì œ ì—†ìŒ"
    
    async def _classify_ml_intent(self, question: str) -> Dict[str, Any]:
        """ğŸ¤– AI ê¸°ë°˜ ML ì˜ë„ ë¶„ë¥˜"""
        try:
            classification_prompt = f"""ì§ˆë¬¸ì„ 4ê°€ì§€ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜:

1. PREDICTION: ì˜ˆì¸¡, ìˆ˜ìš”, ë¯¸ë˜, ì „ë§
2. ANOMALY: ì´ìƒ, ë¬¸ì œ, ë¹„ì •ìƒ
3. CLUSTERING: ë¶„ë¥˜, ê·¸ë£¹, ì¹´í…Œê³ ë¦¬
4. DATA: í˜„ì¬ ì¡°íšŒ, ëª©ë¡

ì§ˆë¬¸: {question}

JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€:
{{"category": "ë¶„ë¥˜ê²°ê³¼", "confidence": 0.8}}"""
            
            # AI ì„œë¹„ìŠ¤ í˜¸ì¶œ
            self.logger.info(f"ğŸ¤– [ML_CLASSIFICATION] AIì—ê²Œ ë¶„ë¥˜ ìš”ì²­: '{question[:30]}...'")
            response = await self.ai_client.answer_simple_query(
                classification_prompt, 
                {"classification_task": True}
            )
            self.logger.info(f"ğŸ¤– [ML_CLASSIFICATION] AI ì‘ë‹µ ì›ë¬¸: '{response[:100]}...'")  
            
            # JSON íŒŒì‹±
            import json
            json_start = response.find('{')
            json_end = response.rfind('}') + 1
            
            if json_start >= 0 and json_end > json_start:
                json_str = response[json_start:json_end]
                result = json.loads(json_str)
                
                # ì‹ ë¢°ë„ ê²€ì¦
                confidence = result.get('confidence', 0.5)
                category = result.get('category', 'DATA')
                
                self.logger.info(f"ğŸ¯ [ML_CLASSIFICATION] {question[:30]}... â†’ {category} (ì‹ ë¢°ë„: {confidence})")
                
                return {
                    "category": category,
                    "confidence": confidence,
                    "reasoning": result.get("reasoning", "AI ë¶„ë¥˜"),
                    "needs_ml": category in ["PREDICTION", "ANOMALY", "CLUSTERING"]
                }
            else:
                raise ValueError("JSON íŒŒì‹± ì‹¤íŒ¨")
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ [ML_CLASSIFICATION] AI ë¶„ë¥˜ ì‹¤íŒ¨: {e}, fallback ì‚¬ìš©")
            return self._fallback_keyword_classification(question)

    def _fallback_keyword_classification(self, question: str) -> Dict[str, Any]:
        """ğŸ”§ í‚¤ì›Œë“œ ê¸°ë°˜ fallback ë¶„ë¥˜ (ê°•í™”ëœ ë²„ì „)"""
        question_lower = question.lower()
        
        # ì˜ˆì¸¡ ê´€ë ¨ í‚¤ì›Œë“œ
        prediction_keywords = [
            'ì˜ˆì¸¡', 'ì˜ˆìƒ', 'ë¯¸ë˜', 'ë‚´ì¼', 'ë‹¤ìŒ', 'ìˆ˜ìš”', 'ì „ë§', 'íŠ¸ë Œë“œ',
            'ëŠ˜ì–´ë‚ ', 'ì¦ê°€', 'ê°ì†Œ', 'ë³€í™”', 'ì–´ë–¨ê¹Œ', 'ë ê¹Œ', 'ì˜ˆì¸¡í•´'
        ]
        
        # ì´ìƒ íƒì§€ í‚¤ì›Œë“œ  
        anomaly_keywords = [
            'ì´ìƒ', 'ë¬¸ì œ', 'ë¹„ì •ìƒ', 'ìœ„í—˜', 'ì˜¤ë¥˜', 'ì˜ëª»', 'ì´ìŠˆ',
            'íŒ¨í„´', 'ë¬¸ì œê°€', 'ìœ„í—˜í•œ', 'ë¹„ì •ìƒì ', 'ì´ìƒí•œ'
        ]
        
        # í´ëŸ¬ìŠ¤í„°ë§ í‚¤ì›Œë“œ
        clustering_keywords = [
            'ë¶„ë¥˜', 'ê·¸ë£¹', 'ì¹´í…Œê³ ë¦¬', 'ë¹„ìŠ·', 'ìœ ì‚¬', 'ë¬¶ì–´', 'ë¶„ì„',
            'í´ëŸ¬ìŠ¤í„°', 'ê·¸ë£¹í•‘', 'ì¹´í…Œê³ ë¦¬ë³„', 'ë¶„ë¥˜í•´'
        ]
        
        # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
        prediction_score = sum(1 for kw in prediction_keywords if kw in question_lower)
        anomaly_score = sum(1 for kw in anomaly_keywords if kw in question_lower)
        clustering_score = sum(1 for kw in clustering_keywords if kw in question_lower)
        
        # ìµœê³  ì ìˆ˜ ê²°ì •
        scores = {
            'PREDICTION': prediction_score,
            'ANOMALY': anomaly_score, 
            'CLUSTERING': clustering_score
        }
        
        max_category = max(scores, key=scores.get)
        max_score = scores[max_category]
        
        if max_score > 0:
            confidence = min(0.8, 0.5 + (max_score * 0.1))  # ìµœëŒ€ 0.8
            self.logger.info(f"ğŸ”§ [FALLBACK] {question[:30]}... â†’ {max_category} (í‚¤ì›Œë“œ {max_score}ê°œ)")
            return {
                "category": max_category,
                "confidence": confidence,
                "reasoning": f"í‚¤ì›Œë“œ ë§¤ì¹­: {max_score}ê°œ",
                "needs_ml": True
            }
        else:
            # ì¼ë°˜ ë°ì´í„° ì¡°íšŒë¡œ ë¶„ë¥˜
            return {
                "category": "DATA",
                "confidence": 0.3,
                "reasoning": "í‚¤ì›Œë“œ ë§¤ì¹­ ì—†ìŒ",
                "needs_ml": False
            }

    async def determine_optimal_mode(self, question: str) -> RAGMode:
        """ğŸ¯ ì§ˆë¬¸ì— ìµœì í™”ëœ ì²˜ë¦¬ ëª¨ë“œ ê²°ì • (AI ê¸°ë°˜ ML ë¶„ë¥˜ í†µí•©)"""
        
        self.logger.info(f"ğŸ“Š [MODE_DECISION] ëª¨ë“œ ê²°ì • ì‹œì‘: '{question[:50]}...'")
        
        # ğŸš€ AI ê¸°ë°˜ ML ë¶„ë¥˜ ë¨¼ì € ì‹œë„
        ml_classification = await self._classify_ml_intent(question)
        
        self.logger.info(f"ğŸ¤– [MODE_DECISION] ML ë¶„ë¥˜ ê²°ê³¼: {ml_classification}")
        
        if ml_classification["needs_ml"] and ml_classification["confidence"] > 0.7:
            self.logger.info(f"âœ… [MODE_DECISION] TOOL_ENHANCED ëª¨ë“œ ì„ íƒ: {ml_classification['category']} (ì‹ ë¢°ë„: {ml_classification['confidence']})")
            return RAGMode.TOOL_ENHANCED
        elif ml_classification["needs_ml"]:
            self.logger.info(f"âš ï¸ [MODE_DECISION] ML í•„ìš”í•˜ì§€ë§Œ ì‹ ë¢°ë„ ë‚®ìŒ: {ml_classification['confidence']} < 0.7, ê¸°ì¡´ ë¡œì§ ì‚¬ìš©")
        
        # ê¸°ì¡´ ë¡œì§ (fallback)
        question_lower = question.lower()
        
        # ğŸš€ ë‹¨ìˆœ í†µê³„ ì§ˆë¬¸ - SIMPLE ëª¨ë“œ
        simple_patterns = [
            "ì´ ì¬ê³ ", "ì „ì²´ ì¬ê³ ", "ì´ ì…ê³ ", "ì´ ì¶œê³ ",
            "ëª‡ ê°œ", "ì–¼ë§ˆë‚˜", "ì´í•©", "ì „ì²´ ê°œìˆ˜"
        ]
        
        # ğŸ¢ ë™ ê´€ë ¨ ì§ˆë¬¸ - TOOL_ENHANCED ëª¨ë“œ
        rack_patterns = [
            "ë™", "rack", "aë™", "bë™", "cë™", "í™œìš©ë¥ ", "ë™ë³„"
        ]
        
        # ğŸ“… ë‚ ì§œ ê´€ë ¨ ì§ˆë¬¸ - TOOL_ENHANCED ëª¨ë“œ  
        date_patterns = [
            "1ì›”", "ë‚ ì§œ", "ì–¸ì œ", "2025", "ì¼ë³„", "ë§¤ì¼"
        ]
        
        # âš ï¸ ì¬ê³  ë¶€ì¡±/ìœ„í—˜ - TOOL_ENHANCED ëª¨ë“œ
        alert_patterns = [
            "ë¶€ì¡±", "ìœ„í—˜", "ë‚®ì€", "ì ì€", "ê²½ê³ ", "ë¶€ì¡±í•œ"
        ]
        
        # ğŸ“Š íŠ¸ë Œë“œ/ë¶„ì„ - TOOL_ENHANCED ëª¨ë“œ
        analysis_patterns = [
            "ë¶„ì„", "íŠ¸ë Œë“œ", "ê²½í–¥", "íŒ¨í„´", "ë³€í™”", "ì¶”ì„¸"
        ]
        
        # ğŸ” ë³µì¡í•œ ì§ˆë¬¸/ë¹„êµ - SELF_RAG ëª¨ë“œ
        complex_patterns = [
            "ë¹„êµ", "ì–´ë–¤ ì°¨ì´", "ì™œ", "ì–´ë–»ê²Œ", "ì›ì¸", "ì´ìœ ",
            "ê°€ì¥ ì¢‹ì€", "ìµœì ì˜", "ì¶”ì²œ", "ì œì•ˆ"
        ]
        
        # íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ëª¨ë“œ ê²°ì •
        if any(pattern in question_lower for pattern in simple_patterns):
            self.logger.info(f"ğŸ¯ [MODE_DECISION] SIMPLE ëª¨ë“œ ì„ íƒ: ë‹¨ìˆœ í†µê³„ ì§ˆë¬¸")
            return RAGMode.SIMPLE
        
        elif any(pattern in question_lower for pattern in (rack_patterns + date_patterns + alert_patterns + analysis_patterns)):
            self.logger.info(f"ğŸ¯ [MODE_DECISION] TOOL_ENHANCED ëª¨ë“œ ì„ íƒ: ì „ë¬¸ ë„êµ¬ í•„ìš”")
            return RAGMode.TOOL_ENHANCED
        
        elif any(pattern in question_lower for pattern in complex_patterns):
            self.logger.info(f"ğŸ¯ [MODE_DECISION] SELF_RAG ëª¨ë“œ ì„ íƒ: ë³µì¡í•œ ë¶„ì„ í•„ìš”")
            return RAGMode.SELF_RAG
        
        # ê¸°ë³¸ê°’: ì§ˆë¬¸ ê¸¸ì´ë¡œ íŒë‹¨
        elif len(question.split()) > 10:
            self.logger.info(f"ğŸ¯ [MODE_DECISION] SELF_RAG ëª¨ë“œ ì„ íƒ: ê¸´ ì§ˆë¬¸ (ë‹¨ì–´ ìˆ˜: {len(question.split())})")
            return RAGMode.SELF_RAG
        
        else:
            self.logger.info(f"ğŸ¯ [MODE_DECISION] TOOL_ENHANCED ëª¨ë“œ ì„ íƒ: ê¸°ë³¸ê°’")
            return RAGMode.TOOL_ENHANCED

    async def process_with_adaptive_mode(self, question: str) -> str:
        """ğŸ¯ ì ì‘í˜• ëª¨ë“œë¡œ ì§ˆë¬¸ ì²˜ë¦¬ (ê°œì„ ëœ ì§„ì…ì )"""
        # ìµœì  ëª¨ë“œ ê²°ì •
        optimal_mode = self.determine_optimal_mode(question)
        
        self.logger.info(f"ğŸ§  [ADAPTIVE] ì ì‘í˜• ì²˜ë¦¬ ì‹œì‘: '{question}'")
        self.logger.info(f"ğŸ¯ [ADAPTIVE] ì„ íƒëœ ëª¨ë“œ: {optimal_mode.value}")
        
        if optimal_mode == RAGMode.SIMPLE:
            return await self.process_simple_query(question)
        elif optimal_mode == RAGMode.TOOL_ENHANCED:
            return await self.process_with_tools(question)
        else:
            return await self.process_with_self_rag(question, optimal_mode)
    
    async def process_simple_query(self, question: str) -> str:
        """âš¡ ë‹¨ìˆœ ì§ˆë¬¸ ë¹ ë¥¸ ì²˜ë¦¬"""
        self.logger.info(f"âš¡ [SIMPLE] ë‹¨ìˆœ ì§ˆë¬¸ ì²˜ë¦¬: '{question}'")
        
        try:
            # ì§ì ‘ í†µê³„ ê³„ì‚°ìœ¼ë¡œ ë¹ ë¥¸ ì‘ë‹µ
            if "ì´ ì¬ê³ " in question.lower() or "ì „ì²´ ì¬ê³ " in question.lower():
                stats = self._calculate_warehouse_statistics(question)
                return f"ğŸ“Š **ì°½ê³  í†µê³„ ì¡°íšŒ ê²°ê³¼:**\n\n{stats}"
            
            # ê¸°ë³¸ ë²¡í„° ê²€ìƒ‰
            if self.vector_db_service:
                result = await self.vector_db_service.search_relevant_data(question, n_results=5)
                if result.get("success") and result.get("documents"):
                    return f"ğŸ“‹ **ê°„ë‹¨ ì¡°íšŒ ê²°ê³¼:**\n{result['documents'][0]}"
            
            return "âŒ ë¹ ë¥¸ ì¡°íšŒ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
        except Exception as e:
            self.logger.error(f"âŒ [SIMPLE] ë‹¨ìˆœ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return f"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹¨ìˆœ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    async def process_with_tools(self, question: str) -> str:
        """ğŸ”§ LangChain Tools í™œìš© ì²˜ë¦¬"""
        self.logger.info(f"ğŸ”§ [TOOLS] ë„êµ¬ ê¸°ë°˜ ì²˜ë¦¬: '{question}'")
        
        try:
            # ì ì ˆí•œ ë„êµ¬ ì„ íƒ ë° ì‹¤í–‰
            if "ë™" in question.lower():
                result = self._get_rack_specific_info(question)
                
            elif any(word in question.lower() for word in ["ë¶€ì¡±", "ìœ„í—˜", "ë‚®ì€"]):
                result = self._get_low_stock_alerts(question)
                
            elif any(word in question.lower() for word in ["íŠ¸ë Œë“œ", "ë¶„ì„", "íŒ¨í„´"]):
                result = self._analyze_inventory_trends(question)
                
            elif any(word in question.lower() for word in ["í™œìš©ë¥ ", "íš¨ìœ¨"]):
                result = self._calculate_rack_utilization(question)
                
            elif any(word in question.lower() for word in ["1ì›”", "ë‚ ì§œ", "ì¼ë³„"]):
                result = self._get_date_specific_data(question)
                
            else:
                # ê¸°ë³¸ í†µê³„ ê³„ì‚°
                result = self._calculate_warehouse_statistics(question)
            
            return f"ğŸ”§ **ì „ë¬¸ ë„êµ¬ ë¶„ì„ ê²°ê³¼:**\n\n{result}"
            
        except Exception as e:
            self.logger.error(f"âŒ [TOOLS] ë„êµ¬ ê¸°ë°˜ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return f"ì£„ì†¡í•©ë‹ˆë‹¤. ì „ë¬¸ ë„êµ¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    async def process_with_self_rag(self, question: str, mode: RAGMode = RAGMode.SELF_RAG) -> str:
        """SELF-RAG í”„ë¡œì„¸ìŠ¤ë¡œ ì§ˆë¬¸ ì²˜ë¦¬ (ê¸°ì¡´ ë°©ì‹ ìœ ì§€)"""
        self.logger.info(f"ğŸ§  [SELF_RAG] SELF-RAG ì²˜ë¦¬ ì‹œì‘: '{question}'")
        self.logger.info(f"ğŸ¯ [SELF_RAG] ì²˜ë¦¬ ëª¨ë“œ: {mode.value}")
        
        try:
            # 1ë‹¨ê³„: Retrieve (ê²€ìƒ‰)
            self.logger.info("ğŸ” [SELF_RAG_STEP1] ë¬¸ì„œ ê²€ìƒ‰ ë‹¨ê³„")
            retrieval_result = await self._retrieve_documents(question)
            self.logger.info(f"ğŸ“Š [SELF_RAG_RETRIEVE] ê²€ìƒ‰ ê²°ê³¼: {retrieval_result.total_found}ê°œ ë¬¸ì„œ, í’ˆì§ˆ: {retrieval_result.search_quality:.2f}")
            
            # 2ë‹¨ê³„: Critique (ë¹„í‰/ê²€ì¦)
            self.logger.info("ğŸ”¬ [SELF_RAG_STEP2] ê²€ìƒ‰ ê²°ê³¼ ê²€ì¦ ë‹¨ê³„")
            critique_result = await self._critique_retrieval(question, retrieval_result)
            self.logger.info(f"ğŸ“‹ [SELF_RAG_CRITIQUE] ê²€ì¦ ê²°ê³¼ - ê´€ë ¨ì„±: {critique_result.relevance_score:.2f}, ì‹ ë¢°ë„: {critique_result.confidence_score:.2f}, ì¶”ê°€ê²€ìƒ‰í•„ìš”: {critique_result.needs_additional_search}")
            
            # 3ë‹¨ê³„: ì¶”ê°€ ê²€ìƒ‰ í•„ìš”ì„± íŒë‹¨
            if critique_result.needs_additional_search and retrieval_result.total_found > 0:
                self.logger.info("ğŸ”„ [SELF_RAG_STEP3] ì¶”ê°€ ê²€ìƒ‰ í•„ìš” - ì¬ì‹œë„")
                enhanced_query = await self._enhance_query(question, critique_result.missing_info)
                self.logger.info(f"ğŸ“ [SELF_RAG_ENHANCE] ê°•í™”ëœ ì¿¼ë¦¬: '{enhanced_query}'")
                retrieval_result = await self._retrieve_documents(enhanced_query)
                critique_result = await self._critique_retrieval(enhanced_query, retrieval_result)
                self.logger.info(f"ğŸ“Š [SELF_RAG_RECHECK] ì¬ê²€ìƒ‰ ê²°ê³¼: {retrieval_result.total_found}ê°œ ë¬¸ì„œ, ì‹ ë¢°ë„: {critique_result.confidence_score:.2f}")
            
            # 4ë‹¨ê³„: Generate (ë‹µë³€ ìƒì„±)
            self.logger.info("ğŸ’­ [SELF_RAG_STEP4] ë‹µë³€ ìƒì„± ë‹¨ê³„")
            if critique_result.confidence_score >= self.critique_threshold:
                self.logger.info(f"âœ… [SELF_RAG_GENERATE] ê²€ì¦ëœ ì‘ë‹µ ìƒì„± (ì‹ ë¢°ë„: {critique_result.confidence_score:.2f} >= {self.critique_threshold})")
                response = await self._generate_verified_response(question, retrieval_result, critique_result)
            else:
                self.logger.info(f"âš ï¸ [SELF_RAG_GENERATE] ì£¼ì˜ ì‘ë‹µ ìƒì„± (ì‹ ë¢°ë„: {critique_result.confidence_score:.2f} < {self.critique_threshold})")
                response = await self._generate_cautious_response(question, retrieval_result, critique_result)
            
            # 5ë‹¨ê³„: Self-Reflect (ìì²´ ê²€ì¦)
            self.logger.info("ğŸ” [SELF_RAG_STEP5] ìì²´ ê²€ì¦ ë‹¨ê³„")
            final_response = await self._self_reflect_response(question, response, retrieval_result)
            
            self.logger.info("âœ… [SELF_RAG_SUCCESS] SELF-RAG ì²˜ë¦¬ ì™„ë£Œ")
            self.logger.info(f"ğŸ¯ [SELF_RAG_OUTPUT] ìµœì¢… ì‘ë‹µ: '{final_response[:200]}...'")
            return final_response
            
        except Exception as e:
            self.logger.error(f"âŒ [SELF_RAG_ERROR] SELF-RAG ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return f"ì£„ì†¡í•©ë‹ˆë‹¤. ê³ ê¸‰ ê²€ì¦ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    async def _retrieve_documents(self, query: str) -> RetrievalResult:
        """ë¬¸ì„œ ê²€ìƒ‰ ë‹¨ê³„"""
        if not self.vector_db_service:
            return RetrievalResult([], [], {}, 0, 0.0)
        
        try:
            result = await self.vector_db_service.search_relevant_data(
                query=query,
                n_results=20  # SELF-RAGì—ì„œëŠ” ë” ë§ì€ ë¬¸ì„œ ê²€ìƒ‰
            )
            
            if result.get("success"):
                documents = result.get("documents", [])
                scores = [0.8] * len(documents)  # ì„ì‹œ ì ìˆ˜
                metadata = result.get("metadata_summary", {})
                total_found = result.get("found_documents", 0)
                
                # ê²€ìƒ‰ í’ˆì§ˆ ê³„ì‚°
                quality = min(1.0, total_found / 10.0) if total_found > 0 else 0.0
                
                return RetrievalResult(
                    documents=documents,
                    scores=scores,
                    metadata=metadata,
                    total_found=total_found,
                    search_quality=quality
                )
            else:
                return RetrievalResult([], [], {}, 0, 0.0)
                
        except Exception as e:
            self.logger.error(f"ë¬¸ì„œ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return RetrievalResult([], [], {}, 0, 0.0)
    
    async def _critique_retrieval(self, query: str, retrieval: RetrievalResult) -> CritiqueResult:
        """ê²€ìƒ‰ ê²°ê³¼ ë¹„í‰/ê²€ì¦"""
        if not self.ai_client or retrieval.total_found == 0:
            return CritiqueResult(0.0, 0.0, ["ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ"], 1.0, True)
        
        critique_prompt = f"""
ë‹¤ìŒ ê²€ìƒ‰ ê²°ê³¼ê°€ ì§ˆë¬¸ì— ì–¼ë§ˆë‚˜ ê´€ë ¨ì„±ì´ ìˆëŠ”ì§€ í‰ê°€í•´ì£¼ì„¸ìš”.

**ì§ˆë¬¸:** {query}

**ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜:** {retrieval.total_found}ê°œ
**ìƒìœ„ 3ê°œ ë¬¸ì„œ:**
{retrieval.documents[:3]}

**í‰ê°€ ê¸°ì¤€:**
1. ê´€ë ¨ì„± (0.0~1.0): ë¬¸ì„œë“¤ì´ ì§ˆë¬¸ê³¼ ì–¼ë§ˆë‚˜ ê´€ë ¨ìˆëŠ”ê°€?
2. ì‹ ë¢°ë„ (0.0~1.0): ì •ë³´ê°€ ì–¼ë§ˆë‚˜ ì •í™•í•˜ê³  ì™„ì „í•œê°€?
3. ë¶€ì¡±í•œ ì •ë³´: ì¶”ê°€ë¡œ í•„ìš”í•œ ì •ë³´ê°€ ìˆëŠ”ê°€?
4. í• ë£¨ì‹œë„¤ì´ì…˜ ìœ„í—˜ë„ (0.0~1.0): ì˜ëª»ëœ ë‹µë³€ì„ ìƒì„±í•  ìœ„í—˜ì´ ì–¼ë§ˆë‚˜ ë˜ëŠ”ê°€?

**ì‘ë‹µ í˜•ì‹ (JSON):**
{{
    "relevance_score": 0.8,
    "confidence_score": 0.7,
    "missing_info": ["ë¶€ì¡±í•œ ì •ë³´1", "ë¶€ì¡±í•œ ì •ë³´2"],
    "hallucination_risk": 0.3,
    "needs_additional_search": true,
    "reasoning": "í‰ê°€ ì´ìœ "
}}
"""
        
        try:
            response = await self.ai_client.answer_simple_query(critique_prompt, {"critique_analysis": True})
            
            # JSON íŒŒì‹±
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                critique_data = json.loads(json_match.group(0))
                
                return CritiqueResult(
                    relevance_score=critique_data.get("relevance_score", 0.5),
                    confidence_score=critique_data.get("confidence_score", 0.5),
                    missing_info=critique_data.get("missing_info", []),
                    hallucination_risk=critique_data.get("hallucination_risk", 0.5),
                    needs_additional_search=critique_data.get("needs_additional_search", False)
                )
            else:
                # Fallback: ë‹¨ìˆœ íœ´ë¦¬ìŠ¤í‹±
                return self._simple_critique_fallback(query, retrieval)
                
        except Exception as e:
            self.logger.warning(f"ë¹„í‰ ë¶„ì„ ì‹¤íŒ¨, fallback ì‚¬ìš©: {e}")
            return self._simple_critique_fallback(query, retrieval)
    
    def _simple_critique_fallback(self, query: str, retrieval: RetrievalResult) -> CritiqueResult:
        """ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜ ë¹„í‰"""
        # ê¸°ë³¸ì ì¸ ê´€ë ¨ì„± íŒë‹¨
        query_lower = query.lower()
        doc_text = " ".join(retrieval.documents[:5]).lower()
        
        # í‚¤ì›Œë“œ ë§¤ì¹­ ê¸°ë°˜ ì ìˆ˜
        query_words = set(query_lower.split())
        doc_words = set(doc_text.split())
        overlap = len(query_words.intersection(doc_words))
        relevance = min(1.0, overlap / max(1, len(query_words)))
        
        # ì‹ ë¢°ë„ëŠ” ë¬¸ì„œ ìˆ˜ì™€ ê²€ìƒ‰ í’ˆì§ˆ ê¸°ë°˜
        confidence = min(1.0, retrieval.search_quality * (retrieval.total_found / 10.0))
        
        # í• ë£¨ì‹œë„¤ì´ì…˜ ìœ„í—˜ë„
        hallucination_risk = 1.0 - confidence
        
        return CritiqueResult(
            relevance_score=relevance,
            confidence_score=confidence,
            missing_info=[] if relevance > 0.5 else ["ë” êµ¬ì²´ì ì¸ ì •ë³´ í•„ìš”"],
            hallucination_risk=hallucination_risk,
            needs_additional_search=relevance < 0.6
        )
    
    async def _enhance_query(self, original_query: str, missing_info: List[str]) -> str:
        """ë¶€ì¡±í•œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê²€ìƒ‰ ì¿¼ë¦¬ í–¥ìƒ"""
        if not missing_info:
            return original_query
        
        enhanced_query = f"{original_query} {' '.join(missing_info)}"
        self.logger.info(f"ğŸ” ì¿¼ë¦¬ í–¥ìƒ: {original_query} â†’ {enhanced_query}")
        return enhanced_query
    
    async def _generate_verified_response(self, question: str, retrieval: RetrievalResult, critique: CritiqueResult) -> str:
        """ê²€ì¦ëœ ê³ í’ˆì§ˆ ì‘ë‹µ ìƒì„±"""
        if not self.ai_client:
            return "AI ì„œë¹„ìŠ¤ê°€ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # ì‹œìŠ¤í…œ ì»¨í…ìŠ¤íŠ¸ ê°•ì œ ì£¼ì…
        system_context = self._get_system_context()
        
        verified_prompt = f"""
ë‹¹ì‹ ì€ ì°½ê³  ê´€ë¦¬ ì „ë¬¸ AIì…ë‹ˆë‹¤. ê²€ì¦ëœ ì •ë³´ë§Œì„ ì‚¬ìš©í•˜ì—¬ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

**ì‹œìŠ¤í…œ ì»¨í…ìŠ¤íŠ¸ (í•„ìˆ˜ í™•ì¸):**
{system_context}

**ì§ˆë¬¸:** {question}

**ê²€ì¦ëœ ê²€ìƒ‰ ê²°ê³¼ (ì‹ ë¢°ë„: {critique.confidence_score:.2f}):**
{retrieval.documents[:5]}

**ì‘ë‹µ ê·œì¹™:**
1. ğŸ• í˜„ì¬ ë‚ ì§œ: {self.current_datetime.strftime('%Yë…„ %mì›” %dì¼')} (ë°˜ë“œì‹œ ëª…ì‹œ)
2. ğŸ“… ë°ì´í„° ë²”ìœ„: 2025ë…„ 1ì›” 1ì¼~7ì¼ (ê³¼ê±° ë°ì´í„°ì„ì„ ëª…ì‹œ)
3. ğŸ” ê²€ìƒ‰ëœ {retrieval.total_found}ê°œ ë¬¸ì„œ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€
4. âœ… í™•ì¸ëœ ì‚¬ì‹¤ë§Œ ì œì‹œ, ì¶”ì •ì€ ëª…ì‹œì ìœ¼ë¡œ êµ¬ë¶„
5. âš ï¸ ë°ì´í„°ì™€ í˜„ì¬ ë‚ ì§œì˜ ì°¨ì´ë¥¼ ë°˜ë“œì‹œ ì–¸ê¸‰

**ë‹µë³€ ì˜ˆì‹œ:**
"ë°ì´í„°ì— ë”°ë¥´ë©´ 2025ë…„ 1ì›” 5ì¼(ê³¼ê±°)ì— Yë™ì—ì„œëŠ” â—‹â—‹ ìƒí’ˆì´ â—â—ê°œ ì¶œê³ ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¨, ì´ëŠ” ê³¼ê±° ë°ì´í„°ì´ë©° í˜„ì¬ ë‚ ì§œ({self.current_datetime.strftime('%Yë…„ %mì›” %dì¼')})ì™€ëŠ” ë‹¤ë¦…ë‹ˆë‹¤."

ë‹µë³€:"""
        
        try:
            response = await self.ai_client.answer_simple_query(verified_prompt, {"verified_generation": True})
            return response
        except Exception as e:
            return f"ê²€ì¦ëœ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(e)}"
    
    async def _generate_cautious_response(self, question: str, retrieval: RetrievalResult, critique: CritiqueResult) -> str:
        """ì‹ ë¢°ë„ê°€ ë‚®ì„ ë•Œ ì‹ ì¤‘í•œ ì‘ë‹µ ìƒì„±"""
        cautious_response = f"""âš ï¸ **ì œí•œëœ ì •ë³´ë¡œ ì¸í•œ ë¶€ë¶„ ì‘ë‹µ**

**í˜„ì¬ ë‚ ì§œ:** {self.current_datetime.strftime('%Yë…„ %mì›” %dì¼')}
**ë°ì´í„° ë²”ìœ„:** 2025ë…„ 1ì›” 1ì¼~7ì¼ (ê³¼ê±° ë°ì´í„°)

ê²€ìƒ‰ëœ ì •ë³´ê°€ ë¶ˆì™„ì „í•©ë‹ˆë‹¤ (ì‹ ë¢°ë„: {critique.confidence_score:.1f}/1.0).

**ë¶€ì¡±í•œ ì •ë³´:**
{chr(10).join(f'â€¢ {info}' for info in critique.missing_info)}

**ê°€ëŠ¥í•œ ë‹µë³€:**
ê²€ìƒ‰ëœ {retrieval.total_found}ê°œ ë¬¸ì„œì—ì„œ ì œí•œì ì¸ ì •ë³´ë¥¼ í™•ì¸í–ˆìœ¼ë‚˜, ì •í™•í•œ ë‹µë³€ì„ ìœ„í•´ì„œëŠ” ì¶”ê°€ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤.

ğŸ’¡ **ê¶Œì¥ ì‚¬í•­:**
1. ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ ì¬ì‹œë„
2. ëŒ€ì‹œë³´ë“œ ì°¨íŠ¸ í™•ì¸
3. ì‹œìŠ¤í…œ ê´€ë¦¬ì ë¬¸ì˜

ğŸ” **ëŒ€ì•ˆ:** "ì „ì²´ ì¬ê³ ëŸ‰", "ì´ ì…ê³ ëŸ‰" ê°™ì€ ê°„ë‹¨í•œ í†µê³„ ì§ˆë¬¸ì„ ì‹œë„í•´ë³´ì„¸ìš”."""
        
        return cautious_response
    
    async def _self_reflect_response(self, question: str, response: str, retrieval: RetrievalResult) -> str:
        """ì‘ë‹µì— ëŒ€í•œ ìì²´ ê²€ì¦ ë° ìµœì¢… ì¡°ì •"""
        # ê¸°ë³¸ì ì¸ ìì²´ ê²€ì¦
        issues = []
        
        # ë‚ ì§œ ê´€ë ¨ í• ë£¨ì‹œë„¤ì´ì…˜ ì²´í¬
        if "í˜„ì¬" in response and "2025ë…„ 1ì›”" in response:
            if self.current_datetime.strftime('%Yë…„ %mì›”') != "2025ë…„ 01ì›”":
                issues.append("ë‚ ì§œ í˜¼ë™ ìœ„í—˜ ê°ì§€")
        
        # ê³¼ë„í•œ í™•ì‹  ì²´í¬
        confident_words = ["í™•ì‹¤íˆ", "ë¶„ëª…íˆ", "í‹€ë¦¼ì—†ì´"]
        if any(word in response for word in confident_words):
            issues.append("ê³¼ë„í•œ í™•ì‹  í‘œí˜„ ê°ì§€")
        
        # ê²€ì¦ ì •ë³´ ì¶”ê°€
        final_response = response
        
        # ìì²´ ê²€ì¦ ë©”ì‹œì§€ ì œê±° (ì‚¬ìš©ì ìš”ì²­ì— ë”°ë¼ ê°„ì†Œí™”)
        
        # í˜„ì¬ ì‹œê° ì •ë³´ ì œê±° (ì‚¬ìš©ì ìš”ì²­ì— ë”°ë¼ ë©”íƒ€ë°ì´í„° ê°„ì†Œí™”)
        
        return final_response
    
    async def process_with_tools(self, question: str) -> str:
        """LangChain Toolsë¥¼ í™œìš©í•œ ì²˜ë¦¬"""
        if not LANGCHAIN_AVAILABLE:
            return await self.process_with_self_rag(question)
        
        self.logger.info(f"ğŸ”§ Tools í™œìš© ì²˜ë¦¬: {question[:50]}...")
        
        try:
            # ê°„ë‹¨í•œ Tool ì²´ì¸ ì‹¤í–‰ (Agent ì—†ì´)
            results = []
            
            # 1. ì‹œìŠ¤í…œ ì»¨í…ìŠ¤íŠ¸ í™•ì¸
            context = self._get_system_context()
            results.append(f"ì‹œìŠ¤í…œ ìƒíƒœ: {context}")
            
            # 2. ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¥¸ ì ì ˆí•œ ë„êµ¬ ì„ íƒ
            if any(word in question.lower() for word in ['ì˜¤ëŠ˜', 'í˜„ì¬', 'ì§€ê¸ˆ', 'ëª‡ì›”', 'ë‚ ì§œ']):
                datetime_info = self._get_current_datetime()
                results.append(f"ë‚ ì§œ/ì‹œê°„ ì •ë³´: {datetime_info}")
            
            if any(word in question.lower() for word in ['ê²€ìƒ‰', 'ì°¾ê¸°', 'ì–´ë–¤', 'ë¬´ì—‡', 'ëˆ„ê°€']):
                search_result = self._search_vector_database(question)
                results.append(f"ê²€ìƒ‰ ê²°ê³¼: {search_result}")
            
            if any(word in question.lower() for word in ['ì´', 'ì „ì²´', 'í•©ê³„', 'í†µê³„']):
                stats = self._calculate_warehouse_statistics(question)
                results.append(f"í†µê³„ ì •ë³´: {stats}")
            
            # 3. AIë¡œ ìµœì¢… ë‹µë³€ ìƒì„±
            if self.ai_client:
                tools_context = "\n\n".join(results)
                final_prompt = f"""
ë‹¤ìŒ ë„êµ¬ë“¤ì˜ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ì •í™•íˆ ë‹µë³€í•˜ì„¸ìš”:

**ì§ˆë¬¸:** {question}

**ë„êµ¬ ì‹¤í–‰ ê²°ê³¼:**
{tools_context}

**ë‹µë³€ ê·œì¹™:**
1. ë„êµ¬ ê²°ê³¼ì˜ ì •í™•í•œ ì •ë³´ë§Œ ì‚¬ìš©
2. í˜„ì¬ ë‚ ì§œì™€ ë°ì´í„° ë²”ìœ„ êµ¬ë¶„ ëª…ì‹œ
3. ê°„ê²°í•˜ê³  ì •í™•í•œ ë‹µë³€

ë‹µë³€:"""
                
                return await self.ai_client.answer_simple_query(final_prompt, {"tools_enhanced": True})
            else:
                return "\n\n".join(results)
                
        except Exception as e:
            self.logger.error(f"Tools ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return f"ë„êµ¬ ê¸°ë°˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
    
    async def smart_process_query(self, question: str) -> str:
        """ğŸ§  í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ë²•: Tools + ë²¡í„° ê²€ìƒ‰ + AI í†µí•© ë‹µë³€"""
        self.logger.info(f"ğŸš€ [LANGCHAIN] ìŠ¤ë§ˆíŠ¸ ì²˜ë¦¬ ì‹œì‘: '{question}'")
        question_lower = question.lower()
        
        try:
            # ğŸš€ 1ë‹¨ê³„: Toolsë¡œ ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘ (í•­ìƒ ì‹¤í–‰)
            self.logger.info("ğŸ”§ [LANGCHAIN_TOOLS] Tool ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘ ì‹œì‘")
            tools_context = await self._collect_tools_context(question)
            self.logger.info(f"ğŸ”§ [LANGCHAIN_TOOLS] Tool ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘ ì™„ë£Œ: {list(tools_context.keys()) if tools_context else 'None'}")
            
            # ğŸš€ 2ë‹¨ê³„: ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¥¸ ì¶”ê°€ ì²˜ë¦¬ ê²°ì •
            processing_mode = self._determine_processing_mode(question_lower)
            self.logger.info(f"ğŸ§  [LANGCHAIN_MODE] ì²˜ë¦¬ ëª¨ë“œ ê²°ì •: {processing_mode}")
            
            if processing_mode == "datetime_only":
                # ë‹¨ìˆœ ë‚ ì§œ/ì‹œê°„ ì§ˆë¬¸ â†’ Tools ê²°ê³¼ë§Œ ë°˜í™˜
                self.logger.info("ğŸ“… [LANGCHAIN_DATETIME] ë‚ ì§œ/ì‹œê°„ ì „ìš© ì²˜ë¦¬")
                raw_response = tools_context.get("datetime_info", "ë‚ ì§œ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                result = self._clean_response(raw_response, question, is_simple_question=True)
                self.logger.info(f"âœ… [LANGCHAIN_SUCCESS] ë‚ ì§œ/ì‹œê°„ ì²˜ë¦¬ ì™„ë£Œ: '{result[:100]}...'")
                return result
            
            elif processing_mode == "hybrid_enhanced":
                # ë³µí•© ì§ˆë¬¸ â†’ Tools + ë²¡í„° ê²€ìƒ‰ + AI í†µí•©
                self.logger.info("ğŸ” [LANGCHAIN_HYBRID] í•˜ì´ë¸Œë¦¬ë“œ ê°•í™” ì²˜ë¦¬")
                result = await self._process_hybrid_enhanced(question, tools_context)
                self.logger.info(f"âœ… [LANGCHAIN_SUCCESS] í•˜ì´ë¸Œë¦¬ë“œ ì²˜ë¦¬ ì™„ë£Œ: '{result[:100]}...'")
                return result
            
            elif processing_mode == "simple_stats":
                # ê°„ë‹¨í•œ í†µê³„ â†’ Tools + ê¸°ë³¸ AI
                self.logger.info("ğŸ“Š [LANGCHAIN_STATS] ê°„ë‹¨ í†µê³„ ì²˜ë¦¬")
                raw_response = await self._process_simple_with_context(question, tools_context)
                result = self._clean_response(raw_response, question, is_simple_question=True)
                self.logger.info(f"âœ… [LANGCHAIN_SUCCESS] í†µê³„ ì²˜ë¦¬ ì™„ë£Œ: '{result[:100]}...'")
                return result
            
            else:
                # ê¸°ë³¸ í•˜ì´ë¸Œë¦¬ë“œ ì²˜ë¦¬
                self.logger.info("ğŸ”„ [LANGCHAIN_DEFAULT] ê¸°ë³¸ í•˜ì´ë¸Œë¦¬ë“œ ì²˜ë¦¬")
                raw_response = await self._process_hybrid_enhanced(question, tools_context)
                result = self._clean_response(raw_response, question, is_simple_question=False)
                self.logger.info(f"âœ… [LANGCHAIN_SUCCESS] ê¸°ë³¸ ì²˜ë¦¬ ì™„ë£Œ: '{result[:100]}...'")
                return result
                
        except Exception as e:
            self.logger.error(f"í•˜ì´ë¸Œë¦¬ë“œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
    
    def _determine_processing_mode(self, question_lower: str) -> str:
        """ğŸ” ê°•í™”ëœ ì§ˆë¬¸ ìœ í˜• ë¶„ì„ - ì²˜ë¦¬ ëª¨ë“œ ê²°ì •"""
        
        # ğŸ• 1ìˆœìœ„: ë‹¨ìˆœ ë‚ ì§œ/ì‹œê°„ ì§ˆë¬¸ (ê°€ì¥ ê°„ë‹¨)
        datetime_patterns = [
            r'ì˜¤ëŠ˜.*ëª‡ì›”', r'ì˜¤ëŠ˜.*ëª‡ì¼', r'í˜„ì¬.*ëª‡ì›”', r'í˜„ì¬.*ëª‡ì¼', 
            r'ì§€ê¸ˆ.*ëª‡ì›”', r'ì§€ê¸ˆ.*ëª‡ì¼', r'ë‚ ì§œ.*ëª‡ì›”', r'ë‚ ì§œ.*ëª‡ì¼',
            r'^ì˜¤ëŠ˜$', r'ì˜¤ëŠ˜.*ë‚ ì§œ', r'í˜„ì¬.*ë‚ ì§œ'
        ]
        
        import re
        if any(re.search(pattern, question_lower) for pattern in datetime_patterns):
            return "datetime_only"
        
        # ğŸ“Š 2ìˆœìœ„: ë‹¨ìˆœ í†µê³„ ì§ˆë¬¸ (ë¹ ë¥¸ ê³„ì‚°)
        simple_stats_patterns = [
            r'ì´.*ì¬ê³ ëŸ‰?$', r'ì „ì²´.*ì¬ê³ $', r'ì´ì¬ê³ $',
            r'ì´.*ì…ê³ ëŸ‰?$', r'ì´.*ì¶œê³ ëŸ‰?$', r'í•©ê³„.*ëª‡',
            r'ì „ì²´.*ëª‡.*ê°œ', r'ì´.*ëª‡.*ê°œ'
        ]
        
        if any(re.search(pattern, question_lower) for pattern in simple_stats_patterns):
            return "simple_stats"
        
        # ğŸ§  3ìˆœìœ„: ë³µì¡í•œ ë¶„ì„ ì§ˆë¬¸ íŒë‹¨
        analysis_indicators = [
            # ë¶„ì„ í‚¤ì›Œë“œ
            'ë¹„ì •ìƒì ', 'ë¶„ì„', 'ë¹„êµ', 'íŠ¸ë Œë“œ', 'íŒ¨í„´', 'ì˜ˆì¸¡',
            'ìƒìœ„', 'í•˜ìœ„', 'ìˆœìœ„', 'ë­í‚¹', 'ìµœê³ ', 'ìµœì €',
            
            # ìƒì„¸ ì •ë³´ ìš”êµ¬
            'ì–´ë–¤', 'ë¬´ì—‡', 'ëˆ„ê°€', 'ì–´ë””', 'ì™œ', 'ì–´ë–»ê²Œ',
            'ìƒíƒœ', 'í˜„í™©', 'ì •ë³´', 'ìƒí™©', 'ë‚´ì—­', 'ëª©ë¡',
            
            # ë¹„êµ/ì¡°ê±´ ì§ˆë¬¸
            'ë§ì€', 'ì ì€', 'ë†’ì€', 'ë‚®ì€', 'ì°¨ì´', 'ë¹„êµ',
            '~ë³´ë‹¤', 'ì´ìƒ', 'ì´í•˜', 'ì´ˆê³¼', 'ë¯¸ë§Œ'
        ]
        
        complex_score = sum(1 for keyword in analysis_indicators if keyword in question_lower)
        
        # ë³µì¡í•œ ì§ˆë¬¸ìœ¼ë¡œ íŒë‹¨ë˜ë©´ ì „ì²´ ë¶„ì„ í•„ìš”
        if complex_score >= 1:
            return "hybrid_enhanced"
        
        # ğŸ”„ 4ìˆœìœ„: ê¸°ë³¸ê°’ (ê¸¸ì´ ê¸°ë°˜ íŒë‹¨)
        if len(question_lower) <= 15:
            return "simple_stats"  # ì§§ì€ ì§ˆë¬¸ì€ ê°„ë‹¨íˆ ì²˜ë¦¬
        else:
            return "hybrid_enhanced"  # ê¸´ ì§ˆë¬¸ì€ ìƒì„¸ ë¶„ì„
    
    async def _collect_tools_context(self, question: str) -> Dict[str, Any]:
        """Toolsë¡œ ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ìˆ˜ì§‘"""
        context = {}
        
        try:
            # í•­ìƒ ì‹œìŠ¤í…œ ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘
            context["system_info"] = self._get_system_context()
            
            # ë‚ ì§œ/ì‹œê°„ ê´€ë ¨ ì§ˆë¬¸ì´ë©´ í˜„ì¬ ì‹œê°„ ì •ë³´ ì¶”ê°€
            if any(word in question.lower() for word in ['ì˜¤ëŠ˜', 'í˜„ì¬', 'ì§€ê¸ˆ', 'ëª‡ì›”', 'ëª‡ì¼', 'ë‚ ì§œ', 'ì‹œê°„']):
                context["datetime_info"] = self._get_current_datetime()
            
            # í†µê³„ ê´€ë ¨ ì§ˆë¬¸ì´ë©´ ê¸°ë³¸ í†µê³„ ìˆ˜ì§‘
            if any(word in question.lower() for word in ['ì´', 'ì „ì²´', 'í•©ê³„', 'í†µê³„', 'ì¬ê³ ', 'ì…ê³ ', 'ì¶œê³ ']):
                context["stats_info"] = self._calculate_warehouse_statistics(question)
            
            self.logger.info(f"âœ… Tools ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘ ì™„ë£Œ: {len(context)}ê°œ í•­ëª©")
            return context
            
        except Exception as e:
            self.logger.warning(f"Tools ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    async def _process_hybrid_enhanced(self, question: str, tools_context: Dict[str, Any]) -> str:
        """í•˜ì´ë¸Œë¦¬ë“œ ê°•í™” ì²˜ë¦¬: Tools + ë²¡í„° ê²€ìƒ‰ + AI í†µí•©"""
        try:
            # ğŸ” ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ìƒì„¸ ë°ì´í„° ìˆ˜ì§‘
            vector_result = await self._enhanced_vector_search(question)
            
            # ğŸ§  ëª¨ë“  ì •ë³´ë¥¼ í†µí•©í•˜ì—¬ AI ë‹µë³€ ìƒì„±
            raw_response = await self._generate_integrated_response(question, tools_context, vector_result)
            
            # ì§ˆë¬¸ ë³µì¡ë„ì— ë”°ë¥¸ ì •ì œ
            is_simple = self._is_simple_question_type(question)
            return self._clean_response(raw_response, question, is_simple_question=is_simple)
            
        except Exception as e:
            self.logger.error(f"í•˜ì´ë¸Œë¦¬ë“œ ê°•í™” ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            # Fallback: Tools ì •ë³´ë§Œìœ¼ë¡œ ê¸°ë³¸ ë‹µë³€
            raw_fallback = await self._process_simple_with_context(question, tools_context)
            return self._clean_response(raw_fallback, question, is_simple_question=True)
    
    async def _process_simple_with_context(self, question: str, tools_context: Dict[str, Any]) -> str:
        """ê°„ë‹¨í•œ ì²˜ë¦¬: Tools ì»¨í…ìŠ¤íŠ¸ë§Œìœ¼ë¡œ AI ë‹µë³€"""
        if not self.ai_client:
            return self._format_tools_only_response(tools_context)
        
        try:
            # ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¥¸ ê²½ëŸ‰ í”„ë¡¬í”„íŠ¸ ìƒì„±
            lightweight_prompt = self._create_lightweight_prompt(question, tools_context)
            
            response = await self.ai_client.answer_simple_query(lightweight_prompt, {"lightweight": True})
            return response
            
        except Exception as e:
            self.logger.error(f"ê°„ë‹¨í•œ ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return self._format_tools_only_response(tools_context)
    
    async def _enhanced_vector_search(self, question: str) -> Dict[str, Any]:
        """ê°•í™”ëœ ë²¡í„° ê²€ìƒ‰ - êµ¬ì¡°í™”ëœ ê²°ê³¼ ë°˜í™˜"""
        if not self.vector_db_service:
            return {"success": False, "error": "ë²¡í„° DB ì„œë¹„ìŠ¤ ì—†ìŒ"}
        
        try:
            # ê¸°ì¡´ ë²¡í„° ê²€ìƒ‰ í˜¸ì¶œ
            result = await self.vector_db_service.search_relevant_data(
                query=question,
                n_results=15  # ìƒì„¸ ë¶„ì„ì„ ìœ„í•´ ì¶©ë¶„í•œ ë¬¸ì„œ ìˆ˜ì§‘
            )
            
            if result.get("success"):
                # ê²€ìƒ‰ ê²°ê³¼ë¥¼ AI ì²˜ë¦¬ìš©ìœ¼ë¡œ êµ¬ì¡°í™”
                enhanced_result = {
                    "success": True,
                    "documents": result.get("documents", []),
                    "metadata": result.get("metadata_summary", {}),
                    "chart_data": result.get("chart_data", {}),
                    "found_count": result.get("found_documents", 0),
                    "search_query": question
                }
                
                self.logger.info(f"ğŸ” ê°•í™”ëœ ë²¡í„° ê²€ìƒ‰ ì„±ê³µ: {enhanced_result['found_count']}ê°œ ë¬¸ì„œ")
                return enhanced_result
            else:
                return {"success": False, "error": result.get("error", "ê²€ìƒ‰ ì‹¤íŒ¨")}
                
        except Exception as e:
            self.logger.error(f"ê°•í™”ëœ ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    async def _generate_integrated_response(self, question: str, tools_context: Dict[str, Any], vector_result: Dict[str, Any]) -> str:
        """ëª¨ë“  ì •ë³´ë¥¼ í†µí•©í•˜ì—¬ ì™„ì „í•œ AI ë‹µë³€ ìƒì„±"""
        if not self.ai_client:
            return self._format_integrated_fallback(tools_context, vector_result)
        
        try:
            # í†µí•© ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            integrated_context = self._build_integrated_context(tools_context, vector_result)
            
            integrated_prompt = f"""
ë‹¹ì‹ ì€ ì°½ê³  ê´€ë¦¬ ì „ë¬¸ AIì…ë‹ˆë‹¤. ë‹¤ìŒì˜ ëª¨ë“  ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ìƒì„¸í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

**ì§ˆë¬¸:** {question}

{integrated_context}

**ë‹µë³€ ê·œì¹™:**
1. ğŸ• í˜„ì¬ ë‚ ì§œ: {self.current_datetime.strftime('%Yë…„ %mì›” %dì¼')} (ë°˜ë“œì‹œ ëª…ì‹œ)
2. ğŸ“… ë°ì´í„° ë²”ìœ„: 2025ë…„ 1ì›” 1ì¼~7ì¼ (ê³¼ê±° ë°ì´í„°ì„ì„ ëª…ì‹œ)
3. ğŸ” Tools ì •ë³´ì™€ ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ë¥¼ ëª¨ë‘ í™œìš©
4. âœ… êµ¬ì²´ì ì¸ ìˆ˜ì¹˜, íšŒì‚¬ëª…, ìƒí’ˆëª… ë“± ìƒì„¸ ì •ë³´ í¬í•¨
5. âš ï¸ ì¶”ì •ì´ë‚˜ ë¶ˆí™•ì‹¤í•œ ë‚´ìš©ì€ ëª…ì‹œì ìœ¼ë¡œ êµ¬ë¶„
6. ğŸ“Š ì •ë³´ ì¶œì²˜ ëª…ì‹œ (ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜, ë„êµ¬ ê²°ê³¼ ë“±)

**ë‹µë³€ ì˜ˆì‹œ:**
"í˜„ì¬ ë‚ ì§œ({self.current_datetime.strftime('%Yë…„ %mì›” %dì¼')})ì™€ ê³¼ê±° ë°ì´í„°(2025ë…„ 1ì›”)ë¥¼ êµ¬ë¶„í•˜ì—¬ ë§ì”€ë“œë¦¬ë©´..."

ë‹µë³€:"""
            
            response = await self.ai_client.answer_simple_query(integrated_prompt, {"integrated_response": True})
            
            # ğŸ“Š ì¡°ê±´ë¶€ ì¶œì²˜ ì •ë³´ ì¶”ê°€ (ë³µì¡í•œ ì§ˆë¬¸ì—ë§Œ)
            if vector_result.get("success"):
                found_count = vector_result.get('found_count', 0)
                # ì§ˆë¬¸ ë³µì¡ë„ì— ë”°ë¥¸ ì¶œì²˜ ì •ë³´ í‘œì‹œ
                if self._should_show_source_info(question, found_count):
                    response += f"\n\n({found_count}ê°œ ë°ì´í„° ê¸°ë°˜)"
            
            return response
            
        except Exception as e:
            self.logger.error(f"í†µí•© ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            return self._format_integrated_fallback(tools_context, vector_result)
    
    def _build_integrated_context(self, tools_context: Dict[str, Any], vector_result: Dict[str, Any]) -> str:
        """í†µí•© ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±"""
        context_parts = []
        
        # Tools ì •ë³´ ì¶”ê°€
        if tools_context:
            context_parts.append("== ë„êµ¬ ìˆ˜ì§‘ ì •ë³´ ==")
            
            if "datetime_info" in tools_context:
                context_parts.append(f"í˜„ì¬ ì‹œê°„ ì •ë³´:\n{tools_context['datetime_info']}")
            
            if "stats_info" in tools_context:
                context_parts.append(f"í†µê³„ ì •ë³´:\n{tools_context['stats_info']}")
            
            if "system_info" in tools_context:
                context_parts.append(f"ì‹œìŠ¤í…œ ì •ë³´:\n{tools_context['system_info']}")
        
        # ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€
        if vector_result.get("success"):
            context_parts.append("== ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ ==")
            context_parts.append(f"ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {vector_result.get('found_count', 0)}ê°œ")
            
            documents = vector_result.get("documents", [])
            if documents:
                context_parts.append("ì£¼ìš” ê²€ìƒ‰ ë¬¸ì„œ:")
                for i, doc in enumerate(documents[:5], 1):
                    context_parts.append(f"{i}. {doc[:200]}...")
            
            metadata = vector_result.get("metadata", {})
            if metadata:
                context_parts.append(f"ë©”íƒ€ë°ì´í„°: {metadata}")
            
            chart_data = vector_result.get("chart_data", {})
            if chart_data:
                context_parts.append(f"ì°¨íŠ¸ ë°ì´í„°: {chart_data}")
        
        return "\n\n".join(context_parts)
    
    def _format_tools_context(self, tools_context: Dict[str, Any]) -> str:
        """Tools ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…"""
        formatted_parts = []
        
        for key, value in tools_context.items():
            if key == "error":
                formatted_parts.append(f"âš ï¸ ì˜¤ë¥˜: {value}")
            elif key == "datetime_info":
                formatted_parts.append(f"ğŸ“… ë‚ ì§œ/ì‹œê°„: {value}")
            elif key == "stats_info":
                formatted_parts.append(f"ğŸ“Š í†µê³„: {value}")
            elif key == "system_info":
                formatted_parts.append(f"ğŸ”§ ì‹œìŠ¤í…œ: {value}")
            else:
                formatted_parts.append(f"{key}: {value}")
        
        return "\n".join(formatted_parts)
    
    def _format_tools_only_response(self, tools_context: Dict[str, Any]) -> str:
        """Tools ì •ë³´ë§Œìœ¼ë¡œ ê¸°ë³¸ ì‘ë‹µ ìƒì„±"""
        if not tools_context:
            return "ìˆ˜ì§‘ëœ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        if "error" in tools_context:
            return f"ì •ë³´ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {tools_context['error']}"
        
        response_parts = []
        
        if "datetime_info" in tools_context:
            response_parts.append(tools_context["datetime_info"])
        
        if "stats_info" in tools_context:
            response_parts.append(tools_context["stats_info"])
        
        if response_parts:
            return "\n\n".join(response_parts)
        else:
            return "ìš”ì²­í•˜ì‹  ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    def _format_integrated_fallback(self, tools_context: Dict[str, Any], vector_result: Dict[str, Any]) -> str:
        """í†µí•© ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ fallback ì‘ë‹µ"""
        response_parts = []
        
        response_parts.append("âš ï¸ **AI ì²˜ë¦¬ ì‹¤íŒ¨ - ìˆ˜ì§‘ëœ ì •ë³´ë§Œ ì œê³µ**")
        response_parts.append("")
        
        # Tools ì •ë³´
        if tools_context:
            response_parts.append("ğŸ”§ **ë„êµ¬ ìˆ˜ì§‘ ì •ë³´:**")
            response_parts.append(self._format_tools_context(tools_context))
            response_parts.append("")
        
        # ë²¡í„° ê²€ìƒ‰ ì •ë³´
        if vector_result.get("success"):
            response_parts.append("ğŸ” **ë²¡í„° ê²€ìƒ‰ ì •ë³´:**")
            response_parts.append(f"ê²€ìƒ‰ëœ ë¬¸ì„œ: {vector_result.get('found_count', 0)}ê°œ")
            
            documents = vector_result.get("documents", [])
            if documents:
                response_parts.append("ì£¼ìš” ë‚´ìš©:")
                for i, doc in enumerate(documents[:3], 1):
                    response_parts.append(f"{i}. {doc[:150]}...")
        
        response_parts.append("")
        # í˜„ì¬ ì‹œê° ì •ë³´ ì œê±° (ì‚¬ìš©ì ìš”ì²­)
        
        return "\n".join(response_parts)
    
    def _clean_response(self, response: str, question: str, is_simple_question: bool = False) -> str:
        """ğŸ§¹ ì‘ë‹µ í›„ì²˜ë¦¬ - ê¹”ë”í•œ ë‹µë³€ìœ¼ë¡œ ì •ì œ"""
        if not response:
            return response
        
        try:
            # 1ë‹¨ê³„: ë¶ˆí•„ìš”í•œ ì´ëª¨ì§€ ì œê±° (ê²½ê³  ì´ëª¨ì§€ëŠ” ìœ ì§€)
            cleaned = self._remove_unnecessary_emojis(response)
            
            # 2ë‹¨ê³„: ì¤‘ë³µ ì •ë³´ ì œê±°
            cleaned = self._remove_duplicate_info(cleaned)
            
            # 3ë‹¨ê³„: ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¥¸ íŠ¹ë³„ ì²˜ë¦¬
            if is_simple_question:
                cleaned = self._simplify_for_basic_questions(cleaned, question)
            
            # 4ë‹¨ê³„: ê¸°ìˆ ì  ì •ë³´ ì •ë¦¬
            cleaned = self._clean_technical_info(cleaned, is_simple_question)
            
            # 5ë‹¨ê³„: ì‘ë‹µ í…œí”Œë¦¿ ì ìš©
            cleaned = self._apply_response_template(cleaned, question, is_complex=not is_simple_question)
            
            # 6ë‹¨ê³„: ìµœì¢… í¬ë§·íŒ…
            cleaned = self._final_formatting(cleaned)
            
            self.logger.debug(f"ì‘ë‹µ ì •ì œ ì™„ë£Œ: {len(response)} â†’ {len(cleaned)} ë¬¸ì")
            return cleaned
            
        except Exception as e:
            self.logger.warning(f"ì‘ë‹µ ì •ì œ ì‹¤íŒ¨, ì›ë³¸ ë°˜í™˜: {e}")
            return response
    
    def _remove_unnecessary_emojis(self, text: str) -> str:
        """ë¶ˆí•„ìš”í•œ ì¥ì‹ìš© ì´ëª¨ì§€ ì œê±° (ê²½ê³  ì´ëª¨ì§€ëŠ” ìœ ì§€)"""
        import re
        
        # ìœ ì§€í•  ì´ëª¨ì§€ (ê²½ê³ , ì£¼ì˜ì‚¬í•­ ê´€ë ¨ë§Œ)
        keep_emojis = ["âš ï¸", "âŒ", "âœ…"]
        
        # ì œê±°í•  ì¥ì‹ìš© ì´ëª¨ì§€ íŒ¨í„´
        decorative_patterns = [
            r"ğŸ”\s*\*?",  # ğŸ” ê²€ìƒ‰ ì•„ì´ì½˜
            r"ğŸ“Š\s*\*?",  # ğŸ“Š ì°¨íŠ¸ ì•„ì´ì½˜
            r"ğŸ”§\s*\*?",  # ğŸ”§ ë„êµ¬ ì•„ì´ì½˜
            r"ğŸ§ \s*\*?",  # ğŸ§  ë‘ë‡Œ ì•„ì´ì½˜
            r"ğŸš€\s*\*?",  # ğŸš€ ë¡œì¼“ ì•„ì´ì½˜
            r"ğŸ’¡\s*\*?",  # ğŸ’¡ ì „êµ¬ ì•„ì´ì½˜
            r"ğŸ¯\s*\*?",  # ğŸ¯ íƒ€ê²Ÿ ì•„ì´ì½˜
            r"ğŸ“…\s*\*?",  # ğŸ“… ë‹¬ë ¥ ì•„ì´ì½˜
            r"ğŸ•\s*\*?",  # ğŸ• ì‹œê³„ ì•„ì´ì½˜
            r"ğŸ¢\s*\*?",  # ğŸ¢ ê±´ë¬¼ ì•„ì´ì½˜
            r"ğŸ“¦\s*\*?",  # ğŸ“¦ ë°•ìŠ¤ ì•„ì´ì½˜
            r"ğŸ“ˆ\s*\*?",  # ğŸ“ˆ ê·¸ë˜í”„ ì•„ì´ì½˜
            r"ğŸ”„\s*\*?",  # ğŸ”„ ìˆœí™˜ ì•„ì´ì½˜
        ]
        
        cleaned = text
        for pattern in decorative_patterns:
            cleaned = re.sub(pattern, "", cleaned)
        
        return cleaned
    
    def _remove_duplicate_info(self, text: str) -> str:
        """ì¤‘ë³µëœ ë‚ ì§œ/ì‹œê°„ ì •ë³´ ì œê±°"""
        import re
        
        # í˜„ì¬ ë‚ ì§œ ì¤‘ë³µ ì œê±° (ì—¬ëŸ¬ ë²ˆ ì–¸ê¸‰ëœ ê²½ìš°)
        current_date_pattern = r"í˜„ì¬ ë‚ ì§œ[:\s]*2025ë…„\s*0?8ì›”\s*0?3ì¼"
        matches = re.findall(current_date_pattern, text)
        if len(matches) > 1:
            # ì²« ë²ˆì§¸ ì–¸ê¸‰ë§Œ ìœ ì§€
            text = re.sub(current_date_pattern, "", text)
            text = f"í˜„ì¬ ë‚ ì§œ: 2025ë…„ 8ì›” 3ì¼\n\n{text}"
        
        # ê³¼ë„í•œ ì¤„ë°”ê¿ˆ ì •ë¦¬
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        return text.strip()
    
    def _simplify_for_basic_questions(self, text: str, question: str) -> str:
        """ğŸ“ ê°„ë‹¨í•œ ì§ˆë¬¸ì— ëŒ€í•œ í‘œì¤€ ì‘ë‹µ í…œí”Œë¦¿ ì ìš©"""
        question_lower = question.lower()
        import re
        
        # ğŸ• ë‚ ì§œ/ì‹œê°„ ì§ˆë¬¸ í…œí”Œë¦¿
        if any(word in question_lower for word in ['ì˜¤ëŠ˜', 'ëª‡ì›”', 'ëª‡ì¼', 'ë‚ ì§œ', 'í˜„ì¬']):
            # ë‚ ì§œ ì •ë³´ ì¶”ì¶œ
            date_pattern = r"2025ë…„\s*0?8ì›”\s*0?3ì¼"
            date_match = re.search(date_pattern, text)
            
            if date_match:
                clean_date = re.sub(r'\s+', ' ', date_match.group())
                return f"ì˜¤ëŠ˜ì€ {clean_date}ì…ë‹ˆë‹¤."
            else:
                # í˜„ì¬ ë‚ ì§œ fallback
                current_date = self.current_datetime.strftime('%Yë…„ %mì›” %dì¼')
                return f"ì˜¤ëŠ˜ì€ {current_date}ì…ë‹ˆë‹¤."
        
        # ğŸ“Š ì¬ê³ ëŸ‰ ì§ˆë¬¸ í…œí”Œë¦¿
        if any(pattern in question_lower for pattern in ['ì´ ì¬ê³ ëŸ‰', 'ì´ì¬ê³ ', 'ì „ì²´ ì¬ê³ ']):
            number_pattern = r"(\d{1,3}(?:,\d{3})*)\s*ê°œ"
            number_match = re.search(number_pattern, text)
            
            if number_match:
                return f"ì´ ì¬ê³ ëŸ‰ì€ {number_match.group(1)}ê°œì…ë‹ˆë‹¤."
        
        # ğŸ“¦ ì…ê³ ëŸ‰ ì§ˆë¬¸ í…œí”Œë¦¿
        if any(pattern in question_lower for pattern in ['ì´ ì…ê³ ëŸ‰', 'ì…ê³ ëŸ‰', 'ì…ê³  í˜„í™©']):
            number_pattern = r"(\d{1,3}(?:,\d{3})*)\s*ê°œ"
            number_match = re.search(number_pattern, text)
            
            if number_match:
                return f"ì´ ì…ê³ ëŸ‰ì€ {number_match.group(1)}ê°œì…ë‹ˆë‹¤."
        
        # ğŸšš ì¶œê³ ëŸ‰ ì§ˆë¬¸ í…œí”Œë¦¿
        if any(pattern in question_lower for pattern in ['ì´ ì¶œê³ ëŸ‰', 'ì¶œê³ ëŸ‰', 'ì¶œê³  í˜„í™©']):
            number_pattern = r"(\d{1,3}(?:,\d{3})*)\s*ê°œ"
            number_match = re.search(number_pattern, text)
            
            if number_match:
                return f"ì´ ì¶œê³ ëŸ‰ì€ {number_match.group(1)}ê°œì…ë‹ˆë‹¤."
        
        # ğŸ¢ ë™ ìƒíƒœ ì§ˆë¬¸ í…œí”Œë¦¿
        rack_patterns = ['aë™', 'bë™', 'cë™', 'dë™', 'eë™', 'fë™']
        matched_rack = None
        for rack in rack_patterns:
            if rack in question_lower:
                matched_rack = rack.upper()
                break
        
        if matched_rack and any(word in question_lower for word in ['ìƒíƒœ', 'í˜„í™©', 'ì–´ë•Œ']):
            # ë™ ê´€ë ¨ ìˆ«ì ì •ë³´ ì¶”ì¶œ
            numbers = re.findall(r"(\d{1,3}(?:,\d{3})*)\s*ê°œ", text)
            if numbers:
                return f"{matched_rack}ì˜ í˜„ì¬ ì¬ê³ ëŠ” {numbers[0]}ê°œì…ë‹ˆë‹¤."
        
        # ğŸ“ˆ ìƒí’ˆ ì§ˆë¬¸ í…œí”Œë¦¿ (ê°€ì¥ ë§ì´/ì ê²Œ ë“±)
        if any(word in question_lower for word in ['ê°€ì¥ ë§ì´', 'ìµœëŒ€', 'ê°€ì¥ ì ê²Œ', 'ìµœì†Œ']) and 'ìƒí’ˆ' in question_lower:
            # ìƒí’ˆëª… ì¶”ì¶œ (ê°„ë‹¨í•œ íŒ¨í„´)
            product_patterns = [
                r"'([^']+)'",  # ë”°ì˜´í‘œë¡œ ë‘˜ëŸ¬ì‹¸ì¸ ìƒí’ˆëª…
                r"ã€Œ([^ã€]+)ã€",  # ì¼ë³¸ì‹ ê´„í˜¸
                r"([ê°€-í£]{2,}(?:ì½œë¼|ë‚˜ì´í”„|êµ­ìˆ˜|ìš”êµ¬ë¥´íŠ¸|ìŠ¤í”„ë¼ì´íŠ¸))",  # ì¼ë°˜ì ì¸ ìƒí’ˆëª… íŒ¨í„´
            ]
            
            for pattern in product_patterns:
                product_match = re.search(pattern, text)
                if product_match:
                    product_name = product_match.group(1)
                    # ìƒí’ˆëª… ì •ë¦¬ (ê¸´ ì´ë¦„ì€ ì¶•ì•½)
                    if len(product_name) > 15:
                        product_name = product_name[:12] + "..."
                    
                    # ìˆ˜ëŸ‰ ì •ë³´ ì¶”ì¶œ
                    quantity_match = re.search(r"(\d+)\s*ê°œ", text)
                    if quantity_match:
                        return f"ê°€ì¥ ë§ì´ ì¶œê³ ëœ ìƒí’ˆì€ '{product_name}'ì…ë‹ˆë‹¤ ({quantity_match.group(1)}ê°œ)."
                    else:
                        return f"ê°€ì¥ ë§ì´ ì¶œê³ ëœ ìƒí’ˆì€ '{product_name}'ì…ë‹ˆë‹¤."
        
        return text
    
    def _clean_technical_info(self, text: str, is_simple_question: bool) -> str:
        """ğŸ”§ ê¸°ìˆ ì  ì •ë³´ ì¡°ê±´ë¶€ í‘œì‹œ - ì§ˆë¬¸ ë³µì¡ë„ì— ë”°ë¥¸ ì¶œì²˜ ì •ë³´ ê´€ë¦¬"""
        import re
        
        if is_simple_question:
            # ğŸš« ê°„ë‹¨í•œ ì§ˆë¬¸: ëª¨ë“  ê¸°ìˆ ì  ì •ë³´ ì™„ì „ ì œê±°
            technical_patterns = [
                # ì¶œì²˜ ì •ë³´ íŒ¨í„´
                r"\*.*?ê°œì˜?\s*(?:ë²¡í„°\s*ê²€ìƒ‰\s*ê²°ê³¼|ê´€ë ¨\s*ë°ì´í„°|ë¬¸ì„œ).*?\*",
                r"\*.*?ë°ì´í„°.*?ë¶„ì„.*?ê²°ê³¼.*?\*",
                r"\([0-9]+ê°œ\s*ë°ì´í„°\s*ë¶„ì„\)",
                
                # ê¸°ìˆ ì  ìš©ì–´ ì œê±°
                r"ë²¡í„°\s*ê²€ìƒ‰\s*ê²°ê³¼",
                r"ë„êµ¬\s*ì •ë³´ë¥¼?\s*ì¢…í•©í•œ?",
                r"ê°•í™”ëœ\s*fallback",
                r"ê³ ê¸‰\s*ì²˜ë¦¬\s*ì‹œìŠ¤í…œ",
                r"SELF-RAG\s*ì²˜ë¦¬",
                r"Tools?\s*ì •ë³´",
                r"í•˜ì´ë¸Œë¦¬ë“œ\s*ì²˜ë¦¬",
                
                # ì‹œìŠ¤í…œ ìƒíƒœ ì •ë³´
                r"âš ï¸\s*\*?ê³ ê¸‰\s*ì²˜ë¦¬.*?\*?",
                r"ì¼ì‹œ\s*ë¶ˆê°€.*?í™œìš©.*?",
                
                # ê³¼ë„í•œ ë©”íƒ€ì •ë³´
                r"ê²€ìƒ‰ëœ\s*\d+ê°œ\s*ë¬¸ì„œ",
                r"ê´€ë ¨\s*ë¬¸ì„œ.*?ê°œ",
                r"ë¬¸ì„œ.*?ë¶„ì„.*?ê²°ê³¼"
            ]
            
            for pattern in technical_patterns:
                text = re.sub(pattern, "", text, flags=re.IGNORECASE)
                
            # ë¹ˆ ì¤„ê³¼ ê³µë°± ì •ë¦¬
            text = re.sub(r'\n\s*\n+', '\n\n', text)
            text = re.sub(r'\s+', ' ', text)
            
        else:
            # ğŸ” ë³µì¡í•œ ì§ˆë¬¸: ê¸°ìˆ ì  ì •ë³´ë¥¼ ê°„ì†Œí™”í•˜ì—¬ ìœ ì§€
            # ìƒì„¸í•œ ì¶œì²˜ ì •ë³´ â†’ ê°„ë‹¨í•œ ì¶œì²˜ ì •ë³´ë¡œ ë³€í™˜
            text = re.sub(
                r"\*(\d+)ê°œì˜?\s*(?:ë²¡í„°\s*ê²€ìƒ‰\s*ê²°ê³¼|ê´€ë ¨\s*ë°ì´í„°).*?ì¢…í•©í•œ.*?\*", 
                r"(\1ê°œ ë°ì´í„° ê¸°ë°˜)", 
                text, flags=re.IGNORECASE
            )
            
            # ê¸°ìˆ ì  ìš©ì–´ ê°„ì†Œí™”
            technical_replacements = {
                r"ê°•í™”ëœ\s*fallbackìœ¼ë¡œ": "ë¶„ì„ì„ í†µí•´",
                r"ê³ ê¸‰\s*ì²˜ë¦¬\s*ì‹œìŠ¤í…œ\s*ì¼ì‹œ\s*ë¶ˆê°€": "ì‹œìŠ¤í…œ ì œì•½ìœ¼ë¡œ",
                r"ë²¡í„°\s*ê²€ìƒ‰\s*ê²°ê³¼ì™€\s*ë„êµ¬\s*ì •ë³´": "ê²€ìƒ‰ ì •ë³´",
                r"SELF-RAG\s*ì²˜ë¦¬": "ê³ ê¸‰ ë¶„ì„",
                r"í•˜ì´ë¸Œë¦¬ë“œ\s*ê°•í™”\s*ì²˜ë¦¬": "í†µí•© ë¶„ì„"
            }
            
            for pattern, replacement in technical_replacements.items():
                text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
        
        return text.strip()
    
    def _final_formatting(self, text: str) -> str:
        """ìµœì¢… í¬ë§·íŒ…"""
        import re
        
        # ê³¼ë„í•œ ê³µë°± ì œê±°
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'\n\s*\n', '\n\n', text)
        
        # ë¬¸ì¥ ë ì •ë¦¬
        text = re.sub(r'\s*\.\s*$', '.', text)
        
        # ì‹œì‘/ë ê³µë°± ì œê±°
        text = text.strip()
        
        return text
    
    def _is_simple_question_type(self, question: str) -> bool:
        """ì§ˆë¬¸ì´ ê°„ë‹¨í•œ ìœ í˜•ì¸ì§€ íŒë‹¨"""
        question_lower = question.lower()
        
        # ê°„ë‹¨í•œ ì§ˆë¬¸ íŒ¨í„´
        simple_patterns = [
            # ë‚ ì§œ/ì‹œê°„ ê´€ë ¨
            ['ì˜¤ëŠ˜', 'ëª‡ì›”'], ['ì˜¤ëŠ˜', 'ëª‡ì¼'], ['í˜„ì¬', 'ë‚ ì§œ'], ['ì§€ê¸ˆ', 'ëª‡ì›”'],
            
            # ë‹¨ìˆœ í†µê³„
            ['ì´', 'ì¬ê³ ëŸ‰'], ['ì „ì²´', 'ì¬ê³ '], ['ì´ì¬ê³ '], 
            ['ì´', 'ì…ê³ ëŸ‰'], ['ì´', 'ì¶œê³ ëŸ‰'],
            
            # ë‹¨ìˆœ ìˆ˜ì¹˜ ì§ˆë¬¸
            ['ì–¼ë§ˆ', 'ê°œ'], ['ëª‡', 'ê°œ'], ['ìˆ˜ëŸ‰']
        ]
        
        # ë³µì¡í•œ ì§ˆë¬¸ íŒ¨í„´ (ë¶„ì„, ë¹„êµ, ìƒì„¸ ì •ë³´)
        complex_patterns = [
            'ë¹„ì •ìƒì ', 'ë¶„ì„', 'ë¹„êµ', 'íŠ¸ë Œë“œ', 'íŒ¨í„´', 
            'ìƒìœ„', 'í•˜ìœ„', 'ìˆœìœ„', 'ë§ì€', 'ì ì€',
            'ì–´ë–¤', 'ë¬´ì—‡', 'ëˆ„ê°€', 'ì–´ë””', 'ì™œ',
            'ìƒíƒœ', 'í˜„í™©', 'ì •ë³´', 'ìƒí™©'
        ]
        
        # ê°„ë‹¨í•œ íŒ¨í„´ ì²´í¬
        for pattern_group in simple_patterns:
            if isinstance(pattern_group, list):
                if all(word in question_lower for word in pattern_group):
                    return True
            else:
                if pattern_group in question_lower:
                    return True
        
        # ë³µì¡í•œ íŒ¨í„´ì´ ìˆìœ¼ë©´ ë³µì¡í•œ ì§ˆë¬¸
        if any(word in question_lower for word in complex_patterns):
            return False
        
        # ê¸°ë³¸ì ìœ¼ë¡œ ê°„ë‹¨í•œ ì§ˆë¬¸ìœ¼ë¡œ ë¶„ë¥˜ (ì§§ì€ ì§ˆë¬¸)
        return len(question) < 20
    
    def _create_lightweight_prompt(self, question: str, tools_context: Dict[str, Any]) -> str:
        """ğŸ“ ì§ˆë¬¸ ìœ í˜•ë³„ ê²½ëŸ‰ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        question_lower = question.lower()
        
        # ğŸ• ë‚ ì§œ/ì‹œê°„ ì§ˆë¬¸ìš© ì´ˆê²½ëŸ‰ í”„ë¡¬í”„íŠ¸
        if any(word in question_lower for word in ['ì˜¤ëŠ˜', 'ëª‡ì›”', 'ëª‡ì¼', 'í˜„ì¬', 'ë‚ ì§œ']):
            current_date = self.current_datetime.strftime('%Yë…„ %mì›” %dì¼')
            return f"""ì§ˆë¬¸: {question}
ë‹µë³€: ì˜¤ëŠ˜ì€ {current_date}ì…ë‹ˆë‹¤."""
        
        # ğŸ“Š ë‹¨ìˆœ í†µê³„ ì§ˆë¬¸ìš© ê²½ëŸ‰ í”„ë¡¬í”„íŠ¸
        if any(word in question_lower for word in ['ì´', 'ì „ì²´', 'í•©ê³„']) and any(word in question_lower for word in ['ì¬ê³ ', 'ì…ê³ ', 'ì¶œê³ ']):
            stats_info = tools_context.get("stats_info", "")
            return f"""ì§ˆë¬¸: {question}
ë°ì´í„°: {stats_info}

ê°„ë‹¨íˆ ë‹µë³€í•˜ì„¸ìš”. ì˜ˆ: "ì´ ì¬ê³ ëŸ‰ì€ 1,234ê°œì…ë‹ˆë‹¤."
ë‹µë³€:"""
        
        # ğŸ” ê¸°ë³¸ ê²½ëŸ‰ í”„ë¡¬í”„íŠ¸ (ë³µì¡í•œ ì§ˆë¬¸ìš©)
        context_summary = self._format_tools_context_minimal(tools_context)
        return f"""ì§ˆë¬¸: {question}
ì •ë³´: {context_summary}

ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”. ë¶ˆí•„ìš”í•œ ì„¤ëª…ì€ ìƒëµí•˜ì„¸ìš”.
ë‹µë³€:"""
    
    def _format_tools_context_minimal(self, tools_context: Dict[str, Any]) -> str:
        """Tools ì»¨í…ìŠ¤íŠ¸ë¥¼ ìµœì†Œí•œìœ¼ë¡œ í¬ë§·íŒ…"""
        essential_parts = []
        
        # ë‚ ì§œ ì •ë³´ (í•„ìˆ˜)
        current_date = self.current_datetime.strftime('%Yë…„ %mì›” %dì¼')
        essential_parts.append(f"í˜„ì¬: {current_date}")
        essential_parts.append("ë°ì´í„°: 2025ë…„ 1ì›” 1ì¼~7ì¼ (ê³¼ê±°)")
        
        # í†µê³„ ì •ë³´ (ìˆìœ¼ë©´ í¬í•¨)
        if "stats_info" in tools_context:
            stats = tools_context["stats_info"]
            # í•µì‹¬ ìˆ«ìë§Œ ì¶”ì¶œ
            import re
            numbers = re.findall(r'(\d{1,3}(?:,\d{3})*)\s*ê°œ', stats)
            if numbers:
                essential_parts.append(f"ì£¼ìš” ìˆ˜ì¹˜: {', '.join(numbers[:3])}ê°œ")
        
        return " | ".join(essential_parts)
    
    def _should_show_source_info(self, question: str, found_count: int) -> bool:
        """ğŸ“‹ ì¶œì²˜ ì •ë³´ í‘œì‹œ ì—¬ë¶€ ê²°ì •"""
        question_lower = question.lower()
        
        # ğŸš« ê°„ë‹¨í•œ ì§ˆë¬¸ì—ì„œëŠ” ì¶œì²˜ ì •ë³´ ìˆ¨ê¹€
        simple_question_indicators = [
            # ë‚ ì§œ/ì‹œê°„ ì§ˆë¬¸
            'ì˜¤ëŠ˜', 'ëª‡ì›”', 'ëª‡ì¼', 'í˜„ì¬', 'ë‚ ì§œ', 'ì‹œê°„',
            
            # ë‹¨ìˆœ í†µê³„
            'ì´ì¬ê³ ', 'ì „ì²´ì¬ê³ ', 'ì´ì…ê³ ', 'ì´ì¶œê³ ',
            
            # ë‹¨ìˆœ ìˆ˜ì¹˜
            'ì–¼ë§ˆ', 'ëª‡ê°œ', 'ìˆ˜ëŸ‰'
        ]
        
        # ê°„ë‹¨í•œ ì§ˆë¬¸ìœ¼ë¡œ íŒë‹¨ë˜ë©´ ì¶œì²˜ ì •ë³´ ìˆ¨ê¹€
        if any(indicator in question_lower for indicator in simple_question_indicators):
            return False
        
        # ğŸ” ë³µì¡í•œ ë¶„ì„ ì§ˆë¬¸ì—ì„œëŠ” ì¶œì²˜ ì •ë³´ í‘œì‹œ
        complex_question_indicators = [
            # ë¶„ì„ í‚¤ì›Œë“œ
            'ë¶„ì„', 'ë¹„êµ', 'íŠ¸ë Œë“œ', 'íŒ¨í„´', 'ì˜ˆì¸¡',
            'ìƒìœ„', 'í•˜ìœ„', 'ìˆœìœ„', 'ë­í‚¹',
            
            # ìƒì„¸ ì •ë³´ ìš”êµ¬
            'ì–´ë–¤', 'ë¬´ì—‡', 'ëˆ„ê°€', 'ì–´ë””', 'ì™œ',
            'ìƒíƒœ', 'í˜„í™©', 'ì •ë³´', 'ìƒí™©',
            
            # ë¹„êµ/ì¡°ê±´
            'ë§ì€', 'ì ì€', 'ë¹„ì •ìƒì ', 'íŠ¹ë³„í•œ'
        ]
        
        has_complex_indicators = any(indicator in question_lower for indicator in complex_question_indicators)
        
        # ë³µì¡í•œ ì§ˆë¬¸ì´ê³  ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì¶œì²˜ ì •ë³´ í‘œì‹œ
        if has_complex_indicators and found_count >= 5:
            return True
        
        # ë§¤ìš° ë§ì€ ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ ê²½ìš° (10ê°œ ì´ìƒ) ì¶œì²˜ ì •ë³´ í‘œì‹œ
        if found_count >= 10:
            return True
        
        # ê¸°ë³¸ì ìœ¼ë¡œ ì¶œì²˜ ì •ë³´ ìˆ¨ê¹€ (ê¹”ë”í•œ ë‹µë³€)
        return False
    
    def _apply_response_template(self, text: str, question: str, is_complex: bool) -> str:
        """ğŸ“‹ ì§ˆë¬¸ ìœ í˜•ë³„ ì‘ë‹µ í…œí”Œë¦¿ ì ìš©"""
        if not text or not text.strip():
            return text
        
        question_lower = question.lower()
        
        # ë³µì¡í•œ ì§ˆë¬¸ì— ëŒ€í•œ í…œí”Œë¦¿
        if is_complex:
            return self._apply_complex_template(text, question_lower)
        else:
            # ê°„ë‹¨í•œ ì§ˆë¬¸ì€ ì´ë¯¸ _simplify_for_basic_questionsì—ì„œ ì²˜ë¦¬ë¨
            return text
    
    def _apply_complex_template(self, text: str, question_lower: str) -> str:
        """ğŸ” ë³µì¡í•œ ì§ˆë¬¸ì— ëŒ€í•œ í‘œì¤€ í…œí”Œë¦¿"""
        import re
        
        # ë¶„ì„ ê²°ê³¼ í˜•ì‹ í‘œì¤€í™”
        if any(word in question_lower for word in ['ë¶„ì„', 'ë¹„êµ', 'íŠ¸ë Œë“œ']):
            # ë¶„ì„ ê²°ê³¼ ë¬¸ì¥ ì •ë¦¬
            text = re.sub(r'^ë°ì´í„°ì—\s*ë”°ë¥´ë©´[,\s]*', '', text)
            text = re.sub(r'^ê²€ìƒ‰.*?ê²°ê³¼[,\s]*', '', text)
            
            # ë¶„ì„ ê²°ê³¼ë¡œ ì‹œì‘í•˜ë„ë¡ ì¡°ì •
            if not text.startswith('ë¶„ì„'):
                text = f"ë¶„ì„ ê²°ê³¼, {text}"
        
        # ìƒìœ„/ìˆœìœ„ ì§ˆë¬¸ í˜•ì‹ í‘œì¤€í™”
        elif any(word in question_lower for word in ['ìƒìœ„', 'ìˆœìœ„', 'ë­í‚¹', 'ë§ì´', 'ì ê²Œ']):
            # ìˆœìœ„ ì •ë³´ ì •ë¦¬
            text = re.sub(r'ê°€ì¥\s*ë§ì´.*?ìƒí’ˆì€', '1ìœ„ëŠ”', text)
            text = re.sub(r'(\d+)ë²ˆì§¸.*?ë§ì´', r'\1ìœ„ë¡œ', text)
        
        # ìƒíƒœ/í˜„í™© ì§ˆë¬¸ í˜•ì‹ í‘œì¤€í™”
        elif any(word in question_lower for word in ['ìƒíƒœ', 'í˜„í™©', 'ì–´ë–¤']):
            # í˜„í™© ë³´ê³  í˜•ì‹ìœ¼ë¡œ ì¡°ì •
            if not any(text.startswith(prefix) for prefix in ['í˜„ì¬', 'ìƒíƒœ', 'í˜„í™©']):
                text = f"í˜„ì¬ ìƒí™©: {text}"
        
        # ê³¼ë„í•œ ì ‘ì†ì‚¬/ë¶€ì‚¬ ì œê±°
        text = re.sub(r'ê·¸ëŸ¬ë‚˜\s*ë˜í•œ\s*', '', text)
        text = re.sub(r'ë”°ë¼ì„œ\s*ê·¸ë¦¬ê³ \s*', '', text)
        text = re.sub(r'ë˜í•œ\s*ë”ë¶ˆì–´\s*', '', text)
        
        return text.strip()
    
    # ==================== ML Tools êµ¬í˜„ ====================
    
    def _tool_ml_prediction(self, query: str) -> str:
        """ğŸ”® ìˆ˜ìš” ì˜ˆì¸¡ ë„êµ¬"""
        if not self.ml_models.get('demand_predictor'):
            return "ğŸ”® ìˆ˜ìš” ì˜ˆì¸¡ ëª¨ë¸ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ëª¨ë¸ì„ ë¨¼ì € í•™ìŠµí•´ì£¼ì„¸ìš”."
        
        try:
            self.logger.info(f"ğŸ”® [ML_PREDICTION] ìˆ˜ìš” ì˜ˆì¸¡ ì‹œì‘: {query[:50]}...")
            
            # TODO: ì‹¤ì œ ML ëª¨ë¸ í˜¸ì¶œ ë¡œì§ êµ¬í˜„
            # í˜„ì¬ëŠ” ê°œë°œ ì¤‘ ë©”ì‹œì§€ ë°˜í™˜
            return f"""ğŸ”® **ìˆ˜ìš” ì˜ˆì¸¡ ê²°ê³¼:**

ğŸ“Š **ì§ˆë¬¸:** {query}

ğŸ“ˆ **ì˜ˆì¸¡ ë¶„ì„:**
â€¢ í˜„ì¬ ìˆ˜ìš” ì˜ˆì¸¡ ëª¨ë¸ì´ ê°œë°œ ì¤‘ì…ë‹ˆë‹¤
â€¢ í–¥í›„ ì—…ë°ì´íŠ¸ì—ì„œ ì‹¤ì œ ì˜ˆì¸¡ ê¸°ëŠ¥ì„ ì œê³µí•  ì˜ˆì •ì…ë‹ˆë‹¤

âš ï¸ **í˜„ì¬ ìƒíƒœ:** ê°œë°œ ì¤‘ì¸ ê¸°ëŠ¥ì…ë‹ˆë‹¤. ê¸°ë³¸ ë°ì´í„° ë¶„ì„ì€ ë‹¤ë¥¸ ì§ˆë¬¸ìœ¼ë¡œ ë¬¸ì˜í•´ì£¼ì„¸ìš”."""
            
        except Exception as e:
            self.logger.error(f"âŒ [ML_PREDICTION] ìˆ˜ìš” ì˜ˆì¸¡ ì˜¤ë¥˜: {e}")
            return f"ğŸ”® ìˆ˜ìš” ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
    
    def _tool_ml_anomaly(self, query: str) -> str:
        """âš ï¸ ì´ìƒ íƒì§€ ë„êµ¬"""
        if not self.ml_models.get('anomaly_detector'):
            return "âš ï¸ ì´ìƒ íƒì§€ ëª¨ë¸ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ëª¨ë¸ì„ ë¨¼ì € í•™ìŠµí•´ì£¼ì„¸ìš”."
        
        try:
            self.logger.info(f"âš ï¸ [ML_ANOMALY] ì´ìƒ íƒì§€ ì‹œì‘: {query[:50]}...")
            
            # TODO: ì‹¤ì œ ML ëª¨ë¸ í˜¸ì¶œ ë¡œì§ êµ¬í˜„
            # í˜„ì¬ëŠ” ê°œë°œ ì¤‘ ë©”ì‹œì§€ ë°˜í™˜
            return f"""âš ï¸ **ì´ìƒ íƒì§€ ê²°ê³¼:**

ğŸ“Š **ì§ˆë¬¸:** {query}

ğŸ” **ì´ìƒ íŒ¨í„´ ë¶„ì„:**
â€¢ í˜„ì¬ ì´ìƒ íƒì§€ ëª¨ë¸ì´ ê°œë°œ ì¤‘ì…ë‹ˆë‹¤
â€¢ í–¥í›„ ì—…ë°ì´íŠ¸ì—ì„œ ì‹¤ì œ ì´ìƒ íƒì§€ ê¸°ëŠ¥ì„ ì œê³µí•  ì˜ˆì •ì…ë‹ˆë‹¤

âš ï¸ **í˜„ì¬ ìƒíƒœ:** ê°œë°œ ì¤‘ì¸ ê¸°ëŠ¥ì…ë‹ˆë‹¤. ê¸°ë³¸ ë°ì´í„° ë¶„ì„ì€ ë‹¤ë¥¸ ì§ˆë¬¸ìœ¼ë¡œ ë¬¸ì˜í•´ì£¼ì„¸ìš”."""
            
        except Exception as e:
            self.logger.error(f"âŒ [ML_ANOMALY] ì´ìƒ íƒì§€ ì˜¤ë¥˜: {e}")
            return f"âš ï¸ ì´ìƒ íƒì§€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
    
    def _tool_ml_clustering(self, query: str) -> str:
        """ğŸ¯ í´ëŸ¬ìŠ¤í„°ë§ ë„êµ¬"""
        if not self.ml_models.get('product_clusterer'):
            return "ğŸ¯ í´ëŸ¬ìŠ¤í„°ë§ ëª¨ë¸ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ëª¨ë¸ì„ ë¨¼ì € í•™ìŠµí•´ì£¼ì„¸ìš”."
        
        try:
            self.logger.info(f"ğŸ¯ [ML_CLUSTERING] í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘: {query[:50]}...")
            
            # TODO: ì‹¤ì œ ML ëª¨ë¸ í˜¸ì¶œ ë¡œì§ êµ¬í˜„
            # í˜„ì¬ëŠ” ê°œë°œ ì¤‘ ë©”ì‹œì§€ ë°˜í™˜
            return f"""ğŸ¯ **í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼:**

ğŸ“Š **ì§ˆë¬¸:** {query}

ğŸ”— **ìƒí’ˆ ë¶„ë¥˜ ë¶„ì„:**
â€¢ í˜„ì¬ í´ëŸ¬ìŠ¤í„°ë§ ëª¨ë¸ì´ ê°œë°œ ì¤‘ì…ë‹ˆë‹¤
â€¢ í–¥í›„ ì—…ë°ì´íŠ¸ì—ì„œ ì‹¤ì œ ìƒí’ˆ ê·¸ë£¹í™” ê¸°ëŠ¥ì„ ì œê³µí•  ì˜ˆì •ì…ë‹ˆë‹¤

âš ï¸ **í˜„ì¬ ìƒíƒœ:** ê°œë°œ ì¤‘ì¸ ê¸°ëŠ¥ì…ë‹ˆë‹¤. ê¸°ë³¸ ë°ì´í„° ë¶„ì„ì€ ë‹¤ë¥¸ ì§ˆë¬¸ìœ¼ë¡œ ë¬¸ì˜í•´ì£¼ì„¸ìš”."""
            
        except Exception as e:
            self.logger.error(f"âŒ [ML_CLUSTERING] í´ëŸ¬ìŠ¤í„°ë§ ì˜¤ë¥˜: {e}")
            return f"ğŸ¯ í´ëŸ¬ìŠ¤í„°ë§ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"