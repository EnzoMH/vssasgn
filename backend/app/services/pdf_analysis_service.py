import os
import asyncio
import logging
from typing import Dict, List, Optional, Any
from pathlib import Path

try:
    import fitz  # PyMuPDF
except ImportError:
    fitz = None

from .ai_service import WarehouseAI

class PDFAnalysisService:
    """PDF ë¬¸ì„œ ë¶„ì„ ë° AI ì²˜ë¦¬ ì„œë¹„ìŠ¤"""
    
    def __init__(self, logger=None):
        self.logger = logger or logging.getLogger(__name__)
        self.ai_service = WarehouseAI(logger=self.logger)
        
        if fitz is None:
            self.logger.warning("âš ï¸ PyMuPDFê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install PyMuPDFë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
    
    def extract_text_from_pdf(self, pdf_path: str) -> Dict[str, Any]:
        """PDFì—ì„œ í…ìŠ¤íŠ¸ ë° ì´ë¯¸ì§€ ì •ë³´ ì¶”ì¶œ"""
        if fitz is None:
            raise ImportError("PyMuPDFê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install PyMuPDFë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        
        try:
            pdf_document = fitz.open(pdf_path)
            
            extracted_data = {
                "metadata": pdf_document.metadata,
                "page_count": pdf_document.page_count,
                "pages": [],
                "full_text": "",
                "images": []
            }
            
            full_text_parts = []
            
            for page_num in range(pdf_document.page_count):
                page = pdf_document[page_num]
                
                # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                page_text = page.get_text()
                
                # ì´ë¯¸ì§€ ì •ë³´ ì¶”ì¶œ
                image_list = page.get_images()
                page_images = []
                
                for img_index, img in enumerate(image_list):
                    try:
                        # ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„°ë§Œ ì¶”ì¶œ (ì‹¤ì œ ì´ë¯¸ì§€ëŠ” ë„ˆë¬´ í´ ìˆ˜ ìˆìŒ)
                        xref = img[0]
                        img_dict = pdf_document.extract_image(xref)
                        page_images.append({
                            "image_index": img_index,
                            "width": img_dict.get("width"),
                            "height": img_dict.get("height"),
                            "ext": img_dict.get("ext"),
                            "size": len(img_dict.get("image", b""))
                        })
                    except Exception as e:
                        self.logger.warning(f"ì´ë¯¸ì§€ {img_index} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                
                page_info = {
                    "page_number": page_num + 1,
                    "text": page_text,
                    "text_length": len(page_text),
                    "images": page_images,
                    "image_count": len(page_images)
                }
                
                extracted_data["pages"].append(page_info)
                full_text_parts.append(page_text)
            
            # ì „ì²´ í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°
            extracted_data["full_text"] = "\n\n".join(full_text_parts)
            
            pdf_document.close()
            
            self.logger.info(f"âœ… PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ: {pdf_path}")
            self.logger.info(f"ğŸ“„ ì´ {extracted_data['page_count']}í˜ì´ì§€, "
                           f"í…ìŠ¤íŠ¸ ê¸¸ì´: {len(extracted_data['full_text'])}ì")
            
            return extracted_data
            
        except Exception as e:
            self.logger.error(f"âŒ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            raise
    
    async def analyze_document_with_ai(self, extracted_data: Dict[str, Any], 
                                     analysis_prompt: str = None) -> Dict[str, Any]:
        """AIë¥¼ í†µí•œ ë¬¸ì„œ ë‚´ìš© ë¶„ì„ ë° êµ¬ì¡°í™”"""
        
        if not analysis_prompt:
            analysis_prompt = """
ë‹¹ì‹ ì€ ë¬¸ì„œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ PDF ë¬¸ì„œì˜ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì´ êµ¬ì¡°í™”ëœ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:

**ë¶„ì„ ê²°ê³¼ë¥¼ ë‹¤ìŒ JSON í˜•íƒœë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”:**

```json
{
    "document_summary": {
        "title": "ë¬¸ì„œ ì œëª©",
        "type": "ë¬¸ì„œ ìœ í˜• (ê³¼ì œ, ë§¤ë‰´ì–¼, ë³´ê³ ì„œ ë“±)",
        "main_purpose": "ë¬¸ì„œì˜ ì£¼ìš” ëª©ì ",
        "key_sections": ["ì£¼ìš” ì„¹ì…˜ë“¤ì˜ ë¦¬ìŠ¤íŠ¸"]
    },
    "detailed_structure": {
        "sections": [
            {
                "section_name": "ì„¹ì…˜ëª…",
                "content_summary": "í•´ë‹¹ ì„¹ì…˜ì˜ ì£¼ìš” ë‚´ìš© ìš”ì•½",
                "key_points": ["í•µì‹¬ í¬ì¸íŠ¸ë“¤"],
                "requirements": ["ìš”êµ¬ì‚¬í•­ë“¤ (ìˆë‹¤ë©´)"],
                "deliverables": ["ì‚°ì¶œë¬¼ë“¤ (ìˆë‹¤ë©´)"]
            }
        ]
    },
    "tasks_and_requirements": {
        "step1": {
            "title": "Step 1 ì œëª©",
            "description": "ìƒì„¸ ì„¤ëª…",
            "requirements": ["ìš”êµ¬ì‚¬í•­ë“¤"],
            "deliverables": ["ì‚°ì¶œë¬¼ë“¤"],
            "technologies": ["ì‚¬ìš© ê¸°ìˆ /ë„êµ¬ë“¤"]
        },
        "step2": {
            "title": "Step 2 ì œëª©", 
            "description": "ìƒì„¸ ì„¤ëª…",
            "requirements": ["ìš”êµ¬ì‚¬í•­ë“¤"],
            "deliverables": ["ì‚°ì¶œë¬¼ë“¤"],
            "technologies": ["ì‚¬ìš© ê¸°ìˆ /ë„êµ¬ë“¤"]
        }
    },
    "technical_specifications": {
        "programming_languages": ["ì‚¬ìš©í•  í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë“¤"],
        "frameworks": ["í”„ë ˆì„ì›Œí¬ë“¤"],
        "databases": ["ë°ì´í„°ë² ì´ìŠ¤"],
        "apis": ["APIë“¤"],
        "other_tools": ["ê¸°íƒ€ ë„êµ¬ë“¤"]
    },
    "evaluation_criteria": {
        "criteria": ["í‰ê°€ ê¸°ì¤€ë“¤"],
        "weights": ["ê°€ì¤‘ì¹˜ (ëª…ì‹œë˜ì–´ ìˆë‹¤ë©´)"]
    },
    "timeline_and_deadlines": {
        "overall_deadline": "ì „ì²´ ë§ˆê°ì¼",
        "milestones": ["ì¤‘ê°„ ë§ˆì¼ìŠ¤í†¤ë“¤"]
    }
}
```

**ë¬¸ì„œ ë‚´ìš©:**
"""
        
        try:
            # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸´ ê²½ìš° ë¶„í•  ì²˜ë¦¬
            full_text = extracted_data.get("full_text", "")
            
            if len(full_text) > 50000:  # 50,000ì ì´ìƒì¸ ê²½ìš°
                self.logger.info("ğŸ“„ ë¬¸ì„œê°€ ê¸¸ì–´ì„œ í˜ì´ì§€ë³„ë¡œ ë¶„í•  ë¶„ì„ì„ ì§„í–‰í•©ë‹ˆë‹¤.")
                
                # í˜ì´ì§€ë³„ ë¶„ì„ í›„ í†µí•©
                page_analyses = []
                for page_info in extracted_data.get("pages", []):
                    if len(page_info["text"].strip()) > 100:  # ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ê°€ ìˆëŠ” í˜ì´ì§€ë§Œ
                        page_prompt = f"{analysis_prompt}\n\ní˜ì´ì§€ {page_info['page_number']}:\n{page_info['text']}"
                        page_result = await self.ai_service.answer_query(
                            "ì´ í˜ì´ì§€ì˜ ë‚´ìš©ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.", 
                            {"page_text": page_info['text']}
                        )
                        page_analyses.append({
                            "page_number": page_info['page_number'],
                            "analysis": page_result
                        })
                
                # ì „ì²´ í†µí•© ë¶„ì„
                integration_prompt = f"""
ë‹¤ìŒì€ ê° í˜ì´ì§€ë³„ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•©í•˜ì—¬ ì „ì²´ ë¬¸ì„œì˜ êµ¬ì¡°í™”ëœ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”:

{chr(10).join([f"í˜ì´ì§€ {pa['page_number']}: {pa['analysis']}" for pa in page_analyses])}

ìœ„ì˜ í˜ì´ì§€ë³„ ë¶„ì„ì„ ë°”íƒ•ìœ¼ë¡œ ì „ì²´ ë¬¸ì„œì˜ í†µí•©ëœ êµ¬ì¡°í™” ë¶„ì„ì„ JSON í˜•íƒœë¡œ ì œê³µí•´ì£¼ì„¸ìš”.
"""
                
                final_analysis = await self.ai_service.answer_query(
                    integration_prompt,
                    {"page_analyses": page_analyses}
                )
                
                return {
                    "analysis_method": "multi_page_analysis",
                    "page_count": len(page_analyses),
                    "page_analyses": page_analyses,
                    "integrated_analysis": final_analysis,
                    "document_info": {
                        "total_pages": extracted_data.get("page_count", 0),
                        "total_text_length": len(full_text),
                        "has_images": len(extracted_data.get("images", [])) > 0
                    }
                }
            
            else:
                # ì „ì²´ ë¬¸ì„œ í•œë²ˆì— ë¶„ì„
                complete_prompt = f"{analysis_prompt}\n\n{full_text}"
                
                analysis_result = await self.ai_service.answer_query(
                    "ì£¼ì–´ì§„ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì—¬ êµ¬ì¡°í™”ëœ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.",
                    {"full_document": full_text}
                )
                
                return {
                    "analysis_method": "full_document_analysis", 
                    "analysis_result": analysis_result,
                    "document_info": {
                        "total_pages": extracted_data.get("page_count", 0),
                        "total_text_length": len(full_text),
                        "has_images": len(extracted_data.get("images", [])) > 0
                    }
                }
                
        except Exception as e:
            self.logger.error(f"âŒ AI ë¬¸ì„œ ë¶„ì„ ì‹¤íŒ¨: {e}")
            raise
    
    async def process_pdf_completely(self, pdf_path: str, 
                                   custom_prompt: str = None) -> Dict[str, Any]:
        """PDF ì „ì²´ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (í…ìŠ¤íŠ¸ ì¶”ì¶œ + AI ë¶„ì„)"""
        
        try:
            self.logger.info(f"ğŸš€ PDF ì²˜ë¦¬ ì‹œì‘: {pdf_path}")
            
            # 1. PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            extracted_data = self.extract_text_from_pdf(pdf_path)
            
            # 2. AI ë¶„ì„
            analysis_result = await self.analyze_document_with_ai(
                extracted_data, 
                analysis_prompt=custom_prompt
            )
            
            # 3. ê²°ê³¼ í†µí•©
            complete_result = {
                "pdf_info": {
                    "file_path": pdf_path,
                    "file_name": Path(pdf_path).name,
                    "metadata": extracted_data.get("metadata", {}),
                    "extraction_success": True
                },
                "extracted_data": extracted_data,
                "ai_analysis": analysis_result,
                "processing_timestamp": asyncio.get_event_loop().time()
            }
            
            self.logger.info("âœ… PDF ì²˜ë¦¬ ì™„ë£Œ")
            return complete_result
            
        except Exception as e:
            self.logger.error(f"âŒ PDF ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                "pdf_info": {
                    "file_path": pdf_path,
                    "file_name": Path(pdf_path).name,
                    "extraction_success": False,
                    "error": str(e)
                },
                "extracted_data": None,
                "ai_analysis": None,
                "processing_timestamp": asyncio.get_event_loop().time()
            }

# í¸ì˜ í•¨ìˆ˜ë“¤
async def analyze_vss_assignment(pdf_path: str = None) -> Dict[str, Any]:
    """VSS ê³¼ì œ PDF ë¶„ì„ì„ ìœ„í•œ íŠ¹í™”ëœ í•¨ìˆ˜"""
    
    if pdf_path is None:
        pdf_path = r"C:\Users\MyoengHo Shin\pjt\vss_asgnM\VSS_ì…ì‚¬í…ŒìŠ¤íŠ¸ê³¼ì œ_AI.pdf"
    
    vss_prompt = """
ë‹¹ì‹ ì€ VSS ì…ì‚¬ í…ŒìŠ¤íŠ¸ ê³¼ì œë¥¼ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì´ ê³¼ì œëŠ” AI/ML ê´€ë ¨ ê¸°ìˆ  ê³¼ì œë¡œ ë³´ì´ë©°, ë‹¨ê³„ë³„ë¡œ êµ¬ì„±ë˜ì–´ ìˆì„ ê²ƒì…ë‹ˆë‹¤.

ë‹¤ìŒ í˜•íƒœë¡œ ë¶„ì„ ê²°ê³¼ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:

```json
{
    "assignment_overview": {
        "company": "VSS",
        "position": "AI/ML ê´€ë ¨ ì§ë¬´",
        "assignment_type": "ì…ì‚¬ í…ŒìŠ¤íŠ¸ ê³¼ì œ",
        "main_objective": "ê³¼ì œì˜ ì£¼ìš” ëª©í‘œ"
    },
    "steps_breakdown": {
        "step1": {
            "title": "Step 1 ì œëª©",
            "objective": "ëª©í‘œ",
            "requirements": ["ìš”êµ¬ì‚¬í•­ë“¤"],
            "deliverables": ["ì œì¶œë¬¼ë“¤"],
            "technologies": ["í•„ìš” ê¸°ìˆ ë“¤"],
            "difficulty": "ë‚œì´ë„ (1-5)",
            "estimated_time": "ì˜ˆìƒ ì†Œìš” ì‹œê°„"
        },
        "step2": {
            "title": "Step 2 ì œëª©",
            "objective": "ëª©í‘œ", 
            "requirements": ["ìš”êµ¬ì‚¬í•­ë“¤"],
            "deliverables": ["ì œì¶œë¬¼ë“¤"],
            "technologies": ["í•„ìš” ê¸°ìˆ ë“¤"],
            "difficulty": "ë‚œì´ë„ (1-5)",
            "estimated_time": "ì˜ˆìƒ ì†Œìš” ì‹œê°„"
        },
        "step3": {
            "title": "Step 3 ì œëª© (DWG/CAD ê´€ë ¨)",
            "objective": "ëª©í‘œ",
            "requirements": ["ìš”êµ¬ì‚¬í•­ë“¤"], 
            "deliverables": ["ì œì¶œë¬¼ë“¤"],
            "technologies": ["í•„ìš” ê¸°ìˆ ë“¤"],
            "difficulty": "ë‚œì´ë„ (1-5)",
            "estimated_time": "ì˜ˆìƒ ì†Œìš” ì‹œê°„",
            "feasibility": "í˜„ì¬ ë°ì´í„° ìƒí™©ì—ì„œì˜ ì‹¤í˜„ ê°€ëŠ¥ì„±"
        },
        "step4": {
            "title": "Step 4 ì œëª©",
            "objective": "ëª©í‘œ",
            "requirements": ["ìš”êµ¬ì‚¬í•­ë“¤"],
            "deliverables": ["ì œì¶œë¬¼ë“¤"],
            "dependencies": ["Step 2 ì™„ë£Œ í›„ ì§„í–‰ ê°€ëŠ¥í•œ ë¶€ë¶„ë“¤"],
            "difficulty": "ë‚œì´ë„ (1-5)",
            "estimated_time": "ì˜ˆìƒ ì†Œìš” ì‹œê°„"
        }
    },
    "technical_stack": {
        "programming_languages": ["Python", "ê¸°íƒ€"],
        "ml_frameworks": ["scikit-learn", "TensorFlow", "PyTorch", "ê¸°íƒ€"],
        "data_processing": ["pandas", "numpy", "ê¸°íƒ€"],
        "visualization": ["matplotlib", "plotly", "ê¸°íƒ€"],
        "web_frameworks": ["FastAPI", "Flask", "ê¸°íƒ€"],
        "databases": ["í•„ìš”í•œ DBë“¤"],
        "cad_tools": ["CAD ê´€ë ¨ ë„êµ¬ë“¤ (Step 3)"]
    },
    "current_project_alignment": {
        "data_available": ["í˜„ì¬ ê°€ì§€ê³  ìˆëŠ” ë°ì´í„°ë“¤ (ì…ê³ /ì¶œê³  ë°ì´í„°)"],
        "completed_components": ["ì´ë¯¸ ì™„ë£Œëœ ë¶€ë¶„ë“¤ (ë°ì´í„° ë¶„ì„)"],
        "missing_components": ["ë¶€ì¡±í•œ ë¶€ë¶„ë“¤ (DWG íŒŒì¼ ë“±)"],
        "adaptable_steps": ["í˜„ì¬ ìƒí™©ì— ë§ê²Œ ìˆ˜ì • ê°€ëŠ¥í•œ ë‹¨ê³„ë“¤"]
    },
    "implementation_strategy": {
        "immediate_next_steps": ["ë°”ë¡œ ì‹œì‘í•  ìˆ˜ ìˆëŠ” ì‘ì—…ë“¤"],
        "step_by_step_plan": ["ë‹¨ê³„ë³„ ì‹¤í–‰ ê³„íš"],
        "risk_factors": ["ìœ„í—˜ ìš”ì†Œë“¤ (ë°ì´í„° ë¶€ì¡± ë“±)"],
        "mitigation_strategies": ["ìœ„í—˜ ì™„í™” ë°©ì•ˆë“¤"]
    },
    "evaluation_criteria": {
        "technical_competency": ["ê¸°ìˆ ì  ì—­ëŸ‰ í‰ê°€ ê¸°ì¤€"],
        "code_quality": ["ì½”ë“œ í’ˆì§ˆ ê¸°ì¤€"],
        "documentation": ["ë¬¸ì„œí™” ìš”êµ¬ì‚¬í•­"],
        "presentation": ["ë°œí‘œ/ë°ëª¨ ìš”êµ¬ì‚¬í•­"]
    }
}
```

ë¶„ì„ ì‹œ íŠ¹íˆ ë‹¤ìŒ ì‚¬í•­ë“¤ì„ ì£¼ì˜ê¹Šê²Œ ì‚´í´ë³´ì„¸ìš”:
- ê° Stepë³„ êµ¬ì²´ì ì¸ ìš”êµ¬ì‚¬í•­
- í˜„ì¬ ê°€ì§€ê³  ìˆëŠ” ì°½ê³  ë°ì´í„° (ì…ê³ /ì¶œê³  ë°ì´í„°)ì™€ì˜ ì—°ê´€ì„±
- Step 3ì˜ DWG/CAD íŒŒì¼ ìš”êµ¬ì‚¬í•­ê³¼ ëŒ€ì•ˆì±…
- RAG (Retrieval-Augmented Generation) êµ¬ì¶• ìš”êµ¬ì‚¬í•­
- ML/AI ëª¨ë¸ ê°œë°œ ìš”êµ¬ì‚¬í•­
"""
    
    service = PDFAnalysisService()
    result = await service.process_pdf_completely(pdf_path, vss_prompt)
    
    return result

# ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
if __name__ == "__main__":
    import asyncio
    
    async def main():
        try:
            result = await analyze_vss_assignment()
            
            print("=" * 80)
            print("ğŸ¯ VSS ê³¼ì œ ë¶„ì„ ê²°ê³¼")
            print("=" * 80)
            
            if result["pdf_info"]["extraction_success"]:
                print(f"âœ… PDF ì²˜ë¦¬ ì„±ê³µ: {result['pdf_info']['file_name']}")
                print(f"ğŸ“„ í˜ì´ì§€ ìˆ˜: {result['extracted_data']['page_count']}")
                print(f"ğŸ“ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(result['extracted_data']['full_text'])}ì")
                print("\nğŸ¤– AI ë¶„ì„ ê²°ê³¼:")
                print("-" * 50)
                
                if result["ai_analysis"]["analysis_method"] == "multi_page_analysis":
                    print("ğŸ“‹ í†µí•© ë¶„ì„ ê²°ê³¼:")
                    print(result["ai_analysis"]["integrated_analysis"])
                else:
                    print("ğŸ“‹ ì „ì²´ ë¶„ì„ ê²°ê³¼:")
                    print(result["ai_analysis"]["analysis_result"])
            else:
                print(f"âŒ PDF ì²˜ë¦¬ ì‹¤íŒ¨: {result['pdf_info']['error']}")
                
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    asyncio.run(main())