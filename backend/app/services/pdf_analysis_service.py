import os
import asyncio
import logging
from typing import Dict, List, Optional, Any
from pathlib import Path

try:
    import fitz  # PyMuPDF
except ImportError:
    fitz = None

from .ai_service import WarehouseAI

class PDFAnalysisService:
    """PDF ë¬¸ì„œ ë¶„ì„ ë° AI ì²˜ë¦¬ ì„œë¹„ìŠ¤"""
    
    def __init__(self, logger=None):
        self.logger = logger or logging.getLogger(__name__)
        self.ai_service = WarehouseAI(logger=self.logger)
        
        if fitz is None:
            self.logger.warning("âš ï¸ PyMuPDFê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install PyMuPDFë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
    
    def extract_text_from_pdf(self, pdf_path: str) -> Dict[str, Any]:
        """PDFì—ì„œ í…ìŠ¤íŠ¸ ë° ì´ë¯¸ì§€ ì •ë³´ ì¶”ì¶œ"""
        if fitz is None:
            raise ImportError("PyMuPDFê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install PyMuPDFë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        
        try:
            pdf_document = fitz.open(pdf_path)
            
            extracted_data = {
                "metadata": pdf_document.metadata,
                "page_count": pdf_document.page_count,
                "pages": [],
                "full_text": "",
                "images": []
            }
            
            full_text_parts = []
            
            for page_num in range(pdf_document.page_count):
                page = pdf_document[page_num]
                
                # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                page_text = page.get_text()
                
                # ì´ë¯¸ì§€ ì •ë³´ ì¶”ì¶œ
                image_list = page.get_images()
                page_images = []
                
                for img_index, img in enumerate(image_list):
                    try:
                        # ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„°ë§Œ ì¶”ì¶œ (ì‹¤ì œ ì´ë¯¸ì§€ëŠ” ë„ˆë¬´ í´ ìˆ˜ ìˆìŒ)
                        xref = img[0]
                        img_dict = pdf_document.extract_image(xref)
                        page_images.append({
                            "image_index": img_index,
                            "width": img_dict.get("width"),
                            "height": img_dict.get("height"),
                            "ext": img_dict.get("ext"),
                            "size": len(img_dict.get("image", b""))
                        })
                    except Exception as e:
                        self.logger.warning(f"ì´ë¯¸ì§€ {img_index} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                
                page_info = {
                    "page_number": page_num + 1,
                    "text": page_text,
                    "text_length": len(page_text),
                    "images": page_images,
                    "image_count": len(page_images)
                }
                
                extracted_data["pages"].append(page_info)
                full_text_parts.append(page_text)
            
            # ì „ì²´ í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°
            extracted_data["full_text"] = "\n\n".join(full_text_parts)
            
            pdf_document.close()
            
            self.logger.info(f"âœ… PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ: {pdf_path}")
            self.logger.info(f"ğŸ“„ ì´ {extracted_data['page_count']}í˜ì´ì§€, "
                           f"í…ìŠ¤íŠ¸ ê¸¸ì´: {len(extracted_data['full_text'])}ì")
            
            return extracted_data
            
        except Exception as e:
            self.logger.error(f"âŒ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            raise
    
    async def analyze_document_with_ai(self, extracted_data: Dict[str, Any], 
                                     analysis_prompt: str = None) -> Dict[str, Any]:
        """AIë¥¼ í†µí•œ ë¬¸ì„œ ë‚´ìš© ë¶„ì„ ë° êµ¬ì¡°í™”"""
        
        if not analysis_prompt:
            analysis_prompt = """
ë‹¹ì‹ ì€ ë¬¸ì„œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ PDF ë¬¸ì„œì˜ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì´ êµ¬ì¡°í™”ëœ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:

**ë¶„ì„ ê²°ê³¼ë¥¼ ë‹¤ìŒ JSON í˜•íƒœë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”:**

```json
{
    "document_summary": {
        "title": "ë¬¸ì„œ ì œëª©",
        "type": "ë¬¸ì„œ ìœ í˜• (ê³¼ì œ, ë§¤ë‰´ì–¼, ë³´ê³ ì„œ ë“±)",
        "main_purpose": "ë¬¸ì„œì˜ ì£¼ìš” ëª©ì ",
        "key_sections": ["ì£¼ìš” ì„¹ì…˜ë“¤ì˜ ë¦¬ìŠ¤íŠ¸"]
    },
    "detailed_structure": {
        "sections": [
            {
                "section_name": "ì„¹ì…˜ëª…",
                "content_summary": "í•´ë‹¹ ì„¹ì…˜ì˜ ì£¼ìš” ë‚´ìš© ìš”ì•½",
                "key_points": ["í•µì‹¬ í¬ì¸íŠ¸ë“¤"],
                "requirements": ["ìš”êµ¬ì‚¬í•­ë“¤ (ìˆë‹¤ë©´)"],
                "deliverables": ["ì‚°ì¶œë¬¼ë“¤ (ìˆë‹¤ë©´)"]
            }
        ]
    },
    "tasks_and_requirements": {
        "step1": {
            "title": "Step 1 ì œëª©",
            "description": "ìƒì„¸ ì„¤ëª…",
            "requirements": ["ìš”êµ¬ì‚¬í•­ë“¤"],
            "deliverables": ["ì‚°ì¶œë¬¼ë“¤"],
            "technologies": ["ì‚¬ìš© ê¸°ìˆ /ë„êµ¬ë“¤"]
        },
        "step2": {
            "title": "Step 2 ì œëª©", 
            "description": "ìƒì„¸ ì„¤ëª…",
            "requirements": ["ìš”êµ¬ì‚¬í•­ë“¤"],
            "deliverables": ["ì‚°ì¶œë¬¼ë“¤"],
            "technologies": ["ì‚¬ìš© ê¸°ìˆ /ë„êµ¬ë“¤"]
        }
    },
    "technical_specifications": {
        "programming_languages": ["ì‚¬ìš©í•  í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë“¤"],
        "frameworks": ["í”„ë ˆì„ì›Œí¬ë“¤"],
        "databases": ["ë°ì´í„°ë² ì´ìŠ¤"],
        "apis": ["APIë“¤"],
        "other_tools": ["ê¸°íƒ€ ë„êµ¬ë“¤"]
    },
    "evaluation_criteria": {
        "criteria": ["í‰ê°€ ê¸°ì¤€ë“¤"],
        "weights": ["ê°€ì¤‘ì¹˜ (ëª…ì‹œë˜ì–´ ìˆë‹¤ë©´)"]
    },
    "timeline_and_deadlines": {
        "overall_deadline": "ì „ì²´ ë§ˆê°ì¼",
        "milestones": ["ì¤‘ê°„ ë§ˆì¼ìŠ¤í†¤ë“¤"]
    }
}
```

**ë¬¸ì„œ ë‚´ìš©:**
"""
        
        try:
            # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸´ ê²½ìš° ë¶„í•  ì²˜ë¦¬
            full_text = extracted_data.get("full_text", "")
            
            if len(full_text) > 50000:  # 50,000ì ì´ìƒì¸ ê²½ìš°
                self.logger.info("ğŸ“„ ë¬¸ì„œê°€ ê¸¸ì–´ì„œ í˜ì´ì§€ë³„ë¡œ ë¶„í•  ë¶„ì„ì„ ì§„í–‰í•©ë‹ˆë‹¤.")
                
                # í˜ì´ì§€ë³„ ë¶„ì„ í›„ í†µí•©
                page_analyses = []
                for page_info in extracted_data.get("pages", []):
                    if len(page_info["text"].strip()) > 100:  # ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ê°€ ìˆëŠ” í˜ì´ì§€ë§Œ
                        page_prompt = f"{analysis_prompt}\n\ní˜ì´ì§€ {page_info['page_number']}:\n{page_info['text']}"
                        page_result = await self.ai_service.answer_query(
                            "ì´ í˜ì´ì§€ì˜ ë‚´ìš©ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.", 
                            {"page_text": page_info['text']}
                        )
                        page_analyses.append({
                            "page_number": page_info['page_number'],
                            "analysis": page_result
                        })
                
                # ì „ì²´ í†µí•© ë¶„ì„
                integration_prompt = f"""
ë‹¤ìŒì€ ê° í˜ì´ì§€ë³„ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•©í•˜ì—¬ ì „ì²´ ë¬¸ì„œì˜ êµ¬ì¡°í™”ëœ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”:

{chr(10).join([f"í˜ì´ì§€ {pa['page_number']}: {pa['analysis']}" for pa in page_analyses])}

ìœ„ì˜ í˜ì´ì§€ë³„ ë¶„ì„ì„ ë°”íƒ•ìœ¼ë¡œ ì „ì²´ ë¬¸ì„œì˜ í†µí•©ëœ êµ¬ì¡°í™” ë¶„ì„ì„ JSON í˜•íƒœë¡œ ì œê³µí•´ì£¼ì„¸ìš”.
"""
                
                final_analysis = await self.ai_service.answer_query(
                    integration_prompt,
                    {"page_analyses": page_analyses}
                )
                
                return {
                    "analysis_method": "multi_page_analysis",
                    "page_count": len(page_analyses),
                    "page_analyses": page_analyses,
                    "integrated_analysis": final_analysis,
                    "document_info": {
                        "total_pages": extracted_data.get("page_count", 0),
                        "total_text_length": len(full_text),
                        "has_images": len(extracted_data.get("images", [])) > 0
                    }
                }
            
            else:
                # ì „ì²´ ë¬¸ì„œ í•œë²ˆì— ë¶„ì„
                complete_prompt = f"{analysis_prompt}\n\n{full_text}"
                
                analysis_result = await self.ai_service.answer_query(
                    "ì£¼ì–´ì§„ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì—¬ êµ¬ì¡°í™”ëœ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.",
                    {"full_document": full_text}
                )
                
                return {
                    "analysis_method": "full_document_analysis", 
                    "analysis_result": analysis_result,
                    "document_info": {
                        "total_pages": extracted_data.get("page_count", 0),
                        "total_text_length": len(full_text),
                        "has_images": len(extracted_data.get("images", [])) > 0
                    }
                }
                
        except Exception as e:
            self.logger.error(f"âŒ AI ë¬¸ì„œ ë¶„ì„ ì‹¤íŒ¨: {e}")
            raise
    
    async def process_pdf_completely(self, pdf_path: str, 
                                   custom_prompt: str = None) -> Dict[str, Any]:
        """PDF ì „ì²´ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (í…ìŠ¤íŠ¸ ì¶”ì¶œ + AI ë¶„ì„)"""
        
        try:
            self.logger.info(f"ğŸš€ PDF ì²˜ë¦¬ ì‹œì‘: {pdf_path}")
            
            # 1. PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            extracted_data = self.extract_text_from_pdf(pdf_path)
            
            # 2. AI ë¶„ì„
            analysis_result = await self.analyze_document_with_ai(
                extracted_data, 
                analysis_prompt=custom_prompt
            )
            
            # 3. ê²°ê³¼ í†µí•©
            complete_result = {
                "pdf_info": {
                    "file_path": pdf_path,
                    "file_name": Path(pdf_path).name,
                    "metadata": extracted_data.get("metadata", {}),
                    "extraction_success": True
                },
                "extracted_data": extracted_data,
                "ai_analysis": analysis_result,
                "processing_timestamp": asyncio.get_event_loop().time()
            }
            
            self.logger.info("âœ… PDF ì²˜ë¦¬ ì™„ë£Œ")
            return complete_result
            
        except Exception as e:
            self.logger.error(f"âŒ PDF ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                "pdf_info": {
                    "file_path": pdf_path,
                    "file_name": Path(pdf_path).name,
                    "extraction_success": False,
                    "error": str(e)
                },
                "extracted_data": None,
                "ai_analysis": None,
                "processing_timestamp": asyncio.get_event_loop().time()
            }

# í¸ì˜ í•¨ìˆ˜ë“¤
async def analyze_vss_assignment(pdf_path: str = None) -> Dict[str, Any]:
    """VSS ê³¼ì œ PDF ë¶„ì„ì„ ìœ„í•œ íŠ¹í™”ëœ í•¨ìˆ˜"""
    
    if pdf_path is None:
        pdf_path = r"C:\Users\MyoengHo Shin\pjt\vss_asgnM\legacy\VSS_ì…ì‚¬í…ŒìŠ¤íŠ¸ê³¼ì œ_AI.pdf"
    
    vss_prompt = """
ë‹¹ì‹ ì€ VSS ì…ì‚¬ í…ŒìŠ¤íŠ¸ ê³¼ì œë¥¼ ë¶„ì„í•˜ëŠ” ì „ë¬¸ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤. 
í˜„ì¬ ìš°ë¦¬ëŠ” ì°½ê³  ê´€ë¦¬ ì‹œìŠ¤í…œ(ì…ê³ /ì¶œê³  ë°ì´í„°, ìƒí’ˆ ë°ì´í„°)ê³¼ Gemini AI ê¸°ë°˜ RAGë¥¼ ì´ë¯¸ êµ¬ì¶•í•œ ìƒíƒœì…ë‹ˆë‹¤.

**ë§¤ìš° êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ë¶„ì„ì„ ìš”ì²­í•©ë‹ˆë‹¤. ë°˜ë“œì‹œ ë‹¤ìŒ JSON êµ¬ì¡°ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:**

```json
{
    "critical_insights": {
        "main_challenge": "ì´ ê³¼ì œì—ì„œ ê°€ì¥ ì–´ë ¤ìš´ ë¶€ë¶„ì€ ë¬´ì—‡ì¸ê°€?",
        "competitive_advantage": "ìš°ë¦¬ê°€ ì´ë¯¸ ê°€ì§„ ê°•ì ì€ ë¬´ì—‡ì¸ê°€?",
        "time_bottlenecks": "ì‹œê°„ì´ ê°€ì¥ ë§ì´ ê±¸ë¦´ ì‘ì—…ë“¤",
        "must_have_vs_nice_to_have": "í•„ìˆ˜ êµ¬í˜„ vs ì„ íƒì  êµ¬í˜„ êµ¬ë¶„"
    },
    "step_by_step_detailed": {
        "step1": {
            "exact_requirements": ["ì •í™•íˆ ë¬´ì—‡ì„ êµ¬í˜„í•´ì•¼ í•˜ëŠ”ì§€"],
            "current_status": "ìš°ë¦¬ í”„ë¡œì íŠ¸ì—ì„œ ì´ë¯¸ ì™„ë£Œëœ ë¶€ë¶„",
            "gap_analysis": "ì¶”ê°€ë¡œ ê°œë°œí•´ì•¼ í•  ë¶€ë¶„ë“¤",
            "specific_tasks": ["êµ¬ì²´ì ì¸ ê°œë°œ ì‘ì—… ë¦¬ìŠ¤íŠ¸"],
            "code_components": ["êµ¬í˜„í•´ì•¼ í•  ì½”ë“œ ì»´í¬ë„ŒíŠ¸ë“¤"],
            "integration_points": "ê¸°ì¡´ ì‹œìŠ¤í…œê³¼ì˜ ì—°ê²° ë°©ë²•"
        },
        "step2": {
            "chart_requirements": ["êµ¬ì²´ì ìœ¼ë¡œ ì–´ë–¤ ì°¨íŠ¸ë“¤ì„ ë§Œë“¤ì–´ì•¼ í•˜ëŠ”ì§€"],
            "llm_chart_logic": "LLMì´ ì°¨íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” êµ¬ì²´ì ì¸ ë¡œì§",
            "user_interaction": "ì‚¬ìš©ì ì¸í„°ë™ì…˜ êµ¬í˜„ ë°©ë²•",
            "data_flow": "inOutboundDataì—ì„œ ì°¨íŠ¸ê¹Œì§€ì˜ ë°ì´í„° íë¦„",
            "kpi_calculations": "KPI/LOI ê³„ì‚° ê³µì‹ë“¤",
            "frontend_requirements": "í”„ë¡ íŠ¸ì—”ë“œì—ì„œ êµ¬í˜„í•´ì•¼ í•  ê¸°ëŠ¥ë“¤"
        },
        "step3_alternatives": {
            "dwg_problem": "DWG íŒŒì¼ì´ ì—†ëŠ” ë¬¸ì œ í•´ê²° ë°©ì•ˆ",
            "virtual_approach": "ê°€ìƒ ë„ë©´ ì‹œìŠ¤í…œ êµ¬í˜„ ë°©ë²•",
            "canvas_implementation": "Canvas/WebGL êµ¬í˜„ ì „ëµ",
            "data_structure": "ë„ë©´ ë°ì´í„°ë¥¼ ì–´ë–»ê²Œ êµ¬ì¡°í™”í•  ê²ƒì¸ê°€",
            "realistic_scope": "ì‹¤ì œë¡œ êµ¬í˜„ ê°€ëŠ¥í•œ ë²”ìœ„"
        },
        "step4": {
            "multimodal_scope": "ë©€í‹°ëª¨ë‹¬ êµ¬í˜„ ë²”ìœ„ì™€ ë°©ë²•",
            "integration_strategy": "Step 1-3ì™€ì˜ í†µí•© ë°©ë²•",
            "demo_scenarios": "ë°ëª¨ì—ì„œ ë³´ì—¬ì¤„ ì‹œë‚˜ë¦¬ì˜¤ë“¤"
        }
    },
    "technical_roadmap": {
        "week1_priorities": ["ì²« ì£¼ì— ì§‘ì¤‘í•  í•µì‹¬ ì‘ì—…ë“¤"],
        "week2_priorities": ["ë‘˜ì§¸ ì£¼ ì‘ì—…ë“¤"],
        "minimum_viable_demo": "ìµœì†Œí•œì˜ ë°ëª¨ë¥¼ ìœ„í•´ í•„ìš”í•œ ê²ƒë“¤",
        "technology_choices": {
            "frontend": "React vs ê¸°ì¡´ HTML/JS ì¤‘ ì–´ëŠ ê²ƒì´ í˜„ì‹¤ì ì¸ê°€",
            "charts": "Chart.js vs D3.js vs ê¸°íƒ€ ì¶”ì²œ",
            "3d_rendering": "Three.js vs Canvas vs WebGL ì¤‘ ì„ íƒ",
            "file_upload": "íŒŒì¼ ì—…ë¡œë“œ êµ¬í˜„ ë°©ë²•"
        }
    },
    "risk_mitigation": {
        "step3_backup_plan": "DWG êµ¬í˜„ì´ ì•ˆ ë  ê²½ìš° Bí”Œëœ",
        "frontend_challenges": "í”„ë¡ íŠ¸ì—”ë“œ ê°œë°œ ì‹œê°„ ë¶€ì¡± í•´ê²°ì±…",
        "integration_risks": "ì‹œìŠ¤í…œ í†µí•© ì‹œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ë¬¸ì œë“¤",
        "demo_fallbacks": "ë°ëª¨ ì‹œì—° ì‹œ ë¬¸ì œ ë°œìƒ ëŒ€ë¹„ì±…"
    },
    "evaluation_strategy": {
        "showcase_priorities": "í‰ê°€ìì—ê²Œ ì–´ë–¤ ë¶€ë¶„ì„ ê°•ì¡°í•  ê²ƒì¸ê°€",
        "technical_depth": "ê¸°ìˆ ì  ê¹Šì´ë¥¼ ë³´ì—¬ì¤„ ìˆ˜ ìˆëŠ” ë¶€ë¶„ë“¤",
        "business_value": "ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜ë¥¼ ì–´í•„í•  ìˆ˜ ìˆëŠ” ìš”ì†Œë“¤",
        "innovation_points": "í˜ì‹ ì ì´ë¼ê³  ì–´í•„í•  ìˆ˜ ìˆëŠ” ë¶€ë¶„ë“¤"
    },
    "immediate_action_items": {
        "today": ["ì˜¤ëŠ˜ ë‹¹ì¥ ì‹œì‘í•  ìˆ˜ ìˆëŠ” ì‘ì—…ë“¤"],
        "this_week": ["ì´ë²ˆ ì£¼ì— ì™„ë£Œí•´ì•¼ í•  ì‘ì—…ë“¤"],
        "next_week": ["ë‹¤ìŒ ì£¼ ëª©í‘œë“¤"],
        "dependencies": "ì‘ì—… ê°„ ì˜ì¡´ì„± ê´€ê³„"
    }
}
```

**ë¶„ì„ ì‹œ íŠ¹ë³„íˆ ê³ ë ¤í•´ì•¼ í•  í˜„ì¬ ìƒí™©:**
- ì´ë¯¸ êµ¬ì¶•ë¨: FastAPI ë°±ì—”ë“œ, Gemini AI, ì°½ê³  ì…ê³ /ì¶œê³  ë°ì´í„° ë¶„ì„
- ìˆìŒ: ê¸°ë³¸ HTML/CSS/JS í”„ë¡ íŠ¸ì—”ë“œ, Chart.js
- ì—†ìŒ: React í™˜ê²½, DWG íŒŒì¼, ê³ ê¸‰ 3D ì‹œê°í™”
- ì œì•½: í”„ë¡ íŠ¸ì—”ë“œ ê°œë°œì ì—†ìŒ, ë†’ì€ UX/UI ìš”êµ¬ì‚¬í•­

**í•µì‹¬ ì§ˆë¬¸ë“¤ì— ëŒ€í•œ êµ¬ì²´ì  ë‹µë³€ ìš”ì²­:**
1. Step 2ì—ì„œ "LLM í™œìš© ì°¨íŠ¸ ë“œë¡œì‰"ì˜ ì •í™•í•œ ì˜ë¯¸ëŠ”?
2. Step 3 DWG ì—†ì´ ì–´ë–»ê²Œ ìš°íšŒí•  ê²ƒì¸ê°€?
3. ê° Stepë³„ ìµœì†Œ êµ¬í˜„ vs ìµœëŒ€ êµ¬í˜„ ë²”ìœ„ëŠ”?
4. í˜„ì¬ ì°½ê³  ë°ì´í„°ë¡œ ì–´ë–¤ ì¸ìƒì ì¸ ë°ëª¨ë¥¼ ë§Œë“¤ ìˆ˜ ìˆëŠ”ê°€?
5. í‰ê°€ìê°€ ê°€ì¥ ì£¼ëª©í•  ê¸°ìˆ ì  í¬ì¸íŠ¸ëŠ”?
"""
    
    service = PDFAnalysisService()
    result = await service.process_pdf_completely(pdf_path, vss_prompt)
    
    return result

# ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
if __name__ == "__main__":
    import asyncio
    
    async def main():
        try:
            result = await analyze_vss_assignment()
            
            print("=" * 80)
            print("ğŸ¯ VSS ê³¼ì œ ë¶„ì„ ê²°ê³¼")
            print("=" * 80)
            
            if result["pdf_info"]["extraction_success"]:
                print(f"âœ… PDF ì²˜ë¦¬ ì„±ê³µ: {result['pdf_info']['file_name']}")
                print(f"ğŸ“„ í˜ì´ì§€ ìˆ˜: {result['extracted_data']['page_count']}")
                print(f"ğŸ“ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(result['extracted_data']['full_text'])}ì")
                print("\nğŸ¤– AI ë¶„ì„ ê²°ê³¼:")
                print("-" * 50)
                
                if result["ai_analysis"]["analysis_method"] == "multi_page_analysis":
                    print("ğŸ“‹ í†µí•© ë¶„ì„ ê²°ê³¼:")
                    print(result["ai_analysis"]["integrated_analysis"])
                else:
                    print("ğŸ“‹ ì „ì²´ ë¶„ì„ ê²°ê³¼:")
                    print(result["ai_analysis"]["analysis_result"])
            else:
                print(f"âŒ PDF ì²˜ë¦¬ ì‹¤íŒ¨: {result['pdf_info']['error']}")
                
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    asyncio.run(main())