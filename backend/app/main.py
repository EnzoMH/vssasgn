from fastapi import FastAPI, Request, File, UploadFile, HTTPException, Depends
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import HTMLResponse
from pydantic import BaseModel
from .utils.ai_chat import WarehouseChatbot
from .services.data_service import DataService
from .models.ml_models import DemandPredictor, ProductClusterer, AnomalyDetector # AnomalyDetector ì¶”ê°€
from .services.data_analysis_service import DataAnalysisService
from .services.ai_service import WarehouseAI
from .services.vector_db_service import VectorDBService
import logging
import io
import pandas as pd
import os
from datetime import datetime
from typing import Dict, Any
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ (backend ë””ë ‰í† ë¦¬ì—ì„œ)
load_dotenv("../.env")

# ë¡œê±° ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

app = FastAPI(title="Warehouse Management API")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ë‹¨ì¼ ì„œë²„ì´ë¯€ë¡œ ëª¨ë“  origin í—ˆìš©
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì •ì  íŒŒì¼ ì„œë¹™ ì„¤ì •
app.mount("/static", StaticFiles(directory="static"), name="static")

# ë©”ì¸ í˜ì´ì§€ ë¼ìš°íŠ¸
@app.get("/", response_class=HTMLResponse)
async def main_page():
    with open("static/index.html", "r", encoding="utf-8") as f:
        content = f.read()
    return HTMLResponse(content=content)

# DataService, Chatbot, ML Models, DataAnalysisService, AI Service, VectorDB ì¸ìŠ¤í„´ìŠ¤ ì´ˆê¸°í™”
data_service = DataService()
demand_predictor = DemandPredictor()
product_clusterer = ProductClusterer()
anomaly_detector = AnomalyDetector() # AnomalyDetector ì¸ìŠ¤í„´ìŠ¤ ì¶”ê°€
data_analysis_service = DataAnalysisService(data_service, anomaly_detector) # anomaly_detector ì „ë‹¬
ai_service = WarehouseAI() # AI ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ì¶”ê°€
vector_db_service = VectorDBService(data_service=data_service) # ë²¡í„° DB ì„œë¹„ìŠ¤ ì¶”ê°€
chatbot = WarehouseChatbot(data_service=data_service, vector_db_service=vector_db_service) # ì„œë¹„ìŠ¤ ì£¼ì…

# ML ëª¨ë¸ í•™ìŠµ ìƒíƒœ
model_trained = {"demand_predictor": False, "product_clusterer": False, "anomaly_detector": False} # anomaly_detector ìƒíƒœ ì¶”ê°€

@app.on_event("startup")
async def startup_event():
    logger.info("ì„œë²„ ì‹œì‘ ì´ë²¤íŠ¸ ë°œìƒ: ë°ì´í„° ë¡œë”© ì‹œì‘...")
    await data_service.load_all_data(rawdata_path="../rawdata")
    logger.info("ë°ì´í„° ë¡œë”© ì™„ë£Œ.")
    
    # ì„œë²„ ì‹œì‘ ì‹œ ML ëª¨ë¸ ì‚¬ì „ í•™ìŠµ (ì„ íƒ ì‚¬í•­)
    try:
        await train_demand_predictor()
        await train_product_clusterer()
        # ì´ìƒ íƒì§€ ëª¨ë¸ì€ data_analysis_service.detect_anomalies_data() í˜¸ì¶œ ì‹œ ë‚´ë¶€ì ìœ¼ë¡œ í•™ìŠµë  ìˆ˜ ìˆìŒ
        # ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœíˆ í•™ìŠµ ìƒíƒœë§Œ Trueë¡œ ì„¤ì •
        if data_service.data_loaded:
            anomaly_result = await data_analysis_service.detect_anomalies_data() # í•™ìŠµ ë° íƒì§€ ìˆ˜í–‰
            if anomaly_result["anomalies"] is not None: # í•™ìŠµ ì„±ê³µ ì—¬ë¶€ íŒë‹¨
                model_trained["anomaly_detector"] = True
            else:
                logger.warning(f"ì´ìƒ íƒì§€ ëª¨ë¸ ì‚¬ì „ í•™ìŠµ ì‹¤íŒ¨: {anomaly_result.get('message', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")

    except Exception as e:
        logger.warning(f"ML ëª¨ë¸ ì‚¬ì „ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    # ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì¸ë±ì‹±
    try:
        if data_service.data_loaded:
            logger.info("ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì¸ë±ì‹± ì‹œì‘...")
            indexing_success = await vector_db_service.index_warehouse_data()
            if indexing_success:
                logger.info("âœ… ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì¸ë±ì‹± ì™„ë£Œ")
            else:
                logger.warning("âš ï¸ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì¸ë±ì‹± ì‹¤íŒ¨")
        else:
            logger.warning("âš ï¸ ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•„ ë²¡í„° DB ì¸ë±ì‹±ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"âŒ ë²¡í„° DB ì¸ë±ì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


async def train_demand_predictor():
    if model_trained["demand_predictor"] or not data_service.data_loaded:
        return
    
    logger.info("ìˆ˜ìš” ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
    try:
        # TODO: ì‹¤ì œ ë°ì´í„° ì „ì²˜ë¦¬ ë° í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ í•„ìš”
        # ì—¬ê¸°ì„œëŠ” ì„ì‹œ ë°ì´í„°ë¡œ ëŒ€ì²´
        # ì‹¤ì œë¡œëŠ” data_service.inbound_dataì™€ outbound_dataë¥¼ ê²°í•©í•˜ì—¬ í”¼ì²˜ë¥¼ ë§Œë“¤ì–´ì•¼ í•©ë‹ˆë‹¤.

        # ì˜ˆì‹œ: ê³¼ê±° Nì¼ê°„ì˜ ì¶œê³ ëŸ‰ì„ í”¼ì²˜ë¡œ, ë‹¤ìŒë‚  ì¶œê³ ëŸ‰ì„ ì˜ˆì¸¡
        # í¸ì˜ìƒ í˜„ì¬ëŠ” ì„ì˜ì˜ ë°ì´í„° ìƒì„±
        dummy_data = {
            'feature1': [10, 12, 15, 13, 16, 18, 20],
            'feature2': [5, 6, 7, 6, 8, 9, 10],
            'target': [11, 14, 16, 14, 17, 19, 22] # ë‹¤ìŒë‚  ì¶œê³ ëŸ‰
        }
        X = pd.DataFrame(dummy_data).drop(columns=['target'])
        y = pd.Series(dummy_data['target'])
        
        demand_predictor.train(X, y)
        model_trained["demand_predictor"] = True
        logger.info("ìˆ˜ìš” ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ.")
    except Exception as e:
        logger.error(f"ìˆ˜ìš” ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        model_trained["demand_predictor"] = False

async def train_product_clusterer():
    if model_trained["product_clusterer"] or not data_service.data_loaded:
        return
    
    logger.info("ì œí’ˆ í´ëŸ¬ìŠ¤í„°ë§ ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
    try:
        # TODO: ì‹¤ì œ ë°ì´í„° ì „ì²˜ë¦¬ ë° í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ í•„ìš”
        # ì—¬ê¸°ì„œëŠ” ì„ì‹œ ë°ì´í„°ë¡œ ëŒ€ì²´
        # ì‹¤ì œë¡œëŠ” product_masterì™€ ì…ì¶œê³  ë°ì´í„°ë¥¼ ê²°í•©í•˜ì—¬ ì œí’ˆë³„ íŠ¹ì„±ì„ ë§Œë“¤ì–´ì•¼ í•©ë‹ˆë‹¤.

        # ì˜ˆì‹œ: ì œí’ˆì˜ íŠ¹ì • ì†ì„± (íšŒì „ìœ¨, ì¬ê³ ëŸ‰ ë“±)ì„ ê¸°ë°˜ìœ¼ë¡œ í´ëŸ¬ìŠ¤í„°ë§
        # í¸ì˜ìƒ í˜„ì¬ëŠ” ì„ì˜ì˜ ë°ì´í„° ìƒì„±
        dummy_data = {
            'feature_a': [1.0, 2.0, 1.5, 3.0, 2.5, 1.2, 2.8],
            'feature_b': [100, 200, 150, 300, 250, 120, 280]
        }
        features = pd.DataFrame(dummy_data)

        product_clusterer.train(features)
        model_trained["product_clusterer"] = True
        logger.info("ì œí’ˆ í´ëŸ¬ìŠ¤í„°ë§ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ.")
    except Exception as e:
        logger.error(f"ì œí’ˆ í´ëŸ¬ìŠ¤í„°ë§ ëª¨ë¸ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        model_trained["product_clusterer"] = False

# train_anomaly_detector í•¨ìˆ˜ëŠ” data_analysis_service.detect_anomalies_data()ë¡œ ì´ë™ë˜ì—ˆìœ¼ë¯€ë¡œ ì£¼ì„ ì²˜ë¦¬ ë˜ëŠ” ì‚­ì œ
# async def train_anomaly_detector():
#    ...

@app.get("/api/dashboard/kpi")
async def get_kpi_data():
    # KPI ê³„ì‚° ë¡œì§
    # ì‹¤ì œ ë°ì´í„° ì„œë¹„ìŠ¤ì—ì„œ ê³„ì‚°ëœ KPIë¥¼ ë°˜í™˜í•˜ë„ë¡ ìˆ˜ì •
    if not data_service.data_loaded:
        raise HTTPException(status_code=404, detail="ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    inbound_df = data_service.inbound_data
    outbound_df = data_service.outbound_data
    product_df = data_service.product_master

    # ì˜ˆì‹œ KPI ê³„ì‚° (ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ë” ì •êµí•˜ê²Œ êµ¬í˜„ í•„ìš”)
    total_inventory = int(product_df['í˜„ì¬ê³ '].sum()) if 'í˜„ì¬ê³ ' in product_df.columns else 0
    daily_throughput = int(len(inbound_df) + len(outbound_df)) # ê°„ë‹¨í•˜ê²Œ ì´ ì…ì¶œê³  ê±´ìˆ˜
    rack_utilization = 0.87 # í”Œë ˆì´ìŠ¤í™€ë”
    inventory_turnover = 2.3 # í”Œë ˆì´ìŠ¤í™€ë”

    return {
        "total_inventory": total_inventory,
        "daily_throughput": daily_throughput,
        "rack_utilization": rack_utilization,
        "inventory_turnover": inventory_turnover
    }

@app.get("/api/inventory/by-rack")
async def get_inventory_by_rack():
    if not data_service.data_loaded:
        raise HTTPException(status_code=404, detail="ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    # ë™ë³„ ì¬ê³  í˜„í™© ë°ì´í„° ìƒì„± (ì˜ˆì‹œ)
    # ì‹¤ì œ ë°ì´í„°í”„ë ˆì„ êµ¬ì¡°ì— ë”°ë¼ ì§‘ê³„ ë¡œì§ ë³€ê²½ í•„ìš”
    product_df = data_service.product_master
    if 'ë™ìœ„ì¹˜' in product_df.columns and 'í˜„ì¬ê³ ' in product_df.columns:
        inventory_by_rack = product_df.groupby('ë™ìœ„ì¹˜')['í˜„ì¬ê³ '].sum().reset_index()
        inventory_by_rack.columns = ['rackName', 'currentStock'] # í”„ë¡ íŠ¸ì—”ë“œ ì°¨íŠ¸ ë°ì´í„° í‚¤ì— ë§ì¶¤
        # ì„ì˜ì˜ ìš©ëŸ‰ ë°ì´í„° ì¶”ê°€
        inventory_by_rack['capacity'] = inventory_by_rack['currentStock'] * 1.2 + 100 # ì˜ˆì‹œ
        return inventory_by_rack.to_dict(orient='records')
    else:
        return []

@app.get("/api/trends/daily")
async def get_daily_trends():
    if not data_service.data_loaded:
        raise HTTPException(status_code=404, detail="ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    daily_trends_df = data_analysis_service.get_daily_movement_summary() # data_analysis_serviceì—ì„œ ê°€ì ¸ì˜´
    return daily_trends_df.to_dict(orient='records') # DataFrameì„ ë¦¬ìŠ¤íŠ¸ ì˜¤ë¸Œ ë”•íŠ¸ë¡œ ë³€í™˜

@app.get("/api/product/category-distribution")
async def get_product_category_distribution():
    if not data_service.data_loaded:
        raise HTTPException(status_code=404, detail="ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    product_df = data_service.product_master

    # 'ì¹´í…Œê³ ë¦¬' ì»¬ëŸ¼ì„ ê°€ì •í•˜ì—¬ ì§‘ê³„
    if 'ì¹´í…Œê³ ë¦¬' in product_df.columns:
        category_counts = product_df['ì¹´í…Œê³ ë¦¬'].value_counts().reset_index()
        category_counts.columns = ['name', 'value'] # íŒŒì´ì°¨íŠ¸ ë°ì´í„° í‚¤ì— ë§ì¶¤
        return category_counts.to_dict(orient='records')
    else:
        # 'ì¹´í…Œê³ ë¦¬' ì»¬ëŸ¼ì´ ì—†ì„ ê²½ìš°, ì œí’ˆëª…ì—ì„œ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ ë˜ëŠ” ìƒìœ„ 10ê°œ ì œí’ˆ
        if 'ì œí’ˆëª…' in product_df.columns and 'ProductName' in product_df.columns:
            # ì œí’ˆëª…ì—ì„œ ê°„ë‹¨í•œ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ ì‹œë„
            try:
                # í˜„ì¬ê³  ê¸°ì¤€ìœ¼ë¡œ ìƒìœ„ 10ê°œ ì œí’ˆë§Œ í‘œì‹œ
                if 'í˜„ì¬ê³ ' in product_df.columns:
                    top_products = product_df.nlargest(10, 'í˜„ì¬ê³ ')
                    category_data = []
                    for _, row in top_products.iterrows():
                        category_data.append({
                            'name': str(row.get('ProductName', row.get('ì œí’ˆëª…', 'ì•Œ ìˆ˜ ì—†ìŒ')))[:20] + ('...' if len(str(row.get('ProductName', row.get('ì œí’ˆëª…', '')))) > 20 else ''),
                            'value': int(row.get('í˜„ì¬ê³ ', 0))
                        })
                    return category_data
                else:
                    # í˜„ì¬ê³  ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ì œí’ˆë³„ë¡œ 1ê°œì”© í• ë‹¹í•˜ì—¬ ìƒìœ„ 8ê°œ
                    product_counts = product_df['ProductName'].value_counts().head(8).reset_index()
                    product_counts.columns = ['name', 'value']
                    return product_counts.to_dict(orient='records')
            except Exception as e:
                logger.error(f"ì¹´í…Œê³ ë¦¬ ë°ì´í„° ìƒì„± ì˜¤ë¥˜: {e}")
                # ê¸°ë³¸ ë”ë¯¸ ë°ì´í„° ë°˜í™˜
                return [
                    {'name': 'ì „ìì œí’ˆ', 'value': 45},
                    {'name': 'ê°€ì „ì œí’ˆ', 'value': 32},
                    {'name': 'ì˜ë¥˜', 'value': 28},
                    {'name': 'ì‹í’ˆ', 'value': 22},
                    {'name': 'ë„ì„œ', 'value': 18},
                    {'name': 'ê¸°íƒ€', 'value': 15}
                ]
        else:
            # ê¸°ë³¸ ë”ë¯¸ ë°ì´í„° ë°˜í™˜
            return [
                {'name': 'ì „ìì œí’ˆ', 'value': 45},
                {'name': 'ê°€ì „ì œí’ˆ', 'value': 32},
                {'name': 'ì˜ë¥˜', 'value': 28},
                {'name': 'ì‹í’ˆ', 'value': 22},
                {'name': 'ë„ì„œ', 'value': 18},
                {'name': 'ê¸°íƒ€', 'value': 15}
            ]

@app.get("/api/analysis/stats/{df_name}")
async def get_analysis_stats(df_name: str):
    if not data_service.data_loaded:
        raise HTTPException(status_code=404, detail="ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    stats = data_analysis_service.get_descriptive_stats(df_name)
    return stats

@app.get("/api/analysis/daily-movement")
async def get_analysis_daily_movement():
    if not data_service.data_loaded:
        raise HTTPException(status_code=404, detail="ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    # data_analysis_service.get_daily_movement_summary()ëŠ” ì´ì œ DataFrameì„ ë°˜í™˜
    summary_df = data_analysis_service.get_daily_movement_summary()
    return summary_df.to_dict(orient='records') # ë¦¬ìŠ¤íŠ¸ ì˜¤ë¸Œ ë”•íŠ¸ë¡œ ë³€í™˜

@app.get("/api/analysis/product-insights")
async def get_analysis_product_insights():
    if not data_service.data_loaded:
        raise HTTPException(status_code=404, detail="ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    insights = data_analysis_service.get_product_insights()
    return insights

@app.get("/api/analysis/rack-utilization")
async def get_analysis_rack_utilization():
    if not data_service.data_loaded:
        raise HTTPException(status_code=404, detail="ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    summary = data_analysis_service.get_rack_utilization_summary()
    return summary

@app.get("/api/analysis/anomalies")
async def get_anomalies():
    # ì´ìƒ íƒì§€ ë¡œì§ì€ data_analysis_serviceë¡œ ì´ë™
    anomalies_result = await data_analysis_service.detect_anomalies_data()
    if not anomalies_result["anomalies"] and anomalies_result.get("message") and "ì˜¤ë¥˜" in anomalies_result["message"]:
        raise HTTPException(status_code=500, detail=anomalies_result["message"])
    return anomalies_result

class DemandPredictionRequest(BaseModel):
    features: Dict[str, Any] # ì˜ˆì¸¡ì— í•„ìš”í•œ í”¼ì²˜ë¥¼ í´ë¼ì´ì–¸íŠ¸ì—ì„œ ì „ë‹¬í•œë‹¤ê³  ê°€ì •

@app.post("/api/predict/demand")
async def predict_demand(request: DemandPredictionRequest):
    if not data_service.data_loaded:
        raise HTTPException(status_code=404, detail="ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    if not model_trained["demand_predictor"]:
        await train_demand_predictor()
        if not model_trained["demand_predictor"]:
            raise HTTPException(status_code=500, detail="ìˆ˜ìš” ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    
    try:
        # í´ë¼ì´ì–¸íŠ¸ì—ì„œ ë°›ì€ í”¼ì²˜ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
        input_features = pd.DataFrame([request.features])
        prediction = demand_predictor.predict_daily_demand(input_features)
        # ì˜ˆì¸¡ ê²°ê³¼ëŠ” numpy ë°°ì—´ì´ë¯€ë¡œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜
        return {"prediction": prediction.tolist()}
    except Exception as e:
        logger.error(f"ìˆ˜ìš” ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise HTTPException(status_code=500, detail=f"ìˆ˜ìš” ì˜ˆì¸¡ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

@app.post("/api/upload/data")
async def upload_data(file: UploadFile = File(...)):
    try:
        # íŒŒì¼ í™•ì¥ì í™•ì¸
        file_extension = os.path.splitext(file.filename)[1].lower()
        if file_extension not in [".csv", ".xlsx", ".xls"]:
            raise HTTPException(status_code=400, detail="ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. CSV ë˜ëŠ” Excel íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")

        contents = await file.read()
        data_io = io.BytesIO(contents)

        if file_extension == ".csv":
            df = pd.read_csv(data_io)
        else: # .xlsx or .xls
            df = pd.read_excel(data_io)
        
        # TODO: ì—…ë¡œë“œëœ ë°ì´í„° ì²˜ë¦¬ ë° ì €ì¥ ë¡œì§ êµ¬í˜„
        # ì˜ˆ: data_serviceì— ë°ì´í„° ì¶”ê°€ ë˜ëŠ” íŠ¹ì • ìœ„ì¹˜ì— ì €ì¥
        # ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœíˆ ë¡œë“œí•˜ê³  ì„±ê³µ ë©”ì‹œì§€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        logger.info(f"Uploaded file: {file.filename}, rows: {len(df)}")

        # ë°ì´í„° ì—…ë¡œë“œ í›„ data_serviceì— ë°˜ì˜í•˜ê³  ëª¨ë¸ ì¬í•™ìŠµ (ì„ íƒ ì‚¬í•­)
        # ì´ ë¶€ë¶„ì€ ë°ì´í„°ì˜ ì„±ê²©ê³¼ í™œìš© ë°©ì‹ì— ë”°ë¼ ë³µì¡ë„ê°€ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        # ì˜ˆë¥¼ ë“¤ì–´, ì…ì¶œê³  ë°ì´í„°ë©´ ê¸°ì¡´ ë°ì´í„°ì— concat, ìƒí’ˆ ë°ì´í„°ë©´ replace ë“±
        # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ ìƒˆë¡œ ë¡œë“œí•˜ëŠ” ê²ƒìœ¼ë¡œ ê°€ì • (ì‹¤ì œ ì„œë¹„ìŠ¤ì—ì„œëŠ” ë°ì´í„° ë³‘í•© ë“± í•„ìš”)
        await data_service.load_all_data(rawdata_path="../rawdata") # ë‹¤ì‹œ ëª¨ë“  ë°ì´í„° ë¡œë“œ (ì„ì‹œ)
        model_trained["demand_predictor"] = False # ëª¨ë¸ ì¬í•™ìŠµ í•„ìš”
        model_trained["product_clusterer"] = False # ëª¨ë¸ ì¬í•™ìŠµ í•„ìš”
        model_trained["anomaly_detector"] = False # ëª¨ë¸ ì¬í•™ìŠµ í•„ìš”

        return {"message": f"íŒŒì¼ \'{file.filename}\'ì´ ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤. ì´ {len(df)}ê°œì˜ í–‰ì´ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.", "rows_processed": len(df)}

    except Exception as e:
        logger.error(f"íŒŒì¼ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise HTTPException(status_code=500, detail=f"íŒŒì¼ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

class ChatRequest(BaseModel):
    question: str

@app.post("/api/ai/chat")
async def ai_chat(request: ChatRequest):
    logger.info(f"AI Chat ìš”ì²­ ìˆ˜ì‹ : {request.question}")
    try:
        response_text = await chatbot.process_query(request.question)
        logger.info(f"AI Chat ì‘ë‹µ ìƒì„± ì™„ë£Œ.")
        return {"answer": response_text}
    except Exception as e:
        logger.error(f"AI Chat ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise HTTPException(status_code=500, detail=f"AI Chat ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

@app.post("/api/product/cluster")
async def cluster_products_api():
    if not data_service.data_loaded:
        raise HTTPException(status_code=404, detail="ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    if not model_trained["product_clusterer"]:
        await train_product_clusterer()
        if not model_trained["product_clusterer"]:
            raise HTTPException(status_code=500, detail="ì œí’ˆ í´ëŸ¬ìŠ¤í„°ë§ ëª¨ë¸ í•™ìŠµì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

    try:
        # TODO: ì‹¤ì œ ì œí’ˆ í´ëŸ¬ìŠ¤í„°ë§ì— í•„ìš”í•œ í”¼ì²˜ë¥¼ product_masterì—ì„œ ì¶”ì¶œ
        # í˜„ì¬ëŠ” train_product_clustererì—ì„œ ì‚¬ìš©ëœ ì„ì‹œ ë°ì´í„°ì™€ ë™ì¼í•œ êµ¬ì¡°ë¥¼ ê°€ì •
        dummy_features = {
            'feature_a': [1.0, 2.0, 1.5, 3.0, 2.5, 1.2, 2.8],
            'feature_b': [100, 200, 150, 300, 250, 120, 280]
        }
        features_df = pd.DataFrame(dummy_features)

        clusters = product_clusterer.cluster_products(features_df)
        return {"clusters": clusters.tolist()}
    except Exception as e:
        logger.error(f"ì œí’ˆ í´ëŸ¬ìŠ¤í„°ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise HTTPException(status_code=500, detail=f"ì œí’ˆ í´ëŸ¬ìŠ¤í„°ë§ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# ì°¨íŠ¸ ìƒì„± ìš”ì²­ ëª¨ë¸
class ChartGenerationRequest(BaseModel):
    user_request: str  # ì‚¬ìš©ìì˜ ìì—°ì–´ ì°¨íŠ¸ ìš”ì²­
    context: str = ""  # ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ (ì„ íƒì‚¬í•­)

@app.post("/api/ai/generate-chart")
async def generate_chart(request: ChartGenerationRequest):
    """LLMì„ í™œìš©í•œ ì°¨íŠ¸ ì„¤ì • ìƒì„± API"""
    if not data_service.data_loaded:
        raise HTTPException(status_code=404, detail="ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    logger.info(f"ì°¨íŠ¸ ìƒì„± ìš”ì²­: {request.user_request}")
    
    try:
        # ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ë°ì´í„° ê²€ìƒ‰
        vector_search_result = await vector_db_service.search_relevant_data(
            query=request.user_request,
            n_results=20
        )
        
        # ê²€ìƒ‰ëœ ì‹¤ì œ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ ë©”íƒ€ë°ì´í„° ì‚¬ìš©
        if vector_search_result.get("success") and vector_search_result.get("chart_data"):
            # ì‹¤ì œ ë°ì´í„°ë¡œ ì°¨íŠ¸ ì„¤ì • ìƒì„±
            chart_result = await _generate_chart_from_real_data(
                user_request=request.user_request,
                search_result=vector_search_result
            )
        else:
            # ê¸°ì¡´ ë°©ì‹: ë©”íƒ€ë°ì´í„°ë¡œ AI ìƒì„±
            available_data = await _prepare_available_data_info()
            chart_result = await ai_service.generate_chart_config(
                user_request=request.user_request,
                available_data=available_data
            )
        
        if chart_result["success"]:
            logger.info(f"ì°¨íŠ¸ ì„¤ì • ìƒì„± ì„±ê³µ: {chart_result['chart_config']['chart_type']}")
            return {
                "success": True,
                "chart_config": chart_result["chart_config"],
                "message": chart_result["message"]
            }
        else:
            logger.warning(f"ì°¨íŠ¸ ì„¤ì • ìƒì„± ì‹¤íŒ¨, ëŒ€ì²´ ì„¤ì • ì‚¬ìš©: {chart_result['error']}")
            return {
                "success": False,
                "chart_config": chart_result["fallback_config"],
                "message": chart_result["message"],
                "error": chart_result["error"]
            }
            
    except Exception as e:
        logger.error(f"ì°¨íŠ¸ ìƒì„± API ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise HTTPException(status_code=500, detail=f"ì°¨íŠ¸ ìƒì„± ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

async def _prepare_available_data_info() -> dict:
    """ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„° ì •ë³´ë¥¼ ì •ë¦¬í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        available_data = {}
        
        # ì…ê³  ë°ì´í„° ì •ë³´
        if data_service.inbound_data is not None and not data_service.inbound_data.empty:
            available_data["inbound_data"] = {
                "description": "ì…ê³  ë°ì´í„° (ê³µê¸‰ì—…ì²´ë³„ ìƒí’ˆ ì…ê³  ì •ë³´)",
                "columns": list(data_service.inbound_data.columns),
                "row_count": len(data_service.inbound_data),
                "date_range": _get_date_range(data_service.inbound_data, 'Date')
            }
        
        # ì¶œê³  ë°ì´í„° ì •ë³´
        if data_service.outbound_data is not None and not data_service.outbound_data.empty:
            available_data["outbound_data"] = {
                "description": "ì¶œê³  ë°ì´í„° (ê³ ê°ì‚¬ë³„ ìƒí’ˆ ì¶œê³  ì •ë³´)",
                "columns": list(data_service.outbound_data.columns),
                "row_count": len(data_service.outbound_data),
                "date_range": _get_date_range(data_service.outbound_data, 'Date')
            }
        
        # ìƒí’ˆ ë§ˆìŠ¤í„° ë°ì´í„° ì •ë³´
        if data_service.product_master is not None and not data_service.product_master.empty:
            available_data["product_master"] = {
                "description": "ìƒí’ˆ ë§ˆìŠ¤í„° ë°ì´í„° (ìƒí’ˆë³„ ê¸°ë³¸ ì •ë³´ ë° ì¬ê³ )",
                "columns": list(data_service.product_master.columns),
                "row_count": len(data_service.product_master)
            }
        
        # ê¸°ë³¸ KPI ì •ë³´ ì¶”ê°€
        available_data["kpi_metrics"] = {
            "description": "ê³„ì‚° ê°€ëŠ¥í•œ KPI ì§€í‘œë“¤",
            "metrics": [
                "ì¼ë³„ ì…ê³ ëŸ‰/ì¶œê³ ëŸ‰",
                "ë™ë³„ ì¬ê³  í˜„í™©",
                "ìƒí’ˆë³„ íšŒì „ìœ¨",
                "ê³µê¸‰ì—…ì²´ë³„ ì…ê³  í˜„í™©",
                "ê³ ê°ì‚¬ë³„ ì¶œê³  í˜„í™©",
                "ì¬ê³  ìˆ˜ì¤€ ë¶„ì„"
            ]
        }
        
        return available_data
        
    except Exception as e:
        logger.error(f"ë°ì´í„° ì •ë³´ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
        return {"error": f"ë°ì´í„° ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}"}

def _get_date_range(df, date_column):
    """ë°ì´í„°í”„ë ˆì„ì—ì„œ ë‚ ì§œ ë²”ìœ„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        if date_column in df.columns:
            dates = pd.to_datetime(df[date_column], errors='coerce')
            min_date = dates.min()
            max_date = dates.max()
            if pd.notna(min_date) and pd.notna(max_date):
                return f"{min_date.strftime('%Y-%m-%d')} ~ {max_date.strftime('%Y-%m-%d')}"
        return "ë‚ ì§œ ì •ë³´ ì—†ìŒ"
    except Exception:
        return "ë‚ ì§œ ì •ë³´ íŒŒì‹± ì‹¤íŒ¨"

async def _generate_chart_from_real_data(user_request: str, search_result: Dict[str, Any]) -> Dict[str, Any]:
    """ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ë¡œ ì‹¤ì œ ì°¨íŠ¸ ì„¤ì • ìƒì„±"""
    try:
        chart_data = search_result.get("chart_data", {})
        
        if not chart_data.get("labels") or not chart_data.get("data"):
            raise ValueError("ê²€ìƒ‰ëœ ë°ì´í„°ì— ì°¨íŠ¸ ìƒì„±ì— í•„ìš”í•œ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # ì‚¬ìš©ì ìš”ì²­ì—ì„œ ì°¨íŠ¸ íƒ€ì… ì¶”ì •
        chart_type = _infer_chart_type_from_request(user_request)
        
        # ìƒ‰ìƒ íŒ”ë ˆíŠ¸
        colors = [
            "#3b82f6", "#10b981", "#f59e0b", "#ef4444", "#8b5cf6",
            "#06b6d4", "#84cc16", "#f97316", "#ec4899", "#6b7280"
        ]
        
        # Chart.js í˜¸í™˜ ì„¤ì • ìƒì„±
        chart_config = {
            "chart_type": chart_type,
            "title": chart_data.get("title", "ë°ì´í„° ì°¨íŠ¸"),
            "data": {
                "labels": chart_data["labels"][:10],  # ìµœëŒ€ 10ê°œê¹Œì§€ë§Œ í‘œì‹œ
                "datasets": [{
                    "label": chart_data.get("title", "ë°ì´í„°"),
                    "data": chart_data["data"][:10],
                    "backgroundColor": colors[:len(chart_data["data"][:10])],
                    "borderColor": colors[0],
                    "borderWidth": 2 if chart_type == "line" else 1,
                    "tension": 0.3 if chart_type == "line" else 0
                }]
            },
            "options": {
                "responsive": True,
                "plugins": {
                    "title": {
                        "display": True,
                        "text": chart_data.get("title", "ë°ì´í„° ì°¨íŠ¸"),
                        "font": {"size": 16, "weight": "bold"}
                    },
                    "legend": {
                        "display": True,
                        "position": "top"
                    }
                },
                "scales": {} if chart_type in ["pie", "doughnut"] else {
                    "y": {
                        "beginAtZero": True,
                        "title": {
                            "display": True,
                            "text": "ìˆ˜ëŸ‰"
                        }
                    },
                    "x": {
                        "title": {
                            "display": True,
                            "text": "í•­ëª©"
                        }
                    }
                }
            },
            "query_info": {
                "data_source": f"ì‹¤ì œ ë°ì´í„° ê²€ìƒ‰ ({search_result.get('found_documents', 0)}ê°œ ë¬¸ì„œ)",
                "search_query": user_request,
                "data_type": chart_data.get("type", "unknown")
            }
        }
        
        logger.info(f"âœ… ì‹¤ì œ ë°ì´í„°ë¡œ ì°¨íŠ¸ ì„¤ì • ìƒì„±: {chart_type} - {chart_data.get('title')}")
        
        return {
            "success": True,
            "chart_config": chart_config,
            "message": f"ì‹¤ì œ ë°ì´í„° {search_result.get('found_documents', 0)}ê°œ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì°¨íŠ¸ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.",
            "data_source": "vector_database"
        }
        
    except Exception as e:
        logger.error(f"âŒ ì‹¤ì œ ë°ì´í„° ì°¨íŠ¸ ìƒì„± ì‹¤íŒ¨: {str(e)}")
        return {
            "success": False,
            "error": str(e),
            "message": "ì‹¤ì œ ë°ì´í„°ë¡œ ì°¨íŠ¸ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. AI ìƒì„±ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.",
            "fallback_config": None
        }

def _infer_chart_type_from_request(user_request: str) -> str:
    """ì‚¬ìš©ì ìš”ì²­ì—ì„œ ì°¨íŠ¸ íƒ€ì… ì¶”ì •"""
    request_lower = user_request.lower()
    
    if any(word in request_lower for word in ['íŒŒì´ì°¨íŠ¸', 'pie', 'ì›ê·¸ë˜í”„', 'ë„ë„›']):
        return "doughnut" if 'ë„ë„›' in request_lower else "pie"
    elif any(word in request_lower for word in ['ì„ ê·¸ë˜í”„', 'line', 'ì¶”ì´', 'íŠ¸ë Œë“œ', 'ë³€í™”']):
        return "line"
    elif any(word in request_lower for word in ['ë§‰ëŒ€ì°¨íŠ¸', 'bar', 'ë§‰ëŒ€ê·¸ë˜í”„', 'ë¹„êµ']):
        return "bar"
    elif any(word in request_lower for word in ['ì‚°ì ë„', 'scatter', 'ë¶„í¬']):
        return "scatter"
    else:
        # ê¸°ë³¸ê°’: ë§‰ëŒ€ì°¨íŠ¸
        return "bar"

@app.get("/api/vector-db/status")
async def get_vector_db_status():
    """ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ í™•ì¸"""
    try:
        status = vector_db_service.get_status()
        return status
    except Exception as e:
        logger.error(f"ë²¡í„° DB ìƒíƒœ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
        return {
            "status": "error",
            "error": str(e),
            "message": "ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        }

@app.post("/api/vector-db/reindex")
async def reindex_vector_db():
    """ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì¬ì¸ë±ì‹±"""
    if not data_service.data_loaded:
        raise HTTPException(status_code=404, detail="ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    try:
        logger.info("ğŸ”„ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì¬ì¸ë±ì‹± ì‹œì‘...")
        success = await vector_db_service.index_warehouse_data(force_rebuild=True)
        
        if success:
            return {
                "success": True,
                "message": "ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì¬ì¸ë±ì‹±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
                "status": vector_db_service.get_status()
            }
        else:
            raise HTTPException(status_code=500, detail="ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì¬ì¸ë±ì‹±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            
    except Exception as e:
        logger.error(f"ë²¡í„° DB ì¬ì¸ë±ì‹± ì¤‘ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"ì¬ì¸ë±ì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}") 